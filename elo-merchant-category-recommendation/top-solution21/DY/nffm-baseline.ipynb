{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:02:57.264785Z",
     "start_time": "2019-02-21T12:02:56.222996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ctrNet-tool' already exists and is not an empty directory.\n",
      "total 415920\n",
      "drwxr-xr-x 14 d d      4096 Feb 21 07:01 .\n",
      "drwxr-xr-x  5 d d      4096 Feb 20 00:22 ..\n",
      "-rw-rw-r--  1 d d   4440024 Feb 20 00:25 3_blend.csv\n",
      "-rw-rw-r--  1 d d     10603 Feb 20 00:25 belnd.ipynb\n",
      "-rw-rw-r--  1 d d    469148 Feb 20 00:14 best_cv.ipynb\n",
      "-rw-r--r--  1 d d    658463 Feb 13 22:56 best_single_model2.ipynb\n",
      "-rw-r--r--  1 d d    657844 Feb 17 06:29 best_single_model-Copy3.ipynb\n",
      "-rw-r--r--  1 d d    657844 Feb 18 23:30 best_single_model.ipynb\n",
      "-rw-rw-r--  1 d d     61461 Feb 17 02:48 best_single_model-tfidf.ipynb\n",
      "-rw-rw-r--  1 d d    639615 Feb 12 21:21 best_single_model-分类-Copy1.ipynb\n",
      "-rw-r--r--  1 d d    668175 Feb 14 01:27 best_single_model-分类.ipynb\n",
      "-rw-r--r--  1 d d    685859 Feb 12 08:23 best_single_model-去除异常点.ipynb\n",
      "-rw-rw-r--  1 d d 409991896 Feb 20 05:02 best_t_weight.h5\n",
      "drwxr-xr-x  3 d d      4096 Feb 20 05:08 class_oof\n",
      "drwxr-xr-x  3 d d      4096 Feb 19 10:41 class_sub\n",
      "-rw-rw-r--  1 d d    692996 Feb 19 11:01 cls_super-big-Copy1.ipynb\n",
      "-rw-r--r--  1 d d     71784 Feb 17 04:12 Combining your model with a model without outlier.ipynb\n",
      "-rw-rw-r--  1 d d       766 Feb 21 07:02 ctrNet.py\n",
      "drwxrwxr-x  6 d d      4096 Feb 12 21:00 ctrNet-tool\n",
      "drwxrwxr-x  2 d d      4096 Feb 16 20:03 data\n",
      "-rw-rw-r--  1 d d    100236 Feb 20 21:41 fork_kernel_lb691.ipynb\n",
      "drwxr-xr-x  2 d d      4096 Feb 20 19:48 .ipynb_checkpoints\n",
      "-rw-r--r--  1 d d    114530 Feb 19 05:19 lgb_cv3.6412_lb_3.684.ipynb\n",
      "-rw-rw-r--  1 d d     82560 Feb 19 08:39 lgb_cv_3.642_lb_3.681.ipynb\n",
      "drwxrwxr-x  3 d d      4096 Feb 12 21:01 models\n",
      "-rw-r--r--  1 d d     67411 Feb 21 07:01 nffm-baseline.ipynb\n",
      "-rw-rw-r--  1 d d    714555 Feb 20 03:29 nffm-分类-Copy1.ipynb\n",
      "-rw-rw-r--  1 d d     62499 Feb 20 05:08 nn_model.ipynb\n",
      "-rw-rw-r--  1 d d     72660 Feb 17 22:00 nn_model-regression.ipynb\n",
      "drwxr-xr-x  3 d d     12288 Feb 21 06:54 oof\n",
      "-rw-rw-r--  1 d d    632405 Feb 15 23:15 pre_0213.ipynb\n",
      "-rw-rw-r--  1 d d    778375 Feb 20 00:12 pre_0214 (1).ipynb\n",
      "-rw-rw-r--  1 d d    681635 Feb 16 08:25 pre_0214.ipynb\n",
      "drwxrwxr-x  2 d d      4096 Feb 21 06:17 __pycache__\n",
      "-rw-rw-r--  1 d d      2268 Feb 21 07:02 README.md\n",
      "-rw-r--r--  1 d d    442134 Feb 18 19:47 reg_0213-Copy1.ipynb\n",
      "-rw-rw-r--  1 d d    851968 Feb 16 07:36 reg_0214.ipynb\n",
      "-rw-rw-r--  1 d d    933676 Feb 17 04:21 reg_0216.ipynb\n",
      "-rw-rw-r--  1 d d    216821 Feb 21 05:08 reg_single_cv3.63480-feature-select.ipynb\n",
      "-rw-rw-r--  1 d d    306329 Feb 20 20:03 reg_single_cv3.63480.ipynb\n",
      "drwxrwxr-x  3 d d      4096 Feb 12 21:01 src\n",
      "drwxr-xr-x  3 d d     16384 Feb 21 06:07 sub\n",
      "drwxr-xr-x  3 d d      4096 Feb 19 10:28 去除异常值_oof\n",
      "drwxr-xr-x  2 d d      4096 Feb 12 08:23 去除异常值_sub\n"
     ]
    }
   ],
   "source": [
    "#Download ctrNet-tool \n",
    "#You can find the code in https://github.com/guoday/ctrNet-tool\n",
    "!git clone https://github.com/guoday/ctrNet-tool.git\n",
    "!cp -r ctrNet-tool/* ./\n",
    "# !rm -r ctrNet-tool data .git\n",
    "!ls -all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:02:57.278167Z",
     "start_time": "2019-02-21T12:02:57.266961Z"
    }
   },
   "outputs": [],
   "source": [
    "import ctrNet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src import misc_utils as utils\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "NROWS = None\n",
    "\n",
    "if DEBUG:\n",
    "    NROWS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:02:57.301682Z",
     "start_time": "2019-02-21T12:02:57.280339Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date, datetime\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "# import workalendar\n",
    "# from workalendar.america import Brazil\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(4590)\n",
    "\n",
    "skf= StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "\n",
    "# by feature select \n",
    "# FILTER_FEATURE = ['new_hist_month_diff_min_-_hist_month_diff_min', 'new_hist_merchant_category_id_mean_mean_add_hist_merchant_category_id_mean_mean', 'new_hist_month_lag_mean_mean_add_hist_month_lag_mean_mean', 'hist_year_nunique', 'new_hist_month_diff_max_/_hist_month_diff_max', 'new_hist_merchant_id_mean_mean_add_hist_merchant_id_mean_mean', 'new_hist_month_diff_min_/_hist_month_diff_min', 'new_hist_year_mean_mean_add_hist_year_mean_mean', 'new_hist_dayofweek_mean_mean_/_hist_dayofweek_mean_mean', 'new_hist_weekofyear_mean_mean_add_hist_weekofyear_mean_mean', 'new_hist_dayofweek_nunique_/_hist_dayofweek_nunique', 'new_hist_month_diff_max', 'new_hist_year_mean_mean_/_hist_year_mean_mean', 'new_hist_merchant_category_id_mean_mean_/_hist_merchant_category_id_mean_mean', 'new_hist_merchant_id_mean_mean_/_hist_merchant_id_mean_mean', 'new_hist_year_nunique_/_hist_year_nunique', 'new_hist_month_diff_min', 'new_hist_merchant_category_id_mean_mean', 'hist_month_diff_mean_hist_month_diff_min_ctr', 'new_hist_year_nunique_-_hist_year_nunique', 'dayofweek', 'new_hist_month_mean_mean_/_hist_month_mean_mean', 'new_hist_month_diff_var', 'new_hist_installments_min_-_hist_installments_min', 'new_hist_subsector_id_mean_mean', 'new_hist_dayofweek_nunique', 'new_hist_weekend_mean', 'new_hist_category_2_mean_mean_/_hist_category_2_mean_mean', 'new_hist_merchant_category_id_nunique_-_hist_merchant_category_id_nunique', 'new_hist_month_diff_mean_add_hist_month_diff_mean', 'new_hist_authorized_flag_mean', 'new_hist_merchant_category_id_mean_mean_-_hist_merchant_category_id_mean_mean', 'new_hist_year_mean_mean_-_hist_year_mean_mean', 'new_hist_category_2_mean_mean', 'new_hist_card_id_mean_mean_add_hist_card_id_mean_mean', 'new_hist_month_diff_max_-_hist_month_diff_max', 'new_hist_weekend_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:04:24.843705Z",
     "start_time": "2019-02-21T12:02:57.303789Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/train.csv',nrows = NROWS)\n",
    "df_test = pd.read_csv('../input/test.csv',nrows = NROWS)\n",
    "df_hist_trans = pd.read_csv('../input/historical_transactions.csv',parse_dates=['purchase_date'], nrows = NROWS)\n",
    "df_new_merchant_trans = pd.read_csv('../input/new_merchant_transactions.csv',parse_dates=['purchase_date'], nrows = NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:04:24.852983Z",
     "start_time": "2019-02-21T12:04:24.845475Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_date=df_new_merchant_trans['purchase_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:04:24.916928Z",
     "start_time": "2019-02-21T12:04:24.854773Z"
    },
    "_uuid": "520b71064293a20e0f2a379dad0acc274374a3c4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:04:31.023818Z",
     "start_time": "2019-02-21T12:04:24.919295Z"
    },
    "_uuid": "44198b73053869f8dfc083204c592fe9193f239c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n",
      "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n",
      "Mem. usage decreased to 1749.11 Mb (43.7% reduction)\n",
      "Mem. usage decreased to 114.20 Mb (45.5% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83400"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "df_hist_trans = reduce_mem_usage(df_hist_trans)\n",
    "df_new_merchant_trans = reduce_mem_usage(df_new_merchant_trans)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:04:31.038905Z",
     "start_time": "2019-02-21T12:04:31.026216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0            2017-06  C_ID_92a2005557          5          2          1   \n",
       "1            2017-01  C_ID_3d0044924f          4          1          0   \n",
       "2            2016-08  C_ID_d639edf6cd          2          2          0   \n",
       "3            2017-09  C_ID_186d6a6901          4          3          0   \n",
       "4            2017-11  C_ID_cdbd2c0db2          1          3          0   \n",
       "\n",
       "     target  \n",
       "0 -0.820312  \n",
       "1  0.392822  \n",
       "2  0.687988  \n",
       "3  0.142456  \n",
       "4 -0.159790  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:04:31.045876Z",
     "start_time": "2019-02-21T12:04:31.041512Z"
    },
    "_uuid": "dda90662d05e22310dd713df106ea07f4b8bccfc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:04:50.143272Z",
     "start_time": "2019-02-21T12:04:31.048305Z"
    },
    "_uuid": "7c91d3b9e9dbaff01962b0facbace75705a9ce18",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for df in [df_hist_trans,df_new_merchant_trans]:\n",
    "#     df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['year'] = df['purchase_date'].dt.year\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['day'] = df['purchase_date'].dt.day\n",
    "    \n",
    "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
    "    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n",
    "    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n",
    "    df['month_diff'] = ((max_date - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "#     df['authorized_flag_purchase_amount'] = df.apply(lambda x: x['purchase_amount'] \\\n",
    "#                             if x['authorized_flag']>0 else 0 ,axis=1)\n",
    "    \n",
    "#     df['unauthorized_flag_purchase_amount'] = df.apply(lambda x: x['purchase_amount'] \\\n",
    "#                             if x['authorized_flag']<1 else 0 ,axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:04:50.147316Z",
     "start_time": "2019-02-21T12:04:50.144940Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_hist_trans.groupby('card_id')['month'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:36.365585Z",
     "start_time": "2019-02-21T12:04:50.149197Z"
    },
    "_uuid": "ddf1d5bb0ade2b22b0f072c208c1506ea64503ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def  get_agg_fea(count_df,prefix):\n",
    "    aggs = {}\n",
    "    for col in ['month','day','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "    aggs['installments'] = ['sum','max','min','mean','var']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var']\n",
    "    # aggs['month_diff'] = ['mean']\n",
    "    aggs['month_diff'] = ['max','min','mean','var']\n",
    "    aggs['authorized_flag'] = ['sum', 'mean']\n",
    "    aggs['weekend'] = ['sum', 'mean']\n",
    "    aggs['category_1'] = ['sum', 'mean']\n",
    "    aggs['card_id'] = ['size']\n",
    "\n",
    "    for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id',\\\n",
    "                'category_1','category_2','category_3','month_lag','card_id']:\n",
    "        count_df[col+'_mean'] = count_df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        aggs[col+'_mean'] = ['mean']   \n",
    "\n",
    "    new_columns = get_new_columns(prefix,aggs)\n",
    "    count_df_gp = count_df.groupby('card_id').agg(aggs)\n",
    "    count_df_gp.columns = new_columns\n",
    "    count_df_gp.reset_index(drop=False,inplace=True)\n",
    "    count_df_gp['%s_purchase_date_diff'%prefix] = (count_df_gp['%s_purchase_date_max'%prefix] - count_df_gp['%s_purchase_date_min'%prefix]).dt.days\n",
    "    count_df_gp['%s_purchase_date_average'%prefix] = count_df_gp['%s_purchase_date_diff'%prefix]/count_df_gp['%s_card_id_size'%prefix]\n",
    "    count_df_gp['%s_purchase_date_uptonow'%prefix] = (max_date - count_df_gp['%s_purchase_date_max'%prefix]).dt.days\n",
    "    count_df_gp['%s_purchase_date_uptomin'%prefix] = (max_date - count_df_gp['%s_purchase_date_min'%prefix]).dt.days\n",
    "    \n",
    "    return count_df_gp\n",
    "\n",
    "\n",
    "\n",
    "hist_count_df=get_agg_fea(df_hist_trans,'hist')\n",
    "df_train = df_train.merge(hist_count_df,on='card_id',how='left')\n",
    "df_test = df_test.merge(hist_count_df,on='card_id',how='left')\n",
    "del hist_count_df\n",
    "gc.collect()\n",
    "new_count_df = get_agg_fea(df_new_merchant_trans,'new_hist')\n",
    "df_train = df_train.merge(new_count_df,on='card_id',how='left')\n",
    "df_test = df_test.merge(new_count_df,on='card_id',how='left')\n",
    "del new_count_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:36.665998Z",
     "start_time": "2019-02-21T12:09:36.367341Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in [df_train,df_test]:\n",
    "    \n",
    "    \n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['first_active_monthtonoew'] = ( max_date-pd.to_datetime(df['first_active_month'])).dt.days\n",
    "\n",
    "    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n",
    "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
    "    df['month'] = df['first_active_month'].dt.month\n",
    "    df['elapsed_time'] = (max_date- df['first_active_month']).dt.days\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['hist_sleep_day'] = df['first_active_monthtonoew'] - df['hist_day_nunique']\n",
    "    df['new_sleep_day'] = df['first_active_monthtonoew'] - df['new_hist_day_nunique']\n",
    "    df['total_sleep_day'] = df['hist_sleep_day'] + df['new_sleep_day'] \n",
    "    df['diff_sleep_day'] = df['hist_sleep_day'] - df['new_sleep_day']\n",
    "   \n",
    "    \n",
    "    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n",
    "                     'new_hist_purchase_date_min']:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "    \n",
    "    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
    "\n",
    "    \n",
    "    df['purchase_amount_diff'] = df['new_hist_purchase_amount_sum']-df['hist_purchase_amount_sum']\n",
    "    df['purchase_amount_rate'] = df['purchase_amount_diff']/df['hist_purchase_amount_sum']\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:37.634038Z",
     "start_time": "2019-02-21T12:09:36.667791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_hist_month_nunique\n",
      "new_hist_day_nunique\n",
      "new_hist_hour_nunique\n",
      "new_hist_weekofyear_nunique\n",
      "new_hist_dayofweek_nunique\n",
      "new_hist_year_nunique\n",
      "new_hist_subsector_id_nunique\n",
      "new_hist_merchant_id_nunique\n",
      "new_hist_merchant_category_id_nunique\n",
      "new_hist_purchase_amount_sum\n",
      "new_hist_purchase_amount_max\n",
      "new_hist_purchase_amount_min\n",
      "new_hist_purchase_amount_mean\n",
      "new_hist_purchase_amount_var\n",
      "new_hist_installments_sum\n",
      "new_hist_installments_max\n",
      "new_hist_installments_min\n",
      "new_hist_installments_mean\n",
      "new_hist_installments_var\n",
      "new_hist_month_lag_max\n",
      "new_hist_month_lag_min\n",
      "new_hist_month_lag_mean\n",
      "new_hist_month_lag_var\n",
      "new_hist_month_diff_max\n",
      "new_hist_month_diff_min\n",
      "new_hist_month_diff_mean\n",
      "new_hist_month_diff_var\n",
      "new_hist_authorized_flag_sum\n",
      "new_hist_authorized_flag_mean\n",
      "new_hist_weekend_sum\n",
      "new_hist_weekend_mean\n",
      "new_hist_category_1_sum\n",
      "new_hist_category_1_mean\n",
      "new_hist_card_id_size\n",
      "new_hist_month_mean_mean\n",
      "new_hist_hour_mean_mean\n",
      "new_hist_weekofyear_mean_mean\n",
      "new_hist_dayofweek_mean_mean\n",
      "new_hist_year_mean_mean\n",
      "new_hist_subsector_id_mean_mean\n",
      "new_hist_merchant_id_mean_mean\n",
      "new_hist_merchant_category_id_mean_mean\n",
      "new_hist_category_1_mean_mean\n",
      "new_hist_category_2_mean_mean\n",
      "new_hist_category_3_mean_mean\n",
      "new_hist_month_lag_mean_mean\n",
      "new_hist_card_id_mean_mean\n",
      "new_hist_purchase_date_diff\n",
      "new_hist_purchase_date_average\n",
      "new_hist_purchase_date_uptonow\n",
      "new_hist_purchase_date_uptomin\n",
      "new_hist_first_buy\n"
     ]
    }
   ],
   "source": [
    "new_hist_feature=[ i for i in df_train  if 'new_hist' in i]\n",
    "\n",
    "for fea in new_hist_feature:\n",
    "    if fea in ['new_hist_purchase_date_max','new_hist_purchase_date_min']:\n",
    "        continue\n",
    "    print(fea)\n",
    "    new_fea=fea\n",
    "    hist_fea=fea.replace(\"new_hist\",'hist')\n",
    "    df_train[new_fea+\"_add_\"+hist_fea]=df_train[new_fea]+df_train[hist_fea]\n",
    "    df_test[new_fea+\"_add_\"+hist_fea]=df_test[new_fea]+df_test[hist_fea]\n",
    "    df_train[new_fea+\"_-_\"+hist_fea]=df_train[new_fea]-df_train[hist_fea]\n",
    "    df_test[new_fea+\"_-_\"+hist_fea]=df_test[new_fea]-df_test[hist_fea]\n",
    "    df_train[new_fea+\"_/_\"+hist_fea]=df_train[new_fea]/(df_train[hist_fea]+0.01)\n",
    "    df_test[new_fea+\"_/_\"+hist_fea]=df_test[new_fea]/(df_test[hist_fea]+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:37.973215Z",
     "start_time": "2019-02-21T12:09:37.635793Z"
    },
    "_uuid": "6f3182aeac0c3bf7a061a1b9e25e859f25fee9b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199710\n",
       "1      2207\n",
       "Name: outliers, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['outliers'] = 0\n",
    "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "df_train['outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:38.004385Z",
     "start_time": "2019-02-21T12:09:37.974883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "      <th>hist_month_nunique</th>\n",
       "      <th>hist_day_nunique</th>\n",
       "      <th>hist_hour_nunique</th>\n",
       "      <th>hist_weekofyear_nunique</th>\n",
       "      <th>...</th>\n",
       "      <th>new_hist_purchase_date_uptonow_add_hist_purchase_date_uptonow</th>\n",
       "      <th>new_hist_purchase_date_uptonow_-_hist_purchase_date_uptonow</th>\n",
       "      <th>new_hist_purchase_date_uptonow_/_hist_purchase_date_uptonow</th>\n",
       "      <th>new_hist_purchase_date_uptomin_add_hist_purchase_date_uptomin</th>\n",
       "      <th>new_hist_purchase_date_uptomin_-_hist_purchase_date_uptomin</th>\n",
       "      <th>new_hist_purchase_date_uptomin_/_hist_purchase_date_uptomin</th>\n",
       "      <th>new_hist_first_buy_add_hist_first_buy</th>\n",
       "      <th>new_hist_first_buy_-_hist_first_buy</th>\n",
       "      <th>new_hist_first_buy_/_hist_first_buy</th>\n",
       "      <th>outliers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820312</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-63.0</td>\n",
       "      <td>0.015623</td>\n",
       "      <td>363.0</td>\n",
       "      <td>-251.0</td>\n",
       "      <td>0.182404</td>\n",
       "      <td>303.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>10.649750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392822</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>0.348275</td>\n",
       "      <td>567.0</td>\n",
       "      <td>-391.0</td>\n",
       "      <td>0.183712</td>\n",
       "      <td>401.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>79.041916</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687988</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>0.032253</td>\n",
       "      <td>476.0</td>\n",
       "      <td>-472.0</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>798.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>3.895467</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142456</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>0.196689</td>\n",
       "      <td>270.0</td>\n",
       "      <td>-162.0</td>\n",
       "      <td>0.249988</td>\n",
       "      <td>212.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>7.477009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159790</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>63.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>0.032782</td>\n",
       "      <td>228.0</td>\n",
       "      <td>-110.0</td>\n",
       "      <td>0.349092</td>\n",
       "      <td>132.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>10.990009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 283 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0         2017-06-01  C_ID_92a2005557          5          2          1   \n",
       "1         2017-01-01  C_ID_3d0044924f          4          1          0   \n",
       "2         2016-08-01  C_ID_d639edf6cd          2          2          0   \n",
       "3         2017-09-01  C_ID_186d6a6901          4          3          0   \n",
       "4         2017-11-01  C_ID_cdbd2c0db2          1          3          0   \n",
       "\n",
       "     target  hist_month_nunique  hist_day_nunique  hist_hour_nunique  \\\n",
       "0 -0.820312                   9                31                 23   \n",
       "1  0.392822                  12                31                 24   \n",
       "2  0.687988                  10                19                 14   \n",
       "3  0.142456                   6                25                 16   \n",
       "4 -0.159790                   4                30                 22   \n",
       "\n",
       "   hist_weekofyear_nunique  ...  \\\n",
       "0                       35  ...   \n",
       "1                       50  ...   \n",
       "2                       22  ...   \n",
       "3                       20  ...   \n",
       "4                       17  ...   \n",
       "\n",
       "   new_hist_purchase_date_uptonow_add_hist_purchase_date_uptonow  \\\n",
       "0                                               65.0               \n",
       "1                                              120.0               \n",
       "2                                               64.0               \n",
       "3                                               73.0               \n",
       "4                                               63.0               \n",
       "\n",
       "   new_hist_purchase_date_uptonow_-_hist_purchase_date_uptonow  \\\n",
       "0                                              -63.0             \n",
       "1                                              -58.0             \n",
       "2                                              -60.0             \n",
       "3                                              -49.0             \n",
       "4                                              -59.0             \n",
       "\n",
       "   new_hist_purchase_date_uptonow_/_hist_purchase_date_uptonow  \\\n",
       "0                                           0.015623             \n",
       "1                                           0.348275             \n",
       "2                                           0.032253             \n",
       "3                                           0.196689             \n",
       "4                                           0.032782             \n",
       "\n",
       "   new_hist_purchase_date_uptomin_add_hist_purchase_date_uptomin  \\\n",
       "0                                              363.0               \n",
       "1                                              567.0               \n",
       "2                                              476.0               \n",
       "3                                              270.0               \n",
       "4                                              228.0               \n",
       "\n",
       "   new_hist_purchase_date_uptomin_-_hist_purchase_date_uptomin  \\\n",
       "0                                             -251.0             \n",
       "1                                             -391.0             \n",
       "2                                             -472.0             \n",
       "3                                             -162.0             \n",
       "4                                             -110.0             \n",
       "\n",
       "   new_hist_purchase_date_uptomin_/_hist_purchase_date_uptomin  \\\n",
       "0                                           0.182404             \n",
       "1                                           0.183712             \n",
       "2                                           0.004219             \n",
       "3                                           0.249988             \n",
       "4                                           0.349092             \n",
       "\n",
       "   new_hist_first_buy_add_hist_first_buy  new_hist_first_buy_-_hist_first_buy  \\\n",
       "0                                  303.0                                251.0   \n",
       "1                                  401.0                                391.0   \n",
       "2                                  798.0                                472.0   \n",
       "3                                  212.0                                162.0   \n",
       "4                                  132.0                                110.0   \n",
       "\n",
       "   new_hist_first_buy_/_hist_first_buy  outliers  \n",
       "0                            10.649750         0  \n",
       "1                            79.041916         0  \n",
       "2                             3.895467         0  \n",
       "3                             7.477009         0  \n",
       "4                            10.990009         0  \n",
       "\n",
       "[5 rows x 283 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:38.091673Z",
     "start_time": "2019-02-21T12:09:38.006041Z"
    },
    "_uuid": "e9f61512c195c66a75dfe220072c5b2d860b78a3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dealing with the one nan in df_test.first_active_month a bit arbitrarily for now\n",
    "try:\n",
    "    df_test.loc[df_test['first_active_month'].isna(),'first_active_month'] = df_test.iloc[11577]['first_active_month']\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:51.064308Z",
     "start_time": "2019-02-21T12:09:38.093324Z"
    },
    "_uuid": "a93c5976d3b395ba8ff0d1002b8075be3e914c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 123.63 Mb (64.4% reduction)\n",
      "Mem. usage decreased to 73.57 Mb (65.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:51.072259Z",
     "start_time": "2019-02-21T12:09:51.065926Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bed_fea = ['new_hist_month_diff_min_-_hist_month_diff_min', 'new_hist_year_mean_mean_/_hist_year_mean_mean', 'new_hist_year_nunique_/_hist_year_nunique', 'new_hist_year_mean_mean_add_hist_year_mean_mean', 'new_hist_merchant_category_id_mean_mean_-_hist_merchant_category_id_mean_mean', 'new_hist_month_lag_mean_mean_-_hist_month_lag_mean_mean', 'new_hist_month_diff_max_add_hist_month_diff_max', 'new_hist_month_diff_min', 'new_hist_weekend_sum_/_hist_weekend_sum', 'new_hist_year_mean_mean', 'new_hist_merchant_id_mean_mean_add_hist_merchant_id_mean_mean', 'new_hist_purchase_amount_sum_-_hist_purchase_amount_sum', 'new_hist_month_diff_var', 'new_hist_month_diff_max_/_hist_month_diff_max', 'new_hist_weekofyear_nunique', 'new_hist_month_diff_max_-_hist_month_diff_max', 'new_hist_weekend_mean_add_hist_weekend_mean', 'new_hist_month_mean_mean_-_hist_month_mean_mean', 'new_hist_month_diff_mean', 'hist_year_nunique', 'new_hist_weekend_mean', 'new_hist_year_mean_mean_-_hist_year_mean_mean', 'hist_month_diff_mean_hist_month_diff_min_ctr', 'hist_dayofweek_nunique', 'new_hist_merchant_category_id_mean_mean', 'new_hist_weekend_sum', 'new_hist_installments_min_-_hist_installments_min', 'new_hist_authorized_flag_mean', 'new_hist_weekend_mean_/_hist_weekend_mean', 'hist_month_diff_min', 'new_hist_card_id_mean_mean_add_hist_card_id_mean_mean', 'new_hist_month_mean_mean_/_hist_month_mean_mean', 'new_hist_category_2_mean_mean', 'new_hist_month_diff_min_/_hist_month_diff_min', 'new_hist_merchant_category_id_mean_mean_add_hist_merchant_category_id_mean_mean', 'new_hist_dayofweek_nunique_/_hist_dayofweek_nunique', 'new_hist_month_mean_mean', 'new_hist_day_nunique_add_hist_day_nunique', 'new_hist_subsector_id_nunique_-_hist_subsector_id_nunique', 'new_hist_purchase_amount_max_add_hist_purchase_amount_max', 'new_hist_month_diff_mean_add_hist_month_diff_mean', 'new_hist_subsector_id_mean_mean', 'new_hist_year_nunique_-_hist_year_nunique', 'new_hist_dayofweek_nunique_-_hist_dayofweek_nunique', 'new_hist_merchant_id_mean_mean_/_hist_merchant_id_mean_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:51.082471Z",
     "start_time": "2019-02-21T12:09:51.074186Z"
    },
    "_uuid": "c4f20f27679889542acfd60d1f1ac381b201ac43",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bed_fea =['new_hist_month_diff_min', 'new_hist_authorized_flag_mean', 'new_hist_year_mean_mean', 'new_hist_year_nunique_/_hist_year_nunique', 'new_hist_year_nunique_-_hist_year_nunique']\n",
    "del_col =  ['card_id', 'first_active_month','target','outliers']+bed_fea\n",
    "\n",
    "features = [c for c in df_train.columns if c not in del_col ]\n",
    "\n",
    "target = df_train['outliers']\n",
    "\n",
    "train_y = target \n",
    "# df_train.drop(columns  = [\"card_id\",\"target\"],inplace=True)\n",
    "# df_test.drop(columns = [\"card_id\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:51.087066Z",
     "start_time": "2019-02-21T12:09:51.084156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    }
   ],
   "source": [
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:51.094726Z",
     "start_time": "2019-02-21T12:09:51.088743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_1',\n",
       " 'feature_2',\n",
       " 'feature_3',\n",
       " 'hist_month_nunique',\n",
       " 'hist_day_nunique',\n",
       " 'hist_hour_nunique',\n",
       " 'hist_weekofyear_nunique',\n",
       " 'hist_subsector_id_nunique',\n",
       " 'hist_merchant_id_nunique',\n",
       " 'hist_merchant_category_id_nunique',\n",
       " 'hist_purchase_amount_sum',\n",
       " 'hist_purchase_amount_max',\n",
       " 'hist_purchase_amount_min',\n",
       " 'hist_purchase_amount_mean',\n",
       " 'hist_purchase_amount_var',\n",
       " 'hist_installments_sum',\n",
       " 'hist_installments_max',\n",
       " 'hist_installments_min',\n",
       " 'hist_installments_mean',\n",
       " 'hist_installments_var',\n",
       " 'hist_purchase_date_max',\n",
       " 'hist_purchase_date_min',\n",
       " 'hist_month_lag_max',\n",
       " 'hist_month_lag_min',\n",
       " 'hist_month_lag_mean',\n",
       " 'hist_month_lag_var',\n",
       " 'hist_month_diff_max',\n",
       " 'hist_month_diff_mean',\n",
       " 'hist_month_diff_var',\n",
       " 'hist_authorized_flag_sum',\n",
       " 'hist_authorized_flag_mean',\n",
       " 'hist_weekend_sum',\n",
       " 'hist_weekend_mean',\n",
       " 'hist_category_1_sum',\n",
       " 'hist_category_1_mean',\n",
       " 'hist_card_id_size',\n",
       " 'hist_month_mean_mean',\n",
       " 'hist_hour_mean_mean',\n",
       " 'hist_weekofyear_mean_mean',\n",
       " 'hist_dayofweek_mean_mean',\n",
       " 'hist_year_mean_mean',\n",
       " 'hist_subsector_id_mean_mean',\n",
       " 'hist_merchant_id_mean_mean',\n",
       " 'hist_merchant_category_id_mean_mean',\n",
       " 'hist_category_1_mean_mean',\n",
       " 'hist_category_2_mean_mean',\n",
       " 'hist_category_3_mean_mean',\n",
       " 'hist_month_lag_mean_mean',\n",
       " 'hist_card_id_mean_mean',\n",
       " 'hist_purchase_date_diff',\n",
       " 'hist_purchase_date_average',\n",
       " 'hist_purchase_date_uptonow',\n",
       " 'hist_purchase_date_uptomin',\n",
       " 'new_hist_month_nunique',\n",
       " 'new_hist_day_nunique',\n",
       " 'new_hist_hour_nunique',\n",
       " 'new_hist_dayofweek_nunique',\n",
       " 'new_hist_year_nunique',\n",
       " 'new_hist_subsector_id_nunique',\n",
       " 'new_hist_merchant_id_nunique',\n",
       " 'new_hist_merchant_category_id_nunique',\n",
       " 'new_hist_purchase_amount_sum',\n",
       " 'new_hist_purchase_amount_max',\n",
       " 'new_hist_purchase_amount_min',\n",
       " 'new_hist_purchase_amount_mean',\n",
       " 'new_hist_purchase_amount_var',\n",
       " 'new_hist_installments_sum',\n",
       " 'new_hist_installments_max',\n",
       " 'new_hist_installments_min',\n",
       " 'new_hist_installments_mean',\n",
       " 'new_hist_installments_var',\n",
       " 'new_hist_purchase_date_max',\n",
       " 'new_hist_purchase_date_min',\n",
       " 'new_hist_month_lag_max',\n",
       " 'new_hist_month_lag_min',\n",
       " 'new_hist_month_lag_mean',\n",
       " 'new_hist_month_lag_var',\n",
       " 'new_hist_month_diff_max',\n",
       " 'new_hist_authorized_flag_sum',\n",
       " 'new_hist_category_1_sum',\n",
       " 'new_hist_category_1_mean',\n",
       " 'new_hist_card_id_size',\n",
       " 'new_hist_hour_mean_mean',\n",
       " 'new_hist_weekofyear_mean_mean',\n",
       " 'new_hist_dayofweek_mean_mean',\n",
       " 'new_hist_merchant_id_mean_mean',\n",
       " 'new_hist_category_1_mean_mean',\n",
       " 'new_hist_category_3_mean_mean',\n",
       " 'new_hist_month_lag_mean_mean',\n",
       " 'new_hist_card_id_mean_mean',\n",
       " 'new_hist_purchase_date_diff',\n",
       " 'new_hist_purchase_date_average',\n",
       " 'new_hist_purchase_date_uptonow',\n",
       " 'new_hist_purchase_date_uptomin',\n",
       " 'first_active_monthtonoew',\n",
       " 'dayofweek',\n",
       " 'weekofyear',\n",
       " 'month',\n",
       " 'elapsed_time',\n",
       " 'hist_first_buy',\n",
       " 'new_hist_first_buy',\n",
       " 'hist_sleep_day',\n",
       " 'new_sleep_day',\n",
       " 'total_sleep_day',\n",
       " 'diff_sleep_day',\n",
       " 'purchase_amount_total',\n",
       " 'purchase_amount_diff',\n",
       " 'purchase_amount_rate',\n",
       " 'new_hist_month_nunique_add_hist_month_nunique',\n",
       " 'new_hist_month_nunique_-_hist_month_nunique',\n",
       " 'new_hist_month_nunique_/_hist_month_nunique',\n",
       " 'new_hist_day_nunique_-_hist_day_nunique',\n",
       " 'new_hist_day_nunique_/_hist_day_nunique',\n",
       " 'new_hist_hour_nunique_add_hist_hour_nunique',\n",
       " 'new_hist_hour_nunique_-_hist_hour_nunique',\n",
       " 'new_hist_hour_nunique_/_hist_hour_nunique',\n",
       " 'new_hist_weekofyear_nunique_add_hist_weekofyear_nunique',\n",
       " 'new_hist_weekofyear_nunique_-_hist_weekofyear_nunique',\n",
       " 'new_hist_weekofyear_nunique_/_hist_weekofyear_nunique',\n",
       " 'new_hist_dayofweek_nunique_add_hist_dayofweek_nunique',\n",
       " 'new_hist_year_nunique_add_hist_year_nunique',\n",
       " 'new_hist_subsector_id_nunique_add_hist_subsector_id_nunique',\n",
       " 'new_hist_subsector_id_nunique_/_hist_subsector_id_nunique',\n",
       " 'new_hist_merchant_id_nunique_add_hist_merchant_id_nunique',\n",
       " 'new_hist_merchant_id_nunique_-_hist_merchant_id_nunique',\n",
       " 'new_hist_merchant_id_nunique_/_hist_merchant_id_nunique',\n",
       " 'new_hist_merchant_category_id_nunique_add_hist_merchant_category_id_nunique',\n",
       " 'new_hist_merchant_category_id_nunique_-_hist_merchant_category_id_nunique',\n",
       " 'new_hist_merchant_category_id_nunique_/_hist_merchant_category_id_nunique',\n",
       " 'new_hist_purchase_amount_sum_add_hist_purchase_amount_sum',\n",
       " 'new_hist_purchase_amount_sum_/_hist_purchase_amount_sum',\n",
       " 'new_hist_purchase_amount_max_-_hist_purchase_amount_max',\n",
       " 'new_hist_purchase_amount_max_/_hist_purchase_amount_max',\n",
       " 'new_hist_purchase_amount_min_add_hist_purchase_amount_min',\n",
       " 'new_hist_purchase_amount_min_-_hist_purchase_amount_min',\n",
       " 'new_hist_purchase_amount_min_/_hist_purchase_amount_min',\n",
       " 'new_hist_purchase_amount_mean_add_hist_purchase_amount_mean',\n",
       " 'new_hist_purchase_amount_mean_-_hist_purchase_amount_mean',\n",
       " 'new_hist_purchase_amount_mean_/_hist_purchase_amount_mean',\n",
       " 'new_hist_purchase_amount_var_add_hist_purchase_amount_var',\n",
       " 'new_hist_purchase_amount_var_-_hist_purchase_amount_var',\n",
       " 'new_hist_purchase_amount_var_/_hist_purchase_amount_var',\n",
       " 'new_hist_installments_sum_add_hist_installments_sum',\n",
       " 'new_hist_installments_sum_-_hist_installments_sum',\n",
       " 'new_hist_installments_sum_/_hist_installments_sum',\n",
       " 'new_hist_installments_max_add_hist_installments_max',\n",
       " 'new_hist_installments_max_-_hist_installments_max',\n",
       " 'new_hist_installments_max_/_hist_installments_max',\n",
       " 'new_hist_installments_min_add_hist_installments_min',\n",
       " 'new_hist_installments_min_/_hist_installments_min',\n",
       " 'new_hist_installments_mean_add_hist_installments_mean',\n",
       " 'new_hist_installments_mean_-_hist_installments_mean',\n",
       " 'new_hist_installments_mean_/_hist_installments_mean',\n",
       " 'new_hist_installments_var_add_hist_installments_var',\n",
       " 'new_hist_installments_var_-_hist_installments_var',\n",
       " 'new_hist_installments_var_/_hist_installments_var',\n",
       " 'new_hist_month_lag_max_add_hist_month_lag_max',\n",
       " 'new_hist_month_lag_max_-_hist_month_lag_max',\n",
       " 'new_hist_month_lag_max_/_hist_month_lag_max',\n",
       " 'new_hist_month_lag_min_add_hist_month_lag_min',\n",
       " 'new_hist_month_lag_min_-_hist_month_lag_min',\n",
       " 'new_hist_month_lag_min_/_hist_month_lag_min',\n",
       " 'new_hist_month_lag_mean_add_hist_month_lag_mean',\n",
       " 'new_hist_month_lag_mean_-_hist_month_lag_mean',\n",
       " 'new_hist_month_lag_mean_/_hist_month_lag_mean',\n",
       " 'new_hist_month_lag_var_add_hist_month_lag_var',\n",
       " 'new_hist_month_lag_var_-_hist_month_lag_var',\n",
       " 'new_hist_month_lag_var_/_hist_month_lag_var',\n",
       " 'new_hist_month_diff_min_add_hist_month_diff_min',\n",
       " 'new_hist_month_diff_mean_-_hist_month_diff_mean',\n",
       " 'new_hist_month_diff_mean_/_hist_month_diff_mean',\n",
       " 'new_hist_month_diff_var_add_hist_month_diff_var',\n",
       " 'new_hist_month_diff_var_-_hist_month_diff_var',\n",
       " 'new_hist_month_diff_var_/_hist_month_diff_var',\n",
       " 'new_hist_authorized_flag_sum_add_hist_authorized_flag_sum',\n",
       " 'new_hist_authorized_flag_sum_-_hist_authorized_flag_sum',\n",
       " 'new_hist_authorized_flag_sum_/_hist_authorized_flag_sum',\n",
       " 'new_hist_authorized_flag_mean_add_hist_authorized_flag_mean',\n",
       " 'new_hist_authorized_flag_mean_-_hist_authorized_flag_mean',\n",
       " 'new_hist_authorized_flag_mean_/_hist_authorized_flag_mean',\n",
       " 'new_hist_weekend_sum_add_hist_weekend_sum',\n",
       " 'new_hist_weekend_sum_-_hist_weekend_sum',\n",
       " 'new_hist_weekend_mean_-_hist_weekend_mean',\n",
       " 'new_hist_category_1_sum_add_hist_category_1_sum',\n",
       " 'new_hist_category_1_sum_-_hist_category_1_sum',\n",
       " 'new_hist_category_1_sum_/_hist_category_1_sum',\n",
       " 'new_hist_category_1_mean_add_hist_category_1_mean',\n",
       " 'new_hist_category_1_mean_-_hist_category_1_mean',\n",
       " 'new_hist_category_1_mean_/_hist_category_1_mean',\n",
       " 'new_hist_card_id_size_add_hist_card_id_size',\n",
       " 'new_hist_card_id_size_-_hist_card_id_size',\n",
       " 'new_hist_card_id_size_/_hist_card_id_size',\n",
       " 'new_hist_month_mean_mean_add_hist_month_mean_mean',\n",
       " 'new_hist_hour_mean_mean_add_hist_hour_mean_mean',\n",
       " 'new_hist_hour_mean_mean_-_hist_hour_mean_mean',\n",
       " 'new_hist_hour_mean_mean_/_hist_hour_mean_mean',\n",
       " 'new_hist_weekofyear_mean_mean_add_hist_weekofyear_mean_mean',\n",
       " 'new_hist_weekofyear_mean_mean_-_hist_weekofyear_mean_mean',\n",
       " 'new_hist_weekofyear_mean_mean_/_hist_weekofyear_mean_mean',\n",
       " 'new_hist_dayofweek_mean_mean_add_hist_dayofweek_mean_mean',\n",
       " 'new_hist_dayofweek_mean_mean_-_hist_dayofweek_mean_mean',\n",
       " 'new_hist_dayofweek_mean_mean_/_hist_dayofweek_mean_mean',\n",
       " 'new_hist_subsector_id_mean_mean_add_hist_subsector_id_mean_mean',\n",
       " 'new_hist_subsector_id_mean_mean_-_hist_subsector_id_mean_mean',\n",
       " 'new_hist_subsector_id_mean_mean_/_hist_subsector_id_mean_mean',\n",
       " 'new_hist_merchant_id_mean_mean_-_hist_merchant_id_mean_mean',\n",
       " 'new_hist_merchant_category_id_mean_mean_/_hist_merchant_category_id_mean_mean',\n",
       " 'new_hist_category_1_mean_mean_add_hist_category_1_mean_mean',\n",
       " 'new_hist_category_1_mean_mean_-_hist_category_1_mean_mean',\n",
       " 'new_hist_category_1_mean_mean_/_hist_category_1_mean_mean',\n",
       " 'new_hist_category_2_mean_mean_add_hist_category_2_mean_mean',\n",
       " 'new_hist_category_2_mean_mean_-_hist_category_2_mean_mean',\n",
       " 'new_hist_category_2_mean_mean_/_hist_category_2_mean_mean',\n",
       " 'new_hist_category_3_mean_mean_add_hist_category_3_mean_mean',\n",
       " 'new_hist_category_3_mean_mean_-_hist_category_3_mean_mean',\n",
       " 'new_hist_category_3_mean_mean_/_hist_category_3_mean_mean',\n",
       " 'new_hist_month_lag_mean_mean_add_hist_month_lag_mean_mean',\n",
       " 'new_hist_month_lag_mean_mean_/_hist_month_lag_mean_mean',\n",
       " 'new_hist_card_id_mean_mean_-_hist_card_id_mean_mean',\n",
       " 'new_hist_card_id_mean_mean_/_hist_card_id_mean_mean',\n",
       " 'new_hist_purchase_date_diff_add_hist_purchase_date_diff',\n",
       " 'new_hist_purchase_date_diff_-_hist_purchase_date_diff',\n",
       " 'new_hist_purchase_date_diff_/_hist_purchase_date_diff',\n",
       " 'new_hist_purchase_date_average_add_hist_purchase_date_average',\n",
       " 'new_hist_purchase_date_average_-_hist_purchase_date_average',\n",
       " 'new_hist_purchase_date_average_/_hist_purchase_date_average',\n",
       " 'new_hist_purchase_date_uptonow_add_hist_purchase_date_uptonow',\n",
       " 'new_hist_purchase_date_uptonow_-_hist_purchase_date_uptonow',\n",
       " 'new_hist_purchase_date_uptonow_/_hist_purchase_date_uptonow',\n",
       " 'new_hist_purchase_date_uptomin_add_hist_purchase_date_uptomin',\n",
       " 'new_hist_purchase_date_uptomin_-_hist_purchase_date_uptomin',\n",
       " 'new_hist_purchase_date_uptomin_/_hist_purchase_date_uptomin',\n",
       " 'new_hist_first_buy_add_hist_first_buy',\n",
       " 'new_hist_first_buy_-_hist_first_buy',\n",
       " 'new_hist_first_buy_/_hist_first_buy']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:51.118921Z",
     "start_time": "2019-02-21T12:09:51.096530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch_norm_decay=0.9\n",
      "  batch_size=129\n",
      "  epoch=1\n",
      "  feature_nums=235\n",
      "  hash_ids=20000\n",
      "  hidden_size=[128, 128]\n",
      "  init_method=uniform\n",
      "  init_value=0.1\n",
      "  k=16\n",
      "  learning_rate=0.001\n",
      "  metric=logloss\n",
      "  model=nffm\n",
      "  norm=True\n",
      "  num_display_steps=1000\n",
      "  num_eval_steps=1000\n",
      "  optimizer=adam\n"
     ]
    }
   ],
   "source": [
    "hparam=tf.contrib.training.HParams(\n",
    "            model='nffm',\n",
    "            norm=True,\n",
    "            batch_norm_decay=0.9,\n",
    "            hidden_size=[128,128],\n",
    "            k=16,\n",
    "            hash_ids=int(2e4),\n",
    "            batch_size= 129,\n",
    "            optimizer=\"adam\",\n",
    "            learning_rate=0.001,\n",
    "            num_display_steps=1000,\n",
    "            num_eval_steps=1000,\n",
    "            epoch=1,\n",
    "            metric= 'logloss',\n",
    "            init_method='uniform',\n",
    "            init_value=0.1,\n",
    "            feature_nums=len(features))\n",
    "utils.print_hparams(hparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:51.123764Z",
     "start_time": "2019-02-21T12:09:51.120554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:09:51.128428Z",
     "start_time": "2019-02-21T12:09:51.125344Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# float_features = []\n",
    "\n",
    "# for i in  features:\n",
    "#     df_train[i].fillna(1e10,inplace=True)\n",
    "#     df_test[i].fillna(1e10,inplace=True)\n",
    "#     df_train[i] = df_train[i].apply(lambda x: round(x)).astype(int)\n",
    "#     df_test[i] = df_test[i].apply(lambda x: round(x)).astype(int)\n",
    "#     print(i,df_train[i].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:10:25.745257Z",
     "start_time": "2019-02-21T12:09:51.130287Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##分桶\n",
    "def make_bucket(data,num=10):\n",
    "    data.sort()\n",
    "    bins=[]\n",
    "    for i in range(num):\n",
    "        bins.append(data[int(len(data)*(i+1)//num)-1])\n",
    "    return bins\n",
    "\n",
    "\n",
    "for f in features:\n",
    "    df_train[f]=df_train[f].fillna(1e10)\n",
    "    df_test[f]=df_test[f].fillna(1e10)\n",
    "    data=list(df_train[f])+list(df_test[f])\n",
    "    bins=make_bucket(data,num=1000)\n",
    "    df_train[f]=np.digitize(df_train[f],bins=bins)\n",
    "    df_test[f]=np.digitize(df_test[f],bins=bins)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T12:10:25.982237Z",
     "start_time": "2019-02-21T12:10:25.746984Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test['outliers'] = 0  #郭大代码的bug 必须传个label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T13:02:59.751215Z",
     "start_time": "2019-02-21T12:10:25.983912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始第 1 折训练\n",
      "# Trainable variables\n",
      "  emb_v1:0, (20000, 1), \n",
      "  emb_v2:0, (20000, 235, 16), \n",
      "  Variable:0, (27495, 128), \n",
      "  norm_0/beta:0, (128,), \n",
      "  norm_0/gamma:0, (128,), \n",
      "  Variable_1:0, (128, 128), \n",
      "  norm_1/beta:0, (128,), \n",
      "  norm_1/gamma:0, (128,), \n",
      "  Variable_2:0, (128, 1), \n",
      "  Variable_3:0, (), \n",
      "  epoch 0 step 1000 lr 0.001 logloss 0.049089 gN 0.13, Thu Feb 21 07:14:50 2019\n",
      "# Epcho-time 260.28s Eval logloss 0.045482. Best logloss 0.045482.\n",
      "# Epcho-time 367.80s Eval logloss 0.082793. Best logloss 0.045482.\n",
      "INFO:tensorflow:Restoring parameters from model_tmp/model\n",
      "# Epcho-time 410.14s Eval logloss 0.078544. Best logloss 0.045482.\n",
      "auc score 0.45777992908722176\n",
      "开始第 2 折训练\n",
      "# Trainable variables\n",
      "  emb_v1:0, (20000, 1), \n",
      "  emb_v2:0, (20000, 235, 16), \n",
      "  Variable:0, (27495, 128), \n",
      "  norm_0/beta:0, (128,), \n",
      "  norm_0/gamma:0, (128,), \n",
      "  Variable_1:0, (128, 128), \n",
      "  norm_1/beta:0, (128,), \n",
      "  norm_1/gamma:0, (128,), \n",
      "  Variable_2:0, (128, 1), \n",
      "  Variable_3:0, (), \n",
      "  epoch 0 step 1000 lr 0.001 logloss 0.049832 gN 0.14, Thu Feb 21 07:25:17 2019\n",
      "# Epcho-time 257.83s Eval logloss 0.046602. Best logloss 0.046602.\n",
      "# Epcho-time 366.31s Eval logloss 0.073617. Best logloss 0.046602.\n",
      "INFO:tensorflow:Restoring parameters from model_tmp/model\n",
      "# Epcho-time 408.99s Eval logloss 0.078337. Best logloss 0.046602.\n",
      "auc score 0.4504440941627804\n",
      "开始第 3 折训练\n",
      "# Trainable variables\n",
      "  emb_v1:0, (20000, 1), \n",
      "  emb_v2:0, (20000, 235, 16), \n",
      "  Variable:0, (27495, 128), \n",
      "  norm_0/beta:0, (128,), \n",
      "  norm_0/gamma:0, (128,), \n",
      "  Variable_1:0, (128, 128), \n",
      "  norm_1/beta:0, (128,), \n",
      "  norm_1/gamma:0, (128,), \n",
      "  Variable_2:0, (128, 1), \n",
      "  Variable_3:0, (), \n",
      "  epoch 0 step 1000 lr 0.001 logloss 0.048846 gN 0.12, Thu Feb 21 07:35:51 2019\n",
      "# Epcho-time 261.69s Eval logloss 0.046900. Best logloss 0.046900.\n",
      "# Epcho-time 371.07s Eval logloss 0.077948. Best logloss 0.046900.\n",
      "INFO:tensorflow:Restoring parameters from model_tmp/model\n",
      "# Epcho-time 413.56s Eval logloss 0.074469. Best logloss 0.046900.\n",
      "auc score 0.584132167379662\n",
      "开始第 4 折训练\n",
      "# Trainable variables\n",
      "  emb_v1:0, (20000, 1), \n",
      "  emb_v2:0, (20000, 235, 16), \n",
      "  Variable:0, (27495, 128), \n",
      "  norm_0/beta:0, (128,), \n",
      "  norm_0/gamma:0, (128,), \n",
      "  Variable_1:0, (128, 128), \n",
      "  norm_1/beta:0, (128,), \n",
      "  norm_1/gamma:0, (128,), \n",
      "  Variable_2:0, (128, 1), \n",
      "  Variable_3:0, (), \n",
      "  epoch 0 step 1000 lr 0.001 logloss 0.049287 gN 0.12, Thu Feb 21 07:46:24 2019\n",
      "# Epcho-time 261.84s Eval logloss 0.046526. Best logloss 0.046526.\n",
      "# Epcho-time 371.27s Eval logloss 0.083541. Best logloss 0.046526.\n",
      "INFO:tensorflow:Restoring parameters from model_tmp/model\n",
      "# Epcho-time 413.98s Eval logloss 0.085486. Best logloss 0.046526.\n",
      "auc score 0.5372093958007819\n",
      "开始第 5 折训练\n",
      "# Trainable variables\n",
      "  emb_v1:0, (20000, 1), \n",
      "  emb_v2:0, (20000, 235, 16), \n",
      "  Variable:0, (27495, 128), \n",
      "  norm_0/beta:0, (128,), \n",
      "  norm_0/gamma:0, (128,), \n",
      "  Variable_1:0, (128, 128), \n",
      "  norm_1/beta:0, (128,), \n",
      "  norm_1/gamma:0, (128,), \n",
      "  Variable_2:0, (128, 1), \n",
      "  Variable_3:0, (), \n",
      "  epoch 0 step 1000 lr 0.001 logloss 0.048943 gN 0.13, Thu Feb 21 07:56:53 2019\n",
      "# Epcho-time 258.45s Eval logloss 0.046916. Best logloss 0.046916.\n",
      "# Epcho-time 366.61s Eval logloss 0.084362. Best logloss 0.046916.\n",
      "INFO:tensorflow:Restoring parameters from model_tmp/model\n",
      "# Epcho-time 409.16s Eval logloss 0.098377. Best logloss 0.046916.\n",
      "auc score 0.5136549186797046\n",
      "mean score 0.5086441010220301\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import  StratifiedKFold\n",
    "from sklearn import  metrics\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=4590)\n",
    "oof_train = np.zeros((len(df_train),1))\n",
    "oof_test = np.zeros((len(df_test),1))\n",
    "score_csv = []\n",
    "for fold,(trn_idx,val_idx) in enumerate (skf.split(df_train,df_train['outliers'])):\n",
    "    print(\"开始第 %d 折训练\"%(fold+1))\n",
    "    x_tr,x_val = df_train.iloc[trn_idx][features],df_train.iloc[val_idx][features]\n",
    "    y_tr,y_val = df_train.iloc[trn_idx]['outliers'],df_train.iloc[val_idx]['outliers']\n",
    "    model = ctrNet.build_model(hparam)\n",
    "    model.train(train_data = (x_tr,y_tr),dev_data=(x_val,y_val))\n",
    "    del x_tr,y_tr\n",
    "    gc.collect()\n",
    "    predictions = model.infer(dev_data=(x_val,y_val))\n",
    "    oof_train[val_idx] = predictions.reshape(-1,1)\n",
    "    false_positive_rate, recall, thresholds = metrics.roc_curve(y_val, predictions)\n",
    "    score = metrics.auc(false_positive_rate, recall)\n",
    "    print(\"auc score %s\"%score)\n",
    "    score_csv.append(score)\n",
    "    del x_val,y_val\n",
    "    gc.collect()\n",
    "    oof_test = model.infer(dev_data=(df_test[features],df_test['outliers'])).reshape(-1,1)/5\n",
    "    \n",
    "mean_score = np.mean(score_csv)\n",
    "print(\"mean score %s\"%mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T13:02:59.773564Z",
     "start_time": "2019-02-21T12:02:56.292Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_prob=pd.DataFrame(oof_train)\n",
    "train_prob.columns=['class1']\n",
    "train_prob.to_csv(\"class_oof/train_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "test_prob=pd.DataFrame(oof_test)\n",
    "test_prob.columns=['class1']\n",
    "test_prob.to_csv(\"class_oof/test_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "np.save(\"train_y\",target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
