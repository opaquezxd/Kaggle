{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:50:45.302519Z",
     "start_time": "2019-02-22T02:50:44.319730Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date, datetime\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(4590)\n",
    "\n",
    "skf= StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "\n",
    "# by feature select \n",
    "FILTER_FEATURE = ['new_hist_month_diff_min_-_hist_month_diff_min', 'new_hist_merchant_category_id_mean_mean_add_hist_merchant_category_id_mean_mean', 'new_hist_month_lag_mean_mean_add_hist_month_lag_mean_mean', 'hist_year_nunique', 'new_hist_month_diff_max_/_hist_month_diff_max', 'new_hist_merchant_id_mean_mean_add_hist_merchant_id_mean_mean', 'new_hist_month_diff_min_/_hist_month_diff_min', 'new_hist_year_mean_mean_add_hist_year_mean_mean', 'new_hist_dayofweek_mean_mean_/_hist_dayofweek_mean_mean', 'new_hist_weekofyear_mean_mean_add_hist_weekofyear_mean_mean', 'new_hist_dayofweek_nunique_/_hist_dayofweek_nunique', 'new_hist_month_diff_max', 'new_hist_year_mean_mean_/_hist_year_mean_mean', 'new_hist_merchant_category_id_mean_mean_/_hist_merchant_category_id_mean_mean', 'new_hist_merchant_id_mean_mean_/_hist_merchant_id_mean_mean', 'new_hist_year_nunique_/_hist_year_nunique', 'new_hist_month_diff_min', 'new_hist_merchant_category_id_mean_mean', 'hist_month_diff_mean_hist_month_diff_min_ctr', 'new_hist_year_nunique_-_hist_year_nunique', 'dayofweek', 'new_hist_month_mean_mean_/_hist_month_mean_mean', 'new_hist_month_diff_var', 'new_hist_installments_min_-_hist_installments_min', 'new_hist_subsector_id_mean_mean', 'new_hist_dayofweek_nunique', 'new_hist_weekend_mean', 'new_hist_category_2_mean_mean_/_hist_category_2_mean_mean', 'new_hist_merchant_category_id_nunique_-_hist_merchant_category_id_nunique', 'new_hist_month_diff_mean_add_hist_month_diff_mean', 'new_hist_authorized_flag_mean', 'new_hist_merchant_category_id_mean_mean_-_hist_merchant_category_id_mean_mean', 'new_hist_year_mean_mean_-_hist_year_mean_mean', 'new_hist_category_2_mean_mean', 'new_hist_card_id_mean_mean_add_hist_card_id_mean_mean', 'new_hist_month_diff_max_-_hist_month_diff_max', 'new_hist_weekend_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:52:12.433224Z",
     "start_time": "2019-02-22T02:50:45.304533Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_test = pd.read_csv('../input/test.csv')\n",
    "df_hist_trans = pd.read_csv('../input/historical_transactions.csv',parse_dates=['purchase_date'])\n",
    "df_new_merchant_trans = pd.read_csv('../input/new_merchant_transactions.csv',parse_dates=['purchase_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:52:12.442877Z",
     "start_time": "2019-02-22T02:52:12.435112Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_date=df_new_merchant_trans['purchase_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:52:12.508698Z",
     "start_time": "2019-02-22T02:52:12.444568Z"
    },
    "_uuid": "520b71064293a20e0f2a379dad0acc274374a3c4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:52:18.819125Z",
     "start_time": "2019-02-22T02:52:12.510561Z"
    },
    "_uuid": "44198b73053869f8dfc083204c592fe9193f239c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n",
      "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n",
      "Mem. usage decreased to 1749.11 Mb (43.7% reduction)\n",
      "Mem. usage decreased to 114.20 Mb (45.5% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "df_hist_trans = reduce_mem_usage(df_hist_trans)\n",
    "df_new_merchant_trans = reduce_mem_usage(df_new_merchant_trans)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:52:18.835818Z",
     "start_time": "2019-02-22T02:52:18.820901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0            2017-06  C_ID_92a2005557          5          2          1   \n",
       "1            2017-01  C_ID_3d0044924f          4          1          0   \n",
       "2            2016-08  C_ID_d639edf6cd          2          2          0   \n",
       "3            2017-09  C_ID_186d6a6901          4          3          0   \n",
       "4            2017-11  C_ID_cdbd2c0db2          1          3          0   \n",
       "\n",
       "     target  \n",
       "0 -0.820312  \n",
       "1  0.392822  \n",
       "2  0.687988  \n",
       "3  0.142456  \n",
       "4 -0.159790  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:52:18.840580Z",
     "start_time": "2019-02-22T02:52:18.837432Z"
    },
    "_uuid": "dda90662d05e22310dd713df106ea07f4b8bccfc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:52:37.607323Z",
     "start_time": "2019-02-22T02:52:18.842515Z"
    },
    "_uuid": "7c91d3b9e9dbaff01962b0facbace75705a9ce18",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for df in [df_hist_trans,df_new_merchant_trans]:\n",
    "#     df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['year'] = df['purchase_date'].dt.year\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
    "    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n",
    "    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n",
    "    df['month_diff'] = ((max_date - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "#     df['authorized_flag_purchase_amount'] = df.apply(lambda x: x['purchase_amount'] \\\n",
    "#                             if x['authorized_flag']>0 else 0 ,axis=1)\n",
    "    \n",
    "#     df['unauthorized_flag_purchase_amount'] = df.apply(lambda x: x['purchase_amount'] \\\n",
    "#                             if x['authorized_flag']<1 else 0 ,axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:56:28.478412Z",
     "start_time": "2019-02-22T02:52:37.609554Z"
    },
    "_uuid": "ddf1d5bb0ade2b22b0f072c208c1506ea64503ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def  get_agg_fea(count_df,prefix):\n",
    "    aggs = {}\n",
    "    for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "    aggs['installments'] = ['sum','max','min','mean','var']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var']\n",
    "    # aggs['month_diff'] = ['mean']\n",
    "    aggs['month_diff'] = ['max','min','mean','var']\n",
    "    aggs['authorized_flag'] = ['sum', 'mean']\n",
    "    aggs['weekend'] = ['sum', 'mean']\n",
    "    aggs['category_1'] = ['sum', 'mean']\n",
    "    aggs['card_id'] = ['size']\n",
    "\n",
    "    for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id',\\\n",
    "                'category_1','category_2','category_3','month_lag','card_id']:\n",
    "        count_df[col+'_mean'] = count_df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        aggs[col+'_mean'] = ['mean']   \n",
    "\n",
    "    new_columns = get_new_columns(prefix,aggs)\n",
    "    count_df_gp = count_df.groupby('card_id').agg(aggs)\n",
    "    count_df_gp.columns = new_columns\n",
    "    count_df_gp.reset_index(drop=False,inplace=True)\n",
    "    count_df_gp['%s_purchase_date_diff'%prefix] = (count_df_gp['%s_purchase_date_max'%prefix] - count_df_gp['%s_purchase_date_min'%prefix]).dt.days\n",
    "    count_df_gp['%s_purchase_date_average'%prefix] = count_df_gp['%s_purchase_date_diff'%prefix]/count_df_gp['%s_card_id_size'%prefix]\n",
    "    count_df_gp['%s_purchase_date_uptonow'%prefix] = (max_date - count_df_gp['%s_purchase_date_max'%prefix]).dt.days\n",
    "    return count_df_gp\n",
    "\n",
    "\n",
    "\n",
    "hist_count_df=get_agg_fea(df_hist_trans,'hist')\n",
    "df_train = df_train.merge(hist_count_df,on='card_id',how='left')\n",
    "df_test = df_test.merge(hist_count_df,on='card_id',how='left')\n",
    "del hist_count_df\n",
    "gc.collect()\n",
    "new_count_df = get_agg_fea(df_new_merchant_trans,'new_hist')\n",
    "df_train = df_train.merge(new_count_df,on='card_id',how='left')\n",
    "df_test = df_test.merge(new_count_df,on='card_id',how='left')\n",
    "del new_count_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:56:29.239954Z",
     "start_time": "2019-02-22T02:56:28.480196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_hist_month_nunique\n",
      "new_hist_hour_nunique\n",
      "new_hist_weekofyear_nunique\n",
      "new_hist_dayofweek_nunique\n",
      "new_hist_year_nunique\n",
      "new_hist_subsector_id_nunique\n",
      "new_hist_merchant_id_nunique\n",
      "new_hist_merchant_category_id_nunique\n",
      "new_hist_purchase_amount_sum\n",
      "new_hist_purchase_amount_max\n",
      "new_hist_purchase_amount_min\n",
      "new_hist_purchase_amount_mean\n",
      "new_hist_purchase_amount_var\n",
      "new_hist_installments_sum\n",
      "new_hist_installments_max\n",
      "new_hist_installments_min\n",
      "new_hist_installments_mean\n",
      "new_hist_installments_var\n",
      "new_hist_month_lag_max\n",
      "new_hist_month_lag_min\n",
      "new_hist_month_lag_mean\n",
      "new_hist_month_lag_var\n",
      "new_hist_month_diff_max\n",
      "new_hist_month_diff_min\n",
      "new_hist_month_diff_mean\n",
      "new_hist_month_diff_var\n",
      "new_hist_authorized_flag_sum\n",
      "new_hist_authorized_flag_mean\n",
      "new_hist_weekend_sum\n",
      "new_hist_weekend_mean\n",
      "new_hist_category_1_sum\n",
      "new_hist_category_1_mean\n",
      "new_hist_card_id_size\n",
      "new_hist_month_mean_mean\n",
      "new_hist_hour_mean_mean\n",
      "new_hist_weekofyear_mean_mean\n",
      "new_hist_dayofweek_mean_mean\n",
      "new_hist_year_mean_mean\n",
      "new_hist_subsector_id_mean_mean\n",
      "new_hist_merchant_id_mean_mean\n",
      "new_hist_merchant_category_id_mean_mean\n",
      "new_hist_category_1_mean_mean\n",
      "new_hist_category_2_mean_mean\n",
      "new_hist_category_3_mean_mean\n",
      "new_hist_month_lag_mean_mean\n",
      "new_hist_card_id_mean_mean\n",
      "new_hist_purchase_date_diff\n",
      "new_hist_purchase_date_average\n",
      "new_hist_purchase_date_uptonow\n"
     ]
    }
   ],
   "source": [
    "new_hist_feature=[ i for i in df_train  if 'new_hist' in i]\n",
    "\n",
    "for fea in new_hist_feature:\n",
    "    if fea in ['new_hist_purchase_date_max','new_hist_purchase_date_min']:\n",
    "        continue\n",
    "    print(fea)\n",
    "    new_fea=fea\n",
    "    hist_fea=fea.replace(\"new_hist\",'hist')\n",
    "    df_train[new_fea+\"_add_\"+hist_fea]=df_train[new_fea]+df_train[hist_fea]\n",
    "    df_test[new_fea+\"_add_\"+hist_fea]=df_test[new_fea]+df_test[hist_fea]\n",
    "    df_train[new_fea+\"_-_\"+hist_fea]=df_train[new_fea]-df_train[hist_fea]\n",
    "    df_test[new_fea+\"_-_\"+hist_fea]=df_test[new_fea]-df_test[hist_fea]\n",
    "    df_train[new_fea+\"_/_\"+hist_fea]=df_train[new_fea]/(df_train[hist_fea]+0.1)\n",
    "    df_test[new_fea+\"_/_\"+hist_fea]=df_test[new_fea]/(df_test[hist_fea]+0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:56:29.275718Z",
     "start_time": "2019-02-22T02:56:29.241836Z"
    },
    "_uuid": "a075cc90ab1322829e4fad3ff39fce307c5db93c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "      <th>hist_month_nunique</th>\n",
       "      <th>hist_hour_nunique</th>\n",
       "      <th>hist_weekofyear_nunique</th>\n",
       "      <th>hist_dayofweek_nunique</th>\n",
       "      <th>...</th>\n",
       "      <th>new_hist_card_id_mean_mean_/_hist_card_id_mean_mean</th>\n",
       "      <th>new_hist_purchase_date_diff_add_hist_purchase_date_diff</th>\n",
       "      <th>new_hist_purchase_date_diff_-_hist_purchase_date_diff</th>\n",
       "      <th>new_hist_purchase_date_diff_/_hist_purchase_date_diff</th>\n",
       "      <th>new_hist_purchase_date_average_add_hist_purchase_date_average</th>\n",
       "      <th>new_hist_purchase_date_average_-_hist_purchase_date_average</th>\n",
       "      <th>new_hist_purchase_date_average_/_hist_purchase_date_average</th>\n",
       "      <th>new_hist_purchase_date_uptonow_add_hist_purchase_date_uptonow</th>\n",
       "      <th>new_hist_purchase_date_uptonow_-_hist_purchase_date_uptonow</th>\n",
       "      <th>new_hist_purchase_date_uptonow_/_hist_purchase_date_uptonow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820312</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.069365</td>\n",
       "      <td>296.0</td>\n",
       "      <td>-188.0</td>\n",
       "      <td>0.223048</td>\n",
       "      <td>3.278595</td>\n",
       "      <td>1.417057</td>\n",
       "      <td>2.277742</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-63.0</td>\n",
       "      <td>0.015601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392822</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.452096</td>\n",
       "      <td>446.0</td>\n",
       "      <td>-334.0</td>\n",
       "      <td>0.143553</td>\n",
       "      <td>10.447619</td>\n",
       "      <td>8.219048</td>\n",
       "      <td>7.686275</td>\n",
       "      <td>120.0</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>0.347924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687988</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.210758</td>\n",
       "      <td>412.0</td>\n",
       "      <td>-412.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.581395</td>\n",
       "      <td>-9.581395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>0.032206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142456</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.225325</td>\n",
       "      <td>195.0</td>\n",
       "      <td>-113.0</td>\n",
       "      <td>0.266061</td>\n",
       "      <td>7.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>2.789116</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>0.196399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159790</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2.081052</td>\n",
       "      <td>165.0</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>0.527290</td>\n",
       "      <td>2.395363</td>\n",
       "      <td>0.771303</td>\n",
       "      <td>1.736054</td>\n",
       "      <td>63.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>0.032733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0            2017-06  C_ID_92a2005557          5          2          1   \n",
       "1            2017-01  C_ID_3d0044924f          4          1          0   \n",
       "2            2016-08  C_ID_d639edf6cd          2          2          0   \n",
       "3            2017-09  C_ID_186d6a6901          4          3          0   \n",
       "4            2017-11  C_ID_cdbd2c0db2          1          3          0   \n",
       "\n",
       "     target  hist_month_nunique  hist_hour_nunique  hist_weekofyear_nunique  \\\n",
       "0 -0.820312                   9                 23                       35   \n",
       "1  0.392822                  12                 24                       50   \n",
       "2  0.687988                  10                 14                       22   \n",
       "3  0.142456                   6                 16                       20   \n",
       "4 -0.159790                   4                 22                       17   \n",
       "\n",
       "   hist_dayofweek_nunique  ...  \\\n",
       "0                       7  ...   \n",
       "1                       7  ...   \n",
       "2                       7  ...   \n",
       "3                       7  ...   \n",
       "4                       7  ...   \n",
       "\n",
       "   new_hist_card_id_mean_mean_/_hist_card_id_mean_mean  \\\n",
       "0                                           1.069365     \n",
       "1                                           1.452096     \n",
       "2                                           1.210758     \n",
       "3                                           1.225325     \n",
       "4                                           2.081052     \n",
       "\n",
       "   new_hist_purchase_date_diff_add_hist_purchase_date_diff  \\\n",
       "0                                              296.0         \n",
       "1                                              446.0         \n",
       "2                                              412.0         \n",
       "3                                              195.0         \n",
       "4                                              165.0         \n",
       "\n",
       "   new_hist_purchase_date_diff_-_hist_purchase_date_diff  \\\n",
       "0                                             -188.0       \n",
       "1                                             -334.0       \n",
       "2                                             -412.0       \n",
       "3                                             -113.0       \n",
       "4                                              -51.0       \n",
       "\n",
       "   new_hist_purchase_date_diff_/_hist_purchase_date_diff  \\\n",
       "0                                           0.223048       \n",
       "1                                           0.143553       \n",
       "2                                           0.000000       \n",
       "3                                           0.266061       \n",
       "4                                           0.527290       \n",
       "\n",
       "   new_hist_purchase_date_average_add_hist_purchase_date_average  \\\n",
       "0                                           3.278595               \n",
       "1                                          10.447619               \n",
       "2                                           9.581395               \n",
       "3                                           7.857143               \n",
       "4                                           2.395363               \n",
       "\n",
       "   new_hist_purchase_date_average_-_hist_purchase_date_average  \\\n",
       "0                                           1.417057             \n",
       "1                                           8.219048             \n",
       "2                                          -9.581395             \n",
       "3                                           3.857143             \n",
       "4                                           0.771303             \n",
       "\n",
       "   new_hist_purchase_date_average_/_hist_purchase_date_average  \\\n",
       "0                                           2.277742             \n",
       "1                                           7.686275             \n",
       "2                                           0.000000             \n",
       "3                                           2.789116             \n",
       "4                                           1.736054             \n",
       "\n",
       "   new_hist_purchase_date_uptonow_add_hist_purchase_date_uptonow  \\\n",
       "0                                               65.0               \n",
       "1                                              120.0               \n",
       "2                                               64.0               \n",
       "3                                               73.0               \n",
       "4                                               63.0               \n",
       "\n",
       "   new_hist_purchase_date_uptonow_-_hist_purchase_date_uptonow  \\\n",
       "0                                              -63.0             \n",
       "1                                              -58.0             \n",
       "2                                              -60.0             \n",
       "3                                              -49.0             \n",
       "4                                              -59.0             \n",
       "\n",
       "   new_hist_purchase_date_uptonow_/_hist_purchase_date_uptonow  \n",
       "0                                           0.015601            \n",
       "1                                           0.347924            \n",
       "2                                           0.032206            \n",
       "3                                           0.196399            \n",
       "4                                           0.032733            \n",
       "\n",
       "[5 rows x 255 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del df_hist_trans;\n",
    "# gc.collect()\n",
    "\n",
    "# del df_new_merchant_trans;\n",
    "# gc.collect()\n",
    "\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:56:29.423658Z",
     "start_time": "2019-02-22T02:56:29.277376Z"
    },
    "_uuid": "6f3182aeac0c3bf7a061a1b9e25e859f25fee9b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199710\n",
       "1      2207\n",
       "Name: outliers, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['outliers'] = 0\n",
    "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "df_train['outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:56:55.034438Z",
     "start_time": "2019-02-22T02:56:29.425514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test ctr caluate finished\n",
      "df_train ctr caluate finished\n"
     ]
    }
   ],
   "source": [
    "def get_data_ctr_fea(tj_data,self_data,items=['feature_1','feature_2','feature_3','new_hist_month_diff_mean',\n",
    "                                          'hist_month_diff_mean', 'hist_month_diff_max','hist_month_diff_min' ]):\n",
    "\n",
    "    items = items\n",
    "    \n",
    "    tj_drop_columns=[i for i in  tj_data.columns  if \"_ctr\" in i]\n",
    "    if len(tj_drop_columns)>0:\n",
    "        tj_data=tj_data.drop(columns=tj_drop_columns)\n",
    "    \n",
    "    self_drop_columns=[i for i in  self_data  if \"_ctr\" in i]\n",
    "    if len(self_drop_columns)>0:\n",
    "        print(self_drop_columns)\n",
    "        self_data.drop(columns=self_drop_columns,inplace=True)\n",
    "        \n",
    "    \n",
    "    for item in items:\n",
    "        if type(item)==list:\n",
    "            pr_name=\"_\".join(item)\n",
    "            merge_columns=item+[pr_name+'_ctr']\n",
    "        else:\n",
    "            pr_name=item\n",
    "            merge_columns=[item,pr_name+'_ctr']\n",
    "        temp = tj_data.groupby(item, as_index = False)['outliers'].agg({pr_name+'_click':'sum',pr_name+'_count':'count'})\n",
    "        temp[pr_name+'_ctr'] =1000* (temp[pr_name+'_click']+0.01)/(temp[pr_name+'_count']+0.01)\n",
    "        \n",
    "        self_data = pd.merge(self_data, temp[merge_columns], on=item, how='left')\n",
    "        \n",
    "    for i in range(len(items)):\n",
    "        for j in range(i+1, len(items)):\n",
    "            item_g = [items[i], items[j]]\n",
    "            merge_columns=item_g+['_'.join(item_g)+'_ctr']\n",
    "            temp = tj_data.groupby(item_g, as_index=False)['outliers'].agg({'_'.join(item_g)+'_click': 'sum','_'.join(item_g)+'count':'count'})\n",
    "            temp['_'.join(item_g)+'_ctr'] =1000* (temp['_'.join(item_g)+'_click']+0.01)/(temp['_'.join(item_g)+'count']+0.01)\n",
    "            self_data = pd.merge(self_data, temp[merge_columns], on=item_g, how='left')\n",
    "            \n",
    "    return self_data\n",
    "            \n",
    "df_test=get_data_ctr_fea(df_train,df_test)  \n",
    "print(\"df_test ctr caluate finished\")\n",
    "\n",
    "from sklearn.model_selection import  StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=4590, shuffle=True)\n",
    "\n",
    "df_train['index'] = [i for i in range(len(df_train))]\n",
    "\n",
    "for k, (df_train_in, df_test_in) in enumerate(skf.split(df_train, df_train.outliers.values)):\n",
    "    df_train_df=df_train.iloc[df_train_in]\n",
    "    val_df=df_train.iloc[df_test_in]\n",
    "    val_df=get_data_ctr_fea(df_train_df,val_df)\n",
    "    if k==0:\n",
    "        new_df_train_df=val_df\n",
    "    else:\n",
    "        new_df_train_df=pd.concat([new_df_train_df,val_df])\n",
    "\n",
    "df_train=new_df_train_df\n",
    "df_train.sort_values(by='index',inplace=True)\n",
    "df_train.drop(columns = ['index'],inplace=True)\n",
    "\n",
    "print(\"df_train ctr caluate finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:56:55.064659Z",
     "start_time": "2019-02-22T02:56:55.036491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "      <th>hist_month_nunique</th>\n",
       "      <th>hist_hour_nunique</th>\n",
       "      <th>hist_weekofyear_nunique</th>\n",
       "      <th>hist_dayofweek_nunique</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_3_new_hist_month_diff_mean_ctr</th>\n",
       "      <th>feature_3_hist_month_diff_mean_ctr</th>\n",
       "      <th>feature_3_hist_month_diff_max_ctr</th>\n",
       "      <th>feature_3_hist_month_diff_min_ctr</th>\n",
       "      <th>new_hist_month_diff_mean_hist_month_diff_mean_ctr</th>\n",
       "      <th>new_hist_month_diff_mean_hist_month_diff_max_ctr</th>\n",
       "      <th>new_hist_month_diff_mean_hist_month_diff_min_ctr</th>\n",
       "      <th>hist_month_diff_mean_hist_month_diff_max_ctr</th>\n",
       "      <th>hist_month_diff_mean_hist_month_diff_min_ctr</th>\n",
       "      <th>hist_month_diff_max_hist_month_diff_min_ctr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820312</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2.703738</td>\n",
       "      <td>9.900990</td>\n",
       "      <td>3.724646</td>\n",
       "      <td>3.625798</td>\n",
       "      <td>9.900990</td>\n",
       "      <td>2.778752</td>\n",
       "      <td>2.731318</td>\n",
       "      <td>3.322259</td>\n",
       "      <td>9.900990</td>\n",
       "      <td>3.909765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392822</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>10.125030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.765042</td>\n",
       "      <td>3.082772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.680257</td>\n",
       "      <td>11.453054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.906786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687988</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.868991</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>2.876681</td>\n",
       "      <td>3.068904</td>\n",
       "      <td>0.086949</td>\n",
       "      <td>2.707610</td>\n",
       "      <td>2.773063</td>\n",
       "      <td>0.072987</td>\n",
       "      <td>0.083326</td>\n",
       "      <td>3.923032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-09</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142456</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.792096</td>\n",
       "      <td>0.357015</td>\n",
       "      <td>2.856462</td>\n",
       "      <td>0.868094</td>\n",
       "      <td>16.287696</td>\n",
       "      <td>2.778752</td>\n",
       "      <td>0.749600</td>\n",
       "      <td>14.025830</td>\n",
       "      <td>0.666223</td>\n",
       "      <td>1.170184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159790</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.868991</td>\n",
       "      <td>0.999001</td>\n",
       "      <td>2.876681</td>\n",
       "      <td>0.872406</td>\n",
       "      <td>0.768640</td>\n",
       "      <td>2.707610</td>\n",
       "      <td>0.750575</td>\n",
       "      <td>0.624610</td>\n",
       "      <td>1.426534</td>\n",
       "      <td>1.013272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0            2017-06  C_ID_92a2005557          5          2          1   \n",
       "1            2017-01  C_ID_3d0044924f          4          1          0   \n",
       "0            2016-08  C_ID_d639edf6cd          2          2          0   \n",
       "2            2017-09  C_ID_186d6a6901          4          3          0   \n",
       "1            2017-11  C_ID_cdbd2c0db2          1          3          0   \n",
       "\n",
       "     target  hist_month_nunique  hist_hour_nunique  hist_weekofyear_nunique  \\\n",
       "0 -0.820312                   9                 23                       35   \n",
       "1  0.392822                  12                 24                       50   \n",
       "0  0.687988                  10                 14                       22   \n",
       "2  0.142456                   6                 16                       20   \n",
       "1 -0.159790                   4                 22                       17   \n",
       "\n",
       "   hist_dayofweek_nunique  ...  feature_3_new_hist_month_diff_mean_ctr  \\\n",
       "0                       7  ...                                2.703738   \n",
       "1                       7  ...                               10.125030   \n",
       "0                       7  ...                                1.868991   \n",
       "2                       7  ...                                1.792096   \n",
       "1                       7  ...                                1.868991   \n",
       "\n",
       "   feature_3_hist_month_diff_mean_ctr  feature_3_hist_month_diff_max_ctr  \\\n",
       "0                            9.900990                           3.724646   \n",
       "1                                 NaN                          18.765042   \n",
       "0                            0.185151                           2.876681   \n",
       "2                            0.357015                           2.856462   \n",
       "1                            0.999001                           2.876681   \n",
       "\n",
       "   feature_3_hist_month_diff_min_ctr  \\\n",
       "0                           3.625798   \n",
       "1                           3.082772   \n",
       "0                           3.068904   \n",
       "2                           0.868094   \n",
       "1                           0.872406   \n",
       "\n",
       "   new_hist_month_diff_mean_hist_month_diff_mean_ctr  \\\n",
       "0                                           9.900990   \n",
       "1                                                NaN   \n",
       "0                                           0.086949   \n",
       "2                                          16.287696   \n",
       "1                                           0.768640   \n",
       "\n",
       "   new_hist_month_diff_mean_hist_month_diff_max_ctr  \\\n",
       "0                                          2.778752   \n",
       "1                                         11.680257   \n",
       "0                                          2.707610   \n",
       "2                                          2.778752   \n",
       "1                                          2.707610   \n",
       "\n",
       "   new_hist_month_diff_mean_hist_month_diff_min_ctr  \\\n",
       "0                                          2.731318   \n",
       "1                                         11.453054   \n",
       "0                                          2.773063   \n",
       "2                                          0.749600   \n",
       "1                                          0.750575   \n",
       "\n",
       "   hist_month_diff_mean_hist_month_diff_max_ctr  \\\n",
       "0                                      3.322259   \n",
       "1                                           NaN   \n",
       "0                                      0.072987   \n",
       "2                                     14.025830   \n",
       "1                                      0.624610   \n",
       "\n",
       "   hist_month_diff_mean_hist_month_diff_min_ctr  \\\n",
       "0                                      9.900990   \n",
       "1                                           NaN   \n",
       "0                                      0.083326   \n",
       "2                                      0.666223   \n",
       "1                                      1.426534   \n",
       "\n",
       "   hist_month_diff_max_hist_month_diff_min_ctr  \n",
       "0                                     3.909765  \n",
       "1                                    14.906786  \n",
       "0                                     3.923032  \n",
       "2                                     1.170184  \n",
       "1                                     1.013272  \n",
       "\n",
       "[5 rows x 284 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:56:55.175295Z",
     "start_time": "2019-02-22T02:56:55.066288Z"
    },
    "_uuid": "e9f61512c195c66a75dfe220072c5b2d860b78a3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dealing with the one nan in df_test.first_active_month a bit arbitrarily for now\n",
    "df_test.loc[df_test['first_active_month'].isna(),'first_active_month'] = df_test.iloc[11577]['first_active_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:56:55.432859Z",
     "start_time": "2019-02-22T02:56:55.177176Z"
    },
    "_uuid": "ce2082fc1fb0e3f8f7d27fc166aa7a8351b65504",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in [df_train,df_test]:\n",
    "    \n",
    "    \n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n",
    "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
    "    df['month'] = df['first_active_month'].dt.month\n",
    "    df['elapsed_time'] = (max_date- df['first_active_month']).dt.days\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n",
    "                     'new_hist_purchase_date_min']:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n",
    "    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
    "\n",
    "    \n",
    "    df['purchase_amount_diff'] = df['new_hist_purchase_amount_sum']-df['hist_purchase_amount_sum']\n",
    "    df['purchase_amount_rate'] = df['purchase_amount_diff']/df['hist_purchase_amount_sum']\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:57:12.764908Z",
     "start_time": "2019-02-22T02:56:55.434665Z"
    },
    "_uuid": "a93c5976d3b395ba8ff0d1002b8075be3e914c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 128.82 Mb (66.0% reduction)\n",
      "Mem. usage decreased to 77.58 Mb (66.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:57:12.774781Z",
     "start_time": "2019-02-22T02:57:12.766721Z"
    },
    "_uuid": "c4f20f27679889542acfd60d1f1ac381b201ac43",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bed_fea =['new_hist_month_diff_min', 'new_hist_authorized_flag_mean', 'new_hist_year_mean_mean', 'new_hist_year_nunique_/_hist_year_nunique', 'new_hist_year_nunique_-_hist_year_nunique']\n",
    "del_col =  ['card_id', 'first_active_month','target','outliers']+FILTER_FEATURE\n",
    "\n",
    "features = [c for c in df_train.columns if c not in del_col ]\n",
    "\n",
    "target = df_train['target']\n",
    "\n",
    "train_y = target \n",
    "# df_train.drop(columns  = [\"card_id\",\"target\"],inplace=True)\n",
    "# df_test.drop(columns = [\"card_id\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:57:12.779508Z",
     "start_time": "2019-02-22T02:57:12.776682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:57:12.791276Z",
     "start_time": "2019-02-22T02:57:12.781217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_1',\n",
       " 'feature_2',\n",
       " 'feature_3',\n",
       " 'hist_month_nunique',\n",
       " 'hist_hour_nunique',\n",
       " 'hist_weekofyear_nunique',\n",
       " 'hist_dayofweek_nunique',\n",
       " 'hist_subsector_id_nunique',\n",
       " 'hist_merchant_id_nunique',\n",
       " 'hist_merchant_category_id_nunique',\n",
       " 'hist_purchase_amount_sum',\n",
       " 'hist_purchase_amount_max',\n",
       " 'hist_purchase_amount_min',\n",
       " 'hist_purchase_amount_mean',\n",
       " 'hist_purchase_amount_var',\n",
       " 'hist_installments_sum',\n",
       " 'hist_installments_max',\n",
       " 'hist_installments_min',\n",
       " 'hist_installments_mean',\n",
       " 'hist_installments_var',\n",
       " 'hist_purchase_date_max',\n",
       " 'hist_purchase_date_min',\n",
       " 'hist_month_lag_max',\n",
       " 'hist_month_lag_min',\n",
       " 'hist_month_lag_mean',\n",
       " 'hist_month_lag_var',\n",
       " 'hist_month_diff_max',\n",
       " 'hist_month_diff_min',\n",
       " 'hist_month_diff_mean',\n",
       " 'hist_month_diff_var',\n",
       " 'hist_authorized_flag_sum',\n",
       " 'hist_authorized_flag_mean',\n",
       " 'hist_weekend_sum',\n",
       " 'hist_weekend_mean',\n",
       " 'hist_category_1_sum',\n",
       " 'hist_category_1_mean',\n",
       " 'hist_card_id_size',\n",
       " 'hist_month_mean_mean',\n",
       " 'hist_hour_mean_mean',\n",
       " 'hist_weekofyear_mean_mean',\n",
       " 'hist_dayofweek_mean_mean',\n",
       " 'hist_year_mean_mean',\n",
       " 'hist_subsector_id_mean_mean',\n",
       " 'hist_merchant_id_mean_mean',\n",
       " 'hist_merchant_category_id_mean_mean',\n",
       " 'hist_category_1_mean_mean',\n",
       " 'hist_category_2_mean_mean',\n",
       " 'hist_category_3_mean_mean',\n",
       " 'hist_month_lag_mean_mean',\n",
       " 'hist_card_id_mean_mean',\n",
       " 'hist_purchase_date_diff',\n",
       " 'hist_purchase_date_average',\n",
       " 'hist_purchase_date_uptonow',\n",
       " 'new_hist_month_nunique',\n",
       " 'new_hist_hour_nunique',\n",
       " 'new_hist_weekofyear_nunique',\n",
       " 'new_hist_year_nunique',\n",
       " 'new_hist_subsector_id_nunique',\n",
       " 'new_hist_merchant_id_nunique',\n",
       " 'new_hist_merchant_category_id_nunique',\n",
       " 'new_hist_purchase_amount_sum',\n",
       " 'new_hist_purchase_amount_max',\n",
       " 'new_hist_purchase_amount_min',\n",
       " 'new_hist_purchase_amount_mean',\n",
       " 'new_hist_purchase_amount_var',\n",
       " 'new_hist_installments_sum',\n",
       " 'new_hist_installments_max',\n",
       " 'new_hist_installments_min',\n",
       " 'new_hist_installments_mean',\n",
       " 'new_hist_installments_var',\n",
       " 'new_hist_purchase_date_max',\n",
       " 'new_hist_purchase_date_min',\n",
       " 'new_hist_month_lag_max',\n",
       " 'new_hist_month_lag_min',\n",
       " 'new_hist_month_lag_mean',\n",
       " 'new_hist_month_lag_var',\n",
       " 'new_hist_month_diff_mean',\n",
       " 'new_hist_authorized_flag_sum',\n",
       " 'new_hist_category_1_sum',\n",
       " 'new_hist_category_1_mean',\n",
       " 'new_hist_card_id_size',\n",
       " 'new_hist_month_mean_mean',\n",
       " 'new_hist_hour_mean_mean',\n",
       " 'new_hist_weekofyear_mean_mean',\n",
       " 'new_hist_dayofweek_mean_mean',\n",
       " 'new_hist_year_mean_mean',\n",
       " 'new_hist_merchant_id_mean_mean',\n",
       " 'new_hist_category_1_mean_mean',\n",
       " 'new_hist_category_3_mean_mean',\n",
       " 'new_hist_month_lag_mean_mean',\n",
       " 'new_hist_card_id_mean_mean',\n",
       " 'new_hist_purchase_date_diff',\n",
       " 'new_hist_purchase_date_average',\n",
       " 'new_hist_purchase_date_uptonow',\n",
       " 'new_hist_month_nunique_add_hist_month_nunique',\n",
       " 'new_hist_month_nunique_-_hist_month_nunique',\n",
       " 'new_hist_month_nunique_/_hist_month_nunique',\n",
       " 'new_hist_hour_nunique_add_hist_hour_nunique',\n",
       " 'new_hist_hour_nunique_-_hist_hour_nunique',\n",
       " 'new_hist_hour_nunique_/_hist_hour_nunique',\n",
       " 'new_hist_weekofyear_nunique_add_hist_weekofyear_nunique',\n",
       " 'new_hist_weekofyear_nunique_-_hist_weekofyear_nunique',\n",
       " 'new_hist_weekofyear_nunique_/_hist_weekofyear_nunique',\n",
       " 'new_hist_dayofweek_nunique_add_hist_dayofweek_nunique',\n",
       " 'new_hist_dayofweek_nunique_-_hist_dayofweek_nunique',\n",
       " 'new_hist_year_nunique_add_hist_year_nunique',\n",
       " 'new_hist_subsector_id_nunique_add_hist_subsector_id_nunique',\n",
       " 'new_hist_subsector_id_nunique_-_hist_subsector_id_nunique',\n",
       " 'new_hist_subsector_id_nunique_/_hist_subsector_id_nunique',\n",
       " 'new_hist_merchant_id_nunique_add_hist_merchant_id_nunique',\n",
       " 'new_hist_merchant_id_nunique_-_hist_merchant_id_nunique',\n",
       " 'new_hist_merchant_id_nunique_/_hist_merchant_id_nunique',\n",
       " 'new_hist_merchant_category_id_nunique_add_hist_merchant_category_id_nunique',\n",
       " 'new_hist_merchant_category_id_nunique_/_hist_merchant_category_id_nunique',\n",
       " 'new_hist_purchase_amount_sum_add_hist_purchase_amount_sum',\n",
       " 'new_hist_purchase_amount_sum_-_hist_purchase_amount_sum',\n",
       " 'new_hist_purchase_amount_sum_/_hist_purchase_amount_sum',\n",
       " 'new_hist_purchase_amount_max_add_hist_purchase_amount_max',\n",
       " 'new_hist_purchase_amount_max_-_hist_purchase_amount_max',\n",
       " 'new_hist_purchase_amount_max_/_hist_purchase_amount_max',\n",
       " 'new_hist_purchase_amount_min_add_hist_purchase_amount_min',\n",
       " 'new_hist_purchase_amount_min_-_hist_purchase_amount_min',\n",
       " 'new_hist_purchase_amount_min_/_hist_purchase_amount_min',\n",
       " 'new_hist_purchase_amount_mean_add_hist_purchase_amount_mean',\n",
       " 'new_hist_purchase_amount_mean_-_hist_purchase_amount_mean',\n",
       " 'new_hist_purchase_amount_mean_/_hist_purchase_amount_mean',\n",
       " 'new_hist_purchase_amount_var_add_hist_purchase_amount_var',\n",
       " 'new_hist_purchase_amount_var_-_hist_purchase_amount_var',\n",
       " 'new_hist_purchase_amount_var_/_hist_purchase_amount_var',\n",
       " 'new_hist_installments_sum_add_hist_installments_sum',\n",
       " 'new_hist_installments_sum_-_hist_installments_sum',\n",
       " 'new_hist_installments_sum_/_hist_installments_sum',\n",
       " 'new_hist_installments_max_add_hist_installments_max',\n",
       " 'new_hist_installments_max_-_hist_installments_max',\n",
       " 'new_hist_installments_max_/_hist_installments_max',\n",
       " 'new_hist_installments_min_add_hist_installments_min',\n",
       " 'new_hist_installments_min_/_hist_installments_min',\n",
       " 'new_hist_installments_mean_add_hist_installments_mean',\n",
       " 'new_hist_installments_mean_-_hist_installments_mean',\n",
       " 'new_hist_installments_mean_/_hist_installments_mean',\n",
       " 'new_hist_installments_var_add_hist_installments_var',\n",
       " 'new_hist_installments_var_-_hist_installments_var',\n",
       " 'new_hist_installments_var_/_hist_installments_var',\n",
       " 'new_hist_month_lag_max_add_hist_month_lag_max',\n",
       " 'new_hist_month_lag_max_-_hist_month_lag_max',\n",
       " 'new_hist_month_lag_max_/_hist_month_lag_max',\n",
       " 'new_hist_month_lag_min_add_hist_month_lag_min',\n",
       " 'new_hist_month_lag_min_-_hist_month_lag_min',\n",
       " 'new_hist_month_lag_min_/_hist_month_lag_min',\n",
       " 'new_hist_month_lag_mean_add_hist_month_lag_mean',\n",
       " 'new_hist_month_lag_mean_-_hist_month_lag_mean',\n",
       " 'new_hist_month_lag_mean_/_hist_month_lag_mean',\n",
       " 'new_hist_month_lag_var_add_hist_month_lag_var',\n",
       " 'new_hist_month_lag_var_-_hist_month_lag_var',\n",
       " 'new_hist_month_lag_var_/_hist_month_lag_var',\n",
       " 'new_hist_month_diff_max_add_hist_month_diff_max',\n",
       " 'new_hist_month_diff_min_add_hist_month_diff_min',\n",
       " 'new_hist_month_diff_mean_-_hist_month_diff_mean',\n",
       " 'new_hist_month_diff_mean_/_hist_month_diff_mean',\n",
       " 'new_hist_month_diff_var_add_hist_month_diff_var',\n",
       " 'new_hist_month_diff_var_-_hist_month_diff_var',\n",
       " 'new_hist_month_diff_var_/_hist_month_diff_var',\n",
       " 'new_hist_authorized_flag_sum_add_hist_authorized_flag_sum',\n",
       " 'new_hist_authorized_flag_sum_-_hist_authorized_flag_sum',\n",
       " 'new_hist_authorized_flag_sum_/_hist_authorized_flag_sum',\n",
       " 'new_hist_authorized_flag_mean_add_hist_authorized_flag_mean',\n",
       " 'new_hist_authorized_flag_mean_-_hist_authorized_flag_mean',\n",
       " 'new_hist_authorized_flag_mean_/_hist_authorized_flag_mean',\n",
       " 'new_hist_weekend_sum_add_hist_weekend_sum',\n",
       " 'new_hist_weekend_sum_-_hist_weekend_sum',\n",
       " 'new_hist_weekend_sum_/_hist_weekend_sum',\n",
       " 'new_hist_weekend_mean_add_hist_weekend_mean',\n",
       " 'new_hist_weekend_mean_-_hist_weekend_mean',\n",
       " 'new_hist_weekend_mean_/_hist_weekend_mean',\n",
       " 'new_hist_category_1_sum_add_hist_category_1_sum',\n",
       " 'new_hist_category_1_sum_-_hist_category_1_sum',\n",
       " 'new_hist_category_1_sum_/_hist_category_1_sum',\n",
       " 'new_hist_category_1_mean_add_hist_category_1_mean',\n",
       " 'new_hist_category_1_mean_-_hist_category_1_mean',\n",
       " 'new_hist_category_1_mean_/_hist_category_1_mean',\n",
       " 'new_hist_card_id_size_add_hist_card_id_size',\n",
       " 'new_hist_card_id_size_-_hist_card_id_size',\n",
       " 'new_hist_card_id_size_/_hist_card_id_size',\n",
       " 'new_hist_month_mean_mean_add_hist_month_mean_mean',\n",
       " 'new_hist_month_mean_mean_-_hist_month_mean_mean',\n",
       " 'new_hist_hour_mean_mean_add_hist_hour_mean_mean',\n",
       " 'new_hist_hour_mean_mean_-_hist_hour_mean_mean',\n",
       " 'new_hist_hour_mean_mean_/_hist_hour_mean_mean',\n",
       " 'new_hist_weekofyear_mean_mean_-_hist_weekofyear_mean_mean',\n",
       " 'new_hist_weekofyear_mean_mean_/_hist_weekofyear_mean_mean',\n",
       " 'new_hist_dayofweek_mean_mean_add_hist_dayofweek_mean_mean',\n",
       " 'new_hist_dayofweek_mean_mean_-_hist_dayofweek_mean_mean',\n",
       " 'new_hist_subsector_id_mean_mean_add_hist_subsector_id_mean_mean',\n",
       " 'new_hist_subsector_id_mean_mean_-_hist_subsector_id_mean_mean',\n",
       " 'new_hist_subsector_id_mean_mean_/_hist_subsector_id_mean_mean',\n",
       " 'new_hist_merchant_id_mean_mean_-_hist_merchant_id_mean_mean',\n",
       " 'new_hist_category_1_mean_mean_add_hist_category_1_mean_mean',\n",
       " 'new_hist_category_1_mean_mean_-_hist_category_1_mean_mean',\n",
       " 'new_hist_category_1_mean_mean_/_hist_category_1_mean_mean',\n",
       " 'new_hist_category_2_mean_mean_add_hist_category_2_mean_mean',\n",
       " 'new_hist_category_2_mean_mean_-_hist_category_2_mean_mean',\n",
       " 'new_hist_category_3_mean_mean_add_hist_category_3_mean_mean',\n",
       " 'new_hist_category_3_mean_mean_-_hist_category_3_mean_mean',\n",
       " 'new_hist_category_3_mean_mean_/_hist_category_3_mean_mean',\n",
       " 'new_hist_month_lag_mean_mean_-_hist_month_lag_mean_mean',\n",
       " 'new_hist_month_lag_mean_mean_/_hist_month_lag_mean_mean',\n",
       " 'new_hist_card_id_mean_mean_-_hist_card_id_mean_mean',\n",
       " 'new_hist_card_id_mean_mean_/_hist_card_id_mean_mean',\n",
       " 'new_hist_purchase_date_diff_add_hist_purchase_date_diff',\n",
       " 'new_hist_purchase_date_diff_-_hist_purchase_date_diff',\n",
       " 'new_hist_purchase_date_diff_/_hist_purchase_date_diff',\n",
       " 'new_hist_purchase_date_average_add_hist_purchase_date_average',\n",
       " 'new_hist_purchase_date_average_-_hist_purchase_date_average',\n",
       " 'new_hist_purchase_date_average_/_hist_purchase_date_average',\n",
       " 'new_hist_purchase_date_uptonow_add_hist_purchase_date_uptonow',\n",
       " 'new_hist_purchase_date_uptonow_-_hist_purchase_date_uptonow',\n",
       " 'new_hist_purchase_date_uptonow_/_hist_purchase_date_uptonow',\n",
       " 'feature_1_ctr',\n",
       " 'feature_2_ctr',\n",
       " 'feature_3_ctr',\n",
       " 'new_hist_month_diff_mean_ctr',\n",
       " 'hist_month_diff_mean_ctr',\n",
       " 'hist_month_diff_max_ctr',\n",
       " 'hist_month_diff_min_ctr',\n",
       " 'feature_1_feature_2_ctr',\n",
       " 'feature_1_feature_3_ctr',\n",
       " 'feature_1_new_hist_month_diff_mean_ctr',\n",
       " 'feature_1_hist_month_diff_mean_ctr',\n",
       " 'feature_1_hist_month_diff_max_ctr',\n",
       " 'feature_1_hist_month_diff_min_ctr',\n",
       " 'feature_2_feature_3_ctr',\n",
       " 'feature_2_new_hist_month_diff_mean_ctr',\n",
       " 'feature_2_hist_month_diff_mean_ctr',\n",
       " 'feature_2_hist_month_diff_max_ctr',\n",
       " 'feature_2_hist_month_diff_min_ctr',\n",
       " 'feature_3_new_hist_month_diff_mean_ctr',\n",
       " 'feature_3_hist_month_diff_mean_ctr',\n",
       " 'feature_3_hist_month_diff_max_ctr',\n",
       " 'feature_3_hist_month_diff_min_ctr',\n",
       " 'new_hist_month_diff_mean_hist_month_diff_mean_ctr',\n",
       " 'new_hist_month_diff_mean_hist_month_diff_max_ctr',\n",
       " 'new_hist_month_diff_mean_hist_month_diff_min_ctr',\n",
       " 'hist_month_diff_mean_hist_month_diff_max_ctr',\n",
       " 'hist_month_diff_max_hist_month_diff_min_ctr',\n",
       " 'weekofyear',\n",
       " 'month',\n",
       " 'elapsed_time',\n",
       " 'hist_first_buy',\n",
       " 'new_hist_first_buy',\n",
       " 'card_id_total',\n",
       " 'purchase_amount_total',\n",
       " 'purchase_amount_diff',\n",
       " 'purchase_amount_rate']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T03:00:34.264343Z",
     "start_time": "2019-02-22T02:57:12.792981Z"
    },
    "_uuid": "c9bbc95244978b519d94131907b547c2b6c94191",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.57062\tvalid_1's rmse: 3.679\n",
      "[400]\ttraining's rmse: 3.47627\tvalid_1's rmse: 3.65675\n",
      "[600]\ttraining's rmse: 3.41366\tvalid_1's rmse: 3.65078\n",
      "[800]\ttraining's rmse: 3.36752\tvalid_1's rmse: 3.6494\n",
      "[1000]\ttraining's rmse: 3.32629\tvalid_1's rmse: 3.64918\n",
      "[1200]\ttraining's rmse: 3.28872\tvalid_1's rmse: 3.6483\n",
      "[1400]\ttraining's rmse: 3.25355\tvalid_1's rmse: 3.64728\n",
      "[1600]\ttraining's rmse: 3.21951\tvalid_1's rmse: 3.64686\n",
      "[1800]\ttraining's rmse: 3.18771\tvalid_1's rmse: 3.64714\n",
      "[2000]\ttraining's rmse: 3.1572\tvalid_1's rmse: 3.64761\n",
      "[2200]\ttraining's rmse: 3.12821\tvalid_1's rmse: 3.648\n",
      "Early stopping, best iteration is:\n",
      "[1711]\ttraining's rmse: 3.20118\tvalid_1's rmse: 3.64674\n",
      "fold 1\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.57168\tvalid_1's rmse: 3.67266\n",
      "[400]\ttraining's rmse: 3.47629\tvalid_1's rmse: 3.65279\n",
      "[600]\ttraining's rmse: 3.41396\tvalid_1's rmse: 3.64791\n",
      "[800]\ttraining's rmse: 3.36618\tvalid_1's rmse: 3.64513\n",
      "[1000]\ttraining's rmse: 3.32431\tvalid_1's rmse: 3.64343\n",
      "[1200]\ttraining's rmse: 3.28623\tvalid_1's rmse: 3.64243\n",
      "[1400]\ttraining's rmse: 3.25067\tvalid_1's rmse: 3.64163\n",
      "[1600]\ttraining's rmse: 3.21727\tvalid_1's rmse: 3.64129\n",
      "[1800]\ttraining's rmse: 3.18353\tvalid_1's rmse: 3.64152\n",
      "[2000]\ttraining's rmse: 3.15233\tvalid_1's rmse: 3.64132\n",
      "[2200]\ttraining's rmse: 3.12303\tvalid_1's rmse: 3.64136\n",
      "[2400]\ttraining's rmse: 3.09443\tvalid_1's rmse: 3.64175\n",
      "Early stopping, best iteration is:\n",
      "[2085]\ttraining's rmse: 3.13925\tvalid_1's rmse: 3.64099\n",
      "fold 2\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.56796\tvalid_1's rmse: 3.68753\n",
      "[400]\ttraining's rmse: 3.47331\tvalid_1's rmse: 3.6667\n",
      "[600]\ttraining's rmse: 3.41222\tvalid_1's rmse: 3.65926\n",
      "[800]\ttraining's rmse: 3.36407\tvalid_1's rmse: 3.65598\n",
      "[1000]\ttraining's rmse: 3.3218\tvalid_1's rmse: 3.65382\n",
      "[1200]\ttraining's rmse: 3.28279\tvalid_1's rmse: 3.65208\n",
      "[1400]\ttraining's rmse: 3.24655\tvalid_1's rmse: 3.65101\n",
      "[1600]\ttraining's rmse: 3.21181\tvalid_1's rmse: 3.65056\n",
      "[1800]\ttraining's rmse: 3.18126\tvalid_1's rmse: 3.6502\n",
      "[2000]\ttraining's rmse: 3.15133\tvalid_1's rmse: 3.65036\n",
      "[2200]\ttraining's rmse: 3.12008\tvalid_1's rmse: 3.65109\n",
      "[2400]\ttraining's rmse: 3.09152\tvalid_1's rmse: 3.65141\n",
      "Early stopping, best iteration is:\n",
      "[1915]\ttraining's rmse: 3.16392\tvalid_1's rmse: 3.64995\n",
      "fold 3\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.56598\tvalid_1's rmse: 3.6867\n",
      "[400]\ttraining's rmse: 3.47187\tvalid_1's rmse: 3.66746\n",
      "[600]\ttraining's rmse: 3.41005\tvalid_1's rmse: 3.66151\n",
      "[800]\ttraining's rmse: 3.36127\tvalid_1's rmse: 3.65852\n",
      "[1000]\ttraining's rmse: 3.31951\tvalid_1's rmse: 3.65701\n",
      "[1200]\ttraining's rmse: 3.2801\tvalid_1's rmse: 3.65517\n",
      "[1400]\ttraining's rmse: 3.24545\tvalid_1's rmse: 3.65433\n",
      "[1600]\ttraining's rmse: 3.21101\tvalid_1's rmse: 3.65357\n",
      "[1800]\ttraining's rmse: 3.17857\tvalid_1's rmse: 3.65254\n",
      "[2000]\ttraining's rmse: 3.14801\tvalid_1's rmse: 3.6524\n",
      "[2200]\ttraining's rmse: 3.1186\tvalid_1's rmse: 3.65178\n",
      "[2400]\ttraining's rmse: 3.08955\tvalid_1's rmse: 3.65181\n",
      "[2600]\ttraining's rmse: 3.0604\tvalid_1's rmse: 3.65205\n",
      "[2800]\ttraining's rmse: 3.03194\tvalid_1's rmse: 3.65187\n",
      "Early stopping, best iteration is:\n",
      "[2368]\ttraining's rmse: 3.09467\tvalid_1's rmse: 3.65155\n",
      "fold 4\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.56913\tvalid_1's rmse: 3.67449\n",
      "[400]\ttraining's rmse: 3.47394\tvalid_1's rmse: 3.65991\n",
      "[600]\ttraining's rmse: 3.41268\tvalid_1's rmse: 3.65433\n",
      "[800]\ttraining's rmse: 3.36404\tvalid_1's rmse: 3.65227\n",
      "[1000]\ttraining's rmse: 3.31927\tvalid_1's rmse: 3.6514\n",
      "[1200]\ttraining's rmse: 3.28015\tvalid_1's rmse: 3.65068\n",
      "[1400]\ttraining's rmse: 3.24449\tvalid_1's rmse: 3.64998\n",
      "[1600]\ttraining's rmse: 3.20986\tvalid_1's rmse: 3.65044\n",
      "[1800]\ttraining's rmse: 3.17663\tvalid_1's rmse: 3.65133\n",
      "Early stopping, best iteration is:\n",
      "[1418]\ttraining's rmse: 3.24159\tvalid_1's rmse: 3.64973\n"
     ]
    }
   ],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"min_child_samples\": 30,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.5,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         'lambda_l2':0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"n_jobs\":10,\n",
    "         \"random_state\": 4590}\n",
    "NFOLD = 5\n",
    "oof_train = np.zeros((len(df_train),1))\n",
    "oof_test = np.zeros((len(df_test),1))\n",
    "oof_test_skf = np.zeros((NFOLD,len(df_test),))\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(skf.split(df_train,df_train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features],target.iloc[trn_idx])#ategorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][features],target.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 500)\n",
    "    oof_train[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    oof_test_skf[fold_,:]= clf.predict(df_test[features], num_iteration=clf.best_iteration)    \n",
    "oof_test = oof_test_skf.mean(axis=0)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train.reshape(-1), target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T03:00:35.153688Z",
     "start_time": "2019-02-22T03:00:34.266554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6477934509677534\n"
     ]
    }
   ],
   "source": [
    "mean_loss=np.sqrt(mean_squared_error(oof_train.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "train_prob=pd.DataFrame(oof_train)\n",
    "train_prob.columns=['class1']\n",
    "train_prob.to_csv(\"oof/train_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "test_prob=pd.DataFrame(oof_test)\n",
    "test_prob.columns=['class1']\n",
    "test_prob.to_csv(\"oof/test_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "np.save(\"train_y\",target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T03:00:45.457613Z",
     "start_time": "2019-02-22T03:00:35.155785Z"
    },
    "_uuid": "40b64481054fa71e692829c7039eccceb31b77fe",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cv: 3.6497796168629439  lb:3.693\n",
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:500].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"Feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T03:00:45.886304Z",
     "start_time": "2019-02-22T03:00:45.459920Z"
    },
    "_uuid": "355e9c24949b8e5d677fe5a2f117228c3310dab6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = oof_test\n",
    "sub_df.to_csv(\"sub/submission_%s.csv\"%mean_loss, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T03:00:45.912184Z",
     "start_time": "2019-02-22T03:00:45.888628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>new_hist_year_mean_mean</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>new_hist_month_nunique</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>new_hist_month_nunique</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>new_hist_month_nunique</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>new_hist_month_nunique</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>new_hist_year_mean_mean</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>new_hist_year_mean_mean</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>new_hist_year_nunique_add_hist_year_nunique</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>new_hist_year_mean_mean</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>new_hist_year_nunique_add_hist_year_nunique</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>new_hist_month_diff_min_add_hist_month_diff_min</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>new_hist_year_mean_mean</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>new_hist_month_diff_min_add_hist_month_diff_min</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>new_hist_year_nunique_add_hist_year_nunique</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>new_hist_year_nunique_add_hist_year_nunique</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature_3</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hist_month_diff_min</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>new_hist_month_nunique</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature_3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>new_hist_year_nunique_add_hist_year_nunique</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>new_hist_month_diff_min_add_hist_month_diff_min</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>new_hist_month_diff_max_add_hist_month_diff_max</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>new_hist_month_diff_min_add_hist_month_diff_min</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>new_hist_month_diff_min_add_hist_month_diff_min</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>new_hist_subsector_id_nunique</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hist_month_diff_min</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>new_hist_month_diff_max_add_hist_month_diff_max</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>new_hist_year_nunique</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature_3</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>new_hist_month_diff_max_add_hist_month_diff_max</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>hist_month_diff_mean_ctr</td>\n",
       "      <td>681</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>hist_authorized_flag_mean</td>\n",
       "      <td>683</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>feature_1_hist_month_diff_mean_ctr</td>\n",
       "      <td>686</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>feature_3_hist_month_diff_mean_ctr</td>\n",
       "      <td>702</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>feature_2_hist_month_diff_mean_ctr</td>\n",
       "      <td>702</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>feature_1_hist_month_diff_mean_ctr</td>\n",
       "      <td>707</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>feature_2_hist_month_diff_min_ctr</td>\n",
       "      <td>717</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>hist_category_1_sum</td>\n",
       "      <td>724</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>feature_3_hist_month_diff_mean_ctr</td>\n",
       "      <td>725</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>feature_2_hist_month_diff_max_ctr</td>\n",
       "      <td>729</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>hist_authorized_flag_mean</td>\n",
       "      <td>733</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>hist_month_diff_mean_ctr</td>\n",
       "      <td>734</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>feature_1_hist_month_diff_max_ctr</td>\n",
       "      <td>749</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>hist_authorized_flag_mean</td>\n",
       "      <td>749</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>feature_1_hist_month_diff_min_ctr</td>\n",
       "      <td>757</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>feature_2_hist_month_diff_mean_ctr</td>\n",
       "      <td>761</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>feature_1_hist_month_diff_max_ctr</td>\n",
       "      <td>786</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>feature_2_hist_month_diff_max_ctr</td>\n",
       "      <td>786</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>feature_2_hist_month_diff_mean_ctr</td>\n",
       "      <td>799</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>feature_3_hist_month_diff_mean_ctr</td>\n",
       "      <td>805</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>new_hist_purchase_date_uptonow_/_hist_purchase...</td>\n",
       "      <td>815</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>feature_1_hist_month_diff_min_ctr</td>\n",
       "      <td>828</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>new_hist_purchase_date_uptonow_-_hist_purchase...</td>\n",
       "      <td>830</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>feature_2_hist_month_diff_min_ctr</td>\n",
       "      <td>831</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>new_hist_purchase_date_uptonow_/_hist_purchase...</td>\n",
       "      <td>884</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>new_hist_purchase_date_uptonow_-_hist_purchase...</td>\n",
       "      <td>906</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>hist_authorized_flag_mean</td>\n",
       "      <td>910</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>new_hist_purchase_date_uptonow_-_hist_purchase...</td>\n",
       "      <td>959</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>feature_1_hist_month_diff_max_ctr</td>\n",
       "      <td>1009</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>new_hist_purchase_date_uptonow_-_hist_purchase...</td>\n",
       "      <td>1115</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1265 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Feature  importance  fold\n",
       "85                             new_hist_year_mean_mean           2     5\n",
       "53                              new_hist_month_nunique           2     5\n",
       "53                              new_hist_month_nunique           3     2\n",
       "53                              new_hist_month_nunique           4     4\n",
       "53                              new_hist_month_nunique           4     3\n",
       "85                             new_hist_year_mean_mean           4     2\n",
       "85                             new_hist_year_mean_mean           5     4\n",
       "105        new_hist_year_nunique_add_hist_year_nunique           5     4\n",
       "85                             new_hist_year_mean_mean           6     1\n",
       "105        new_hist_year_nunique_add_hist_year_nunique           6     5\n",
       "156    new_hist_month_diff_min_add_hist_month_diff_min           7     3\n",
       "85                             new_hist_year_mean_mean           7     3\n",
       "156    new_hist_month_diff_min_add_hist_month_diff_min           8     2\n",
       "105        new_hist_year_nunique_add_hist_year_nunique           8     2\n",
       "105        new_hist_year_nunique_add_hist_year_nunique           8     3\n",
       "2                                            feature_3           9     5\n",
       "27                                 hist_month_diff_min          10     1\n",
       "53                              new_hist_month_nunique          10     1\n",
       "2                                            feature_3          10     3\n",
       "105        new_hist_year_nunique_add_hist_year_nunique          10     1\n",
       "156    new_hist_month_diff_min_add_hist_month_diff_min          10     4\n",
       "155    new_hist_month_diff_max_add_hist_month_diff_max          13     3\n",
       "156    new_hist_month_diff_min_add_hist_month_diff_min          13     1\n",
       "156    new_hist_month_diff_min_add_hist_month_diff_min          13     5\n",
       "57                       new_hist_subsector_id_nunique          14     5\n",
       "27                                 hist_month_diff_min          14     5\n",
       "155    new_hist_month_diff_max_add_hist_month_diff_max          14     5\n",
       "56                               new_hist_year_nunique          14     2\n",
       "2                                            feature_3          15     2\n",
       "155    new_hist_month_diff_max_add_hist_month_diff_max          15     2\n",
       "..                                                 ...         ...   ...\n",
       "221                           hist_month_diff_mean_ctr         681     2\n",
       "31                           hist_authorized_flag_mean         683     1\n",
       "227                 feature_1_hist_month_diff_mean_ctr         686     2\n",
       "236                 feature_3_hist_month_diff_mean_ctr         702     3\n",
       "232                 feature_2_hist_month_diff_mean_ctr         702     3\n",
       "227                 feature_1_hist_month_diff_mean_ctr         707     4\n",
       "234                  feature_2_hist_month_diff_min_ctr         717     2\n",
       "34                                 hist_category_1_sum         724     4\n",
       "236                 feature_3_hist_month_diff_mean_ctr         725     2\n",
       "233                  feature_2_hist_month_diff_max_ctr         729     2\n",
       "31                           hist_authorized_flag_mean         733     3\n",
       "221                           hist_month_diff_mean_ctr         734     4\n",
       "228                  feature_1_hist_month_diff_max_ctr         749     3\n",
       "31                           hist_authorized_flag_mean         749     2\n",
       "229                  feature_1_hist_month_diff_min_ctr         757     2\n",
       "232                 feature_2_hist_month_diff_mean_ctr         761     2\n",
       "228                  feature_1_hist_month_diff_max_ctr         786     2\n",
       "233                  feature_2_hist_month_diff_max_ctr         786     4\n",
       "232                 feature_2_hist_month_diff_mean_ctr         799     4\n",
       "236                 feature_3_hist_month_diff_mean_ctr         805     4\n",
       "216  new_hist_purchase_date_uptonow_/_hist_purchase...         815     2\n",
       "229                  feature_1_hist_month_diff_min_ctr         828     4\n",
       "215  new_hist_purchase_date_uptonow_-_hist_purchase...         830     1\n",
       "234                  feature_2_hist_month_diff_min_ctr         831     4\n",
       "216  new_hist_purchase_date_uptonow_/_hist_purchase...         884     4\n",
       "215  new_hist_purchase_date_uptonow_-_hist_purchase...         906     3\n",
       "31                           hist_authorized_flag_mean         910     4\n",
       "215  new_hist_purchase_date_uptonow_-_hist_purchase...         959     2\n",
       "228                  feature_1_hist_month_diff_max_ctr        1009     4\n",
       "215  new_hist_purchase_date_uptonow_-_hist_purchase...        1115     4\n",
       "\n",
       "[1265 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_df.sort_values(by='importance',inplace=True)\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T03:00:45.972906Z",
     "start_time": "2019-02-22T03:00:45.914631Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #feature select\n",
    "\n",
    "# def get_feature_importances(data, shuffle, seed=None):\n",
    "#     # Gather real features\n",
    "#     train_features = [f for f in data if f not in ['card_id', 'first_active_month','target','outliers']]\n",
    "#     # Go over fold and keep track of CV score (train and valid) and feature importances\n",
    "    \n",
    "#     # Shuffle target if required\n",
    "#     y = data['target'].copy()\n",
    "#     if shuffle:\n",
    "#         # Here you could as well use a binomial distribution\n",
    "#         y = data['target'].copy().sample(frac=1.0)\n",
    "    \n",
    "#     # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest\n",
    "#     dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True)\n",
    "#     lgb_params = {\n",
    "#         'objective': 'regression',\n",
    "#         'boosting_type': 'rf',\n",
    "#         'subsample': 0.623,\n",
    "#         'colsample_bytree': 0.7,\n",
    "#         'num_leaves': 127,\n",
    "#         'max_depth': 8,\n",
    "#         'seed': 4590,\n",
    "#         'bagging_freq': 1,\n",
    "#         'n_jobs': 4\n",
    "#     }\n",
    "    \n",
    "#     # Fit the model\n",
    "#     clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200)\n",
    "\n",
    "#     # Get feature importances\n",
    "#     imp_df = pd.DataFrame()\n",
    "#     imp_df[\"feature\"] = list(train_features)\n",
    "#     imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n",
    "#     imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n",
    "    \n",
    "#     return imp_df\n",
    "\n",
    "\n",
    "# # Seed the unexpected randomness of this world\n",
    "# np.random.seed(123)\n",
    "# # Get the actual importance, i.e. without shuffling\n",
    "# actual_imp_df = get_feature_importances(data=df_train, shuffle=False)\n",
    "\n",
    "\n",
    "# null_imp_df = pd.DataFrame()\n",
    "# nb_runs = 80\n",
    "# import time\n",
    "# start = time.time()\n",
    "# dsp = ''\n",
    "# for i in range(nb_runs):\n",
    "#     # Get current run importances\n",
    "#     imp_df = get_feature_importances(data=df_train, shuffle=True)\n",
    "#     imp_df['run'] = i + 1 \n",
    "#     # Concat the latest importances with the old ones\n",
    "#     null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
    "#     # Erase previous message\n",
    "#     for l in range(len(dsp)):\n",
    "#         print('\\b', end='', flush=True)\n",
    "#     # Display current run and time used\n",
    "#     spent = (time.time() - start) / 60\n",
    "#     dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n",
    "#     print(dsp, end='', flush=True)\n",
    "\n",
    "# \"\"\"\n",
    "# Score features\n",
    "# There are several ways to score features :\n",
    "\n",
    "# Compute the number of samples in the actual importances that are away from the null importances recorded distribution.\n",
    "# Compute ratios like Actual / Null Max, Actual / Null Mean, Actual Mean / Null Max\n",
    "# In a first step I will use the log actual feature importance divided by the 75 percentile of null distribution.\n",
    "\n",
    "# \"\"\"\n",
    "    \n",
    "# feature_scores = []\n",
    "# for _f in actual_imp_df['feature'].unique():\n",
    "#     f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "#     f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n",
    "#     gain_score = np.log(1e-10 + f_act_imps_gain / (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n",
    "#     f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n",
    "#     f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n",
    "#     split_score = np.log(1e-10 + f_act_imps_split / (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n",
    "#     feature_scores.append((_f, split_score, gain_score))\n",
    "\n",
    "# scores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "\n",
    "# thr_list=scores_df['gain_score'].tolist()\n",
    "# thr_list=[ i for i in set(thr_list)]\n",
    "# thr_list=sorted(thr_list)\n",
    "\n",
    "\n",
    "# def test_feature(test_fea):\n",
    "#     kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "#     train_x = df_train[test_fea].values\n",
    "#     train_y = df_train['target']\n",
    "#     oof = np.zeros((train_x.shape[0],1))\n",
    "#     df_train.reset_index(drop=True,inplace=True)\n",
    "#     for idx,(idx_trn,idx_val) in enumerate(kf.split(train_x,df_train['outliers'].values)):\n",
    "#         print('第 %d fold'%idx)\n",
    "#         tr_x,tr_y,val_x,val_y=train_x[idx_trn],train_y[idx_trn],train_x[idx_val],train_y[idx_val]\n",
    "\n",
    "#         trn_data = lgb.Dataset(tr_x,tr_y)#, categorical_feature=categorical_feats)\n",
    "#         val_data = lgb.Dataset(val_x,val_y)#, categorical_feature=categorical_feats)\n",
    "\n",
    "#         clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 500)\n",
    "#         oof[idx_val] = clf.predict(val_x, num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "\n",
    "#     loss=np.sqrt(mean_squared_error(oof.reshape(-1), target))\n",
    "#     print('mean loss %f'%loss)\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# test_dict={}\n",
    "# for idx,i in enumerate(thr_list):\n",
    "#     if idx>50:\n",
    "#         break\n",
    "#     print(i)\n",
    "#     bed_fea=set(scores_df[scores_df['gain_score']<=i]['feature'].tolist())\n",
    "#     bed_fea=[i for i in bed_fea]\n",
    "#     print(bed_fea)\n",
    "#     del_col =  ['card_id', 'first_active_month','target','outliers']+bed_fea\n",
    "#     df_test_fea = [c for c in df_train.columns if c not in del_col ]\n",
    "#     test_dict[''.join(bed_fea)]=test_feature(df_test_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T03:00:45.977669Z",
     "start_time": "2019-02-22T03:00:45.974972Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# min(test_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T03:36:14.051423Z",
     "start_time": "2019-02-22T03:00:45.980180Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hist_ca=['authorized_flag','city_id','category_1','installments','category_3','merchant_category_id','merchant_id','category_2','state_id',\\\n",
    "          'subsector_id','month_lag','purchase_amount' ]\n",
    "\n",
    "hist_text_feature={}\n",
    "gp=df_hist_trans.groupby('card_id')\n",
    "for fea  in df_hist_ca:\n",
    "    hist_text_feature[fea+\"_hist_text\"]=[]\n",
    "hist_text_feature['card_id']=[]\n",
    "for cid,cid_df in  gp:\n",
    "    hist_text_feature['card_id'].append(cid)\n",
    "    for  fea in  df_hist_ca:\n",
    "        if fea=='card_id':\n",
    "            continue\n",
    "        hist_text_feature[fea+\"_hist_text\"].append(\" \".join(cid_df[fea].astype(str).tolist()))\n",
    "df_hist_text=pd.DataFrame(hist_text_feature)\n",
    "\n",
    "\n",
    "new_mer_text_feature={}\n",
    "gp=df_new_merchant_trans.groupby('card_id')\n",
    "for fea  in df_hist_ca:\n",
    "    new_mer_text_feature[fea+\"_new_mer_text\"]=[]\n",
    "new_mer_text_feature['card_id']=[]\n",
    "for cid,cid_df in  gp:\n",
    "    new_mer_text_feature['card_id'].append(cid)\n",
    "    for  fea in  df_hist_ca:\n",
    "        if fea=='card_id':\n",
    "            continue\n",
    "        new_mer_text_feature[fea+\"_new_mer_text\"].append(\" \".join(cid_df[fea].astype(str).tolist()))\n",
    "df_new_mer_text=pd.DataFrame(new_mer_text_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T02:50:44.166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917, 494482) (123623, 494482)\n"
     ]
    }
   ],
   "source": [
    "df_train=pd.merge(df_train,df_hist_text,how='left',on='card_id')\n",
    "df_test=pd.merge(df_test,df_hist_text,how='left',on='card_id')\n",
    "\n",
    "df_all=pd.concat([df_train,df_test])\n",
    "\n",
    "text_fea=[i for i in df_all.columns if \"text\"in i]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "cv=CountVectorizer(analyzer='word',token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "\n",
    "\n",
    "\n",
    "for idx,i in enumerate(text_fea):\n",
    "    cv.fit(df_all[i])\n",
    "    tr_x=cv.transform(df_train[i])\n",
    "    te_x=cv.transform(df_test[i])\n",
    "    \n",
    "    if idx==0:\n",
    "        cv_Train_x=tr_x\n",
    "        cv_Test_x=te_x\n",
    "    else:\n",
    "        cv_Train_x=sparse.hstack((cv_Train_x,tr_x))\n",
    "        cv_Test_x=sparse.hstack((cv_Test_x,te_x))\n",
    "    \n",
    "print(cv_Train_x.shape,cv_Test_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T02:50:44.168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917, 494735) (123623, 494735)\n",
      "fold 0\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.56189\tvalid_1's rmse: 3.6767\n",
      "[400]\ttraining's rmse: 3.46522\tvalid_1's rmse: 3.65441\n",
      "[600]\ttraining's rmse: 3.40429\tvalid_1's rmse: 3.64805\n",
      "[800]\ttraining's rmse: 3.36059\tvalid_1's rmse: 3.64647\n",
      "[1000]\ttraining's rmse: 3.32259\tvalid_1's rmse: 3.64546\n",
      "[1200]\ttraining's rmse: 3.28723\tvalid_1's rmse: 3.64566\n",
      "[1400]\ttraining's rmse: 3.2558\tvalid_1's rmse: 3.64566\n",
      "[1600]\ttraining's rmse: 3.22472\tvalid_1's rmse: 3.64585\n",
      "Early stopping, best iteration is:\n",
      "[1116]\ttraining's rmse: 3.30158\tvalid_1's rmse: 3.64527\n",
      "fold 1\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.56206\tvalid_1's rmse: 3.67388\n",
      "[400]\ttraining's rmse: 3.46564\tvalid_1's rmse: 3.65588\n",
      "[600]\ttraining's rmse: 3.40516\tvalid_1's rmse: 3.65053\n",
      "[800]\ttraining's rmse: 3.36012\tvalid_1's rmse: 3.64889\n",
      "[1000]\ttraining's rmse: 3.32156\tvalid_1's rmse: 3.64771\n",
      "[1200]\ttraining's rmse: 3.28655\tvalid_1's rmse: 3.64775\n",
      "[1400]\ttraining's rmse: 3.25389\tvalid_1's rmse: 3.64825\n",
      "Early stopping, best iteration is:\n",
      "[948]\ttraining's rmse: 3.33068\tvalid_1's rmse: 3.64764\n",
      "fold 2\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.56022\tvalid_1's rmse: 3.68153\n",
      "[400]\ttraining's rmse: 3.46051\tvalid_1's rmse: 3.65947\n",
      "[600]\ttraining's rmse: 3.40131\tvalid_1's rmse: 3.65219\n",
      "[800]\ttraining's rmse: 3.3565\tvalid_1's rmse: 3.64911\n",
      "[1000]\ttraining's rmse: 3.31735\tvalid_1's rmse: 3.64713\n",
      "[1200]\ttraining's rmse: 3.28252\tvalid_1's rmse: 3.64596\n",
      "[1400]\ttraining's rmse: 3.25029\tvalid_1's rmse: 3.64596\n",
      "[1600]\ttraining's rmse: 3.21856\tvalid_1's rmse: 3.64583\n",
      "[1800]\ttraining's rmse: 3.18935\tvalid_1's rmse: 3.64659\n",
      "[2000]\ttraining's rmse: 3.16177\tvalid_1's rmse: 3.64705\n",
      "Early stopping, best iteration is:\n",
      "[1543]\ttraining's rmse: 3.22721\tvalid_1's rmse: 3.6456\n",
      "fold 3\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.55816\tvalid_1's rmse: 3.68877\n",
      "[400]\ttraining's rmse: 3.46044\tvalid_1's rmse: 3.66711\n",
      "[600]\ttraining's rmse: 3.40051\tvalid_1's rmse: 3.66124\n",
      "[800]\ttraining's rmse: 3.35627\tvalid_1's rmse: 3.65785\n",
      "[1200]\ttraining's rmse: 3.28299\tvalid_1's rmse: 3.65757\n",
      "[1400]\ttraining's rmse: 3.2513\tvalid_1's rmse: 3.65761\n",
      "[1600]\ttraining's rmse: 3.2219\tvalid_1's rmse: 3.65773\n",
      "[1800]\ttraining's rmse: 3.19363\tvalid_1's rmse: 3.65736\n",
      "[2000]\ttraining's rmse: 3.16584\tvalid_1's rmse: 3.65775\n",
      "Early stopping, best iteration is:\n",
      "[1684]\ttraining's rmse: 3.2094\tvalid_1's rmse: 3.65706\n",
      "fold 4\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.56067\tvalid_1's rmse: 3.6733\n",
      "[400]\ttraining's rmse: 3.46406\tvalid_1's rmse: 3.65472\n",
      "[600]\ttraining's rmse: 3.40316\tvalid_1's rmse: 3.65014\n",
      "[800]\ttraining's rmse: 3.35859\tvalid_1's rmse: 3.64842\n",
      "[1000]\ttraining's rmse: 3.32\tvalid_1's rmse: 3.64792\n",
      "[1200]\ttraining's rmse: 3.28421\tvalid_1's rmse: 3.64785\n",
      "Early stopping, best iteration is:\n",
      "[871]\ttraining's rmse: 3.34437\tvalid_1's rmse: 3.64751\n",
      "3.6486189797285364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "train_x=df_train[features].values\n",
    "test_x=df_test[features].values\n",
    "train_y=df_train['target'].values\n",
    "\n",
    "train_x=sparse.hstack((train_x,cv_Train_x),'csr')\n",
    "test_x=sparse.hstack((test_x,cv_Test_x),'csr')\n",
    "oof_train2 = np.zeros((train_x.shape[0],1))\n",
    "oof_test2 = np.zeros((test_x.shape[0],1))\n",
    "oof_test2_skf = np.zeros((NFOLD,test_x.shape[0],))\n",
    "\n",
    "print(train_x.shape,test_x.shape)\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(skf.split(train_x,df_train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    x_tr,y_tr,x_te,y_te=train_x[trn_idx],train_y[trn_idx],train_x[val_idx],train_y[val_idx]\n",
    "    trn_data = lgb.Dataset(x_tr,y_tr)\n",
    "    val_data = lgb.Dataset(x_te,y_te)\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 500)\n",
    "    oof_train2[val_idx] = clf.predict(x_te, num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "\n",
    "    oof_test2_skf[fold_,:]= clf.predict(test_x, num_iteration=clf.best_iteration)    \n",
    "oof_test2 = oof_test2_skf.mean(axis=0)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train2.reshape(-1), target))\n",
    "print(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T02:50:44.171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6486189797285364\n"
     ]
    }
   ],
   "source": [
    "sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = oof_test2\n",
    "sub_df.to_csv(\"sub/submission_%s.csv\"%mean_loss, index=False)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train2.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "train_prob=pd.DataFrame(oof_train2)\n",
    "train_prob.columns=['class1']\n",
    "train_prob.to_csv(\"oof/train_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "test_prob=pd.DataFrame(oof_test2)\n",
    "test_prob.columns=['class1']\n",
    "test_prob.to_csv(\"oof/test_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "np.save(\"train_y\",target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T02:50:44.176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917, 233541) (123623, 233541)\n",
      "(201917, 233794) (123623, 233794)\n",
      "fold 0\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.56898\tvalid_1's rmse: 3.67825\n",
      "[400]\ttraining's rmse: 3.4741\tvalid_1's rmse: 3.6575\n",
      "[600]\ttraining's rmse: 3.41348\tvalid_1's rmse: 3.65248\n",
      "[800]\ttraining's rmse: 3.36601\tvalid_1's rmse: 3.65028\n",
      "[1000]\ttraining's rmse: 3.32488\tvalid_1's rmse: 3.64848\n",
      "[1200]\ttraining's rmse: 3.2867\tvalid_1's rmse: 3.64837\n",
      "[1400]\ttraining's rmse: 3.25186\tvalid_1's rmse: 3.64804\n",
      "[1600]\ttraining's rmse: 3.21877\tvalid_1's rmse: 3.64809\n",
      "[1800]\ttraining's rmse: 3.18683\tvalid_1's rmse: 3.64799\n",
      "[2000]\ttraining's rmse: 3.1579\tvalid_1's rmse: 3.6487\n",
      "[2200]\ttraining's rmse: 3.12957\tvalid_1's rmse: 3.64942\n",
      "Early stopping, best iteration is:\n",
      "[1830]\ttraining's rmse: 3.18238\tvalid_1's rmse: 3.64791\n",
      "fold 1\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.57008\tvalid_1's rmse: 3.67546\n",
      "[400]\ttraining's rmse: 3.47512\tvalid_1's rmse: 3.65669\n",
      "[600]\ttraining's rmse: 3.41427\tvalid_1's rmse: 3.65106\n",
      "[800]\ttraining's rmse: 3.36664\tvalid_1's rmse: 3.6485\n",
      "[1000]\ttraining's rmse: 3.32565\tvalid_1's rmse: 3.64687\n",
      "[1200]\ttraining's rmse: 3.28809\tvalid_1's rmse: 3.64575\n",
      "[1400]\ttraining's rmse: 3.25258\tvalid_1's rmse: 3.64574\n",
      "[1600]\ttraining's rmse: 3.22153\tvalid_1's rmse: 3.6451\n",
      "[1800]\ttraining's rmse: 3.18946\tvalid_1's rmse: 3.64498\n",
      "[2000]\ttraining's rmse: 3.16038\tvalid_1's rmse: 3.64502\n",
      "[2200]\ttraining's rmse: 3.13108\tvalid_1's rmse: 3.64499\n",
      "Early stopping, best iteration is:\n",
      "[1883]\ttraining's rmse: 3.17747\tvalid_1's rmse: 3.64462\n",
      "fold 2\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.56784\tvalid_1's rmse: 3.68436\n",
      "[400]\ttraining's rmse: 3.47253\tvalid_1's rmse: 3.66071\n",
      "[600]\ttraining's rmse: 3.41187\tvalid_1's rmse: 3.65348\n",
      "[800]\ttraining's rmse: 3.36437\tvalid_1's rmse: 3.64991\n",
      "[1000]\ttraining's rmse: 3.32192\tvalid_1's rmse: 3.6478\n",
      "[1200]\ttraining's rmse: 3.28486\tvalid_1's rmse: 3.64613\n",
      "[1400]\ttraining's rmse: 3.24994\tvalid_1's rmse: 3.646\n",
      "[1600]\ttraining's rmse: 3.21582\tvalid_1's rmse: 3.64627\n",
      "[1800]\ttraining's rmse: 3.18605\tvalid_1's rmse: 3.64629\n",
      "Early stopping, best iteration is:\n",
      "[1426]\ttraining's rmse: 3.24582\tvalid_1's rmse: 3.6457\n",
      "fold 3\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.5663\tvalid_1's rmse: 3.68805\n",
      "[400]\ttraining's rmse: 3.47094\tvalid_1's rmse: 3.66835\n",
      "[600]\ttraining's rmse: 3.41195\tvalid_1's rmse: 3.66144\n",
      "[800]\ttraining's rmse: 3.364\tvalid_1's rmse: 3.65798\n",
      "[1000]\ttraining's rmse: 3.3234\tvalid_1's rmse: 3.65588\n",
      "[1200]\ttraining's rmse: 3.28554\tvalid_1's rmse: 3.65483\n",
      "[1400]\ttraining's rmse: 3.25161\tvalid_1's rmse: 3.65321\n",
      "[1600]\ttraining's rmse: 3.21722\tvalid_1's rmse: 3.65271\n",
      "[1800]\ttraining's rmse: 3.18604\tvalid_1's rmse: 3.65189\n",
      "[2000]\ttraining's rmse: 3.1561\tvalid_1's rmse: 3.65089\n",
      "[2200]\ttraining's rmse: 3.12706\tvalid_1's rmse: 3.6504\n",
      "[2400]\ttraining's rmse: 3.09788\tvalid_1's rmse: 3.65051\n",
      "[2600]\ttraining's rmse: 3.07107\tvalid_1's rmse: 3.65077\n",
      "Early stopping, best iteration is:\n",
      "[2250]\ttraining's rmse: 3.11954\tvalid_1's rmse: 3.64999\n",
      "fold 4\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.56847\tvalid_1's rmse: 3.67419\n",
      "[400]\ttraining's rmse: 3.47258\tvalid_1's rmse: 3.65699\n",
      "[600]\ttraining's rmse: 3.41096\tvalid_1's rmse: 3.65214\n",
      "[800]\ttraining's rmse: 3.3623\tvalid_1's rmse: 3.64894\n",
      "[1000]\ttraining's rmse: 3.32065\tvalid_1's rmse: 3.64685\n",
      "[1200]\ttraining's rmse: 3.28202\tvalid_1's rmse: 3.64644\n",
      "[1400]\ttraining's rmse: 3.24599\tvalid_1's rmse: 3.64584\n",
      "[1600]\ttraining's rmse: 3.21271\tvalid_1's rmse: 3.64559\n",
      "[1800]\ttraining's rmse: 3.18124\tvalid_1's rmse: 3.6465\n",
      "[2000]\ttraining's rmse: 3.15053\tvalid_1's rmse: 3.64716\n",
      "Early stopping, best iteration is:\n",
      "[1597]\ttraining's rmse: 3.21312\tvalid_1's rmse: 3.64552\n",
      "3.6467480017295744\n",
      "3.6467480017295744\n"
     ]
    }
   ],
   "source": [
    "df_train=pd.merge(df_train,df_new_mer_text,how='left',on='card_id')\n",
    "df_test=pd.merge(df_test,df_new_mer_text,how='left',on='card_id')\n",
    "\n",
    "df_all=pd.concat([df_train,df_test])\n",
    "\n",
    "text_fea=[i for i in df_all.columns if \"new_mer_text\"in i]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "cv=CountVectorizer(analyzer='word',token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "\n",
    "\n",
    "\n",
    "for idx,i in enumerate(text_fea):\n",
    "    df_all[i].fillna('-1',inplace=True)\n",
    "    df_train[i].fillna('-1',inplace=True)\n",
    "    df_test[i].fillna('-1',inplace=True)\n",
    "    cv.fit(df_all[i])\n",
    "    tr_x=cv.transform(df_train[i])\n",
    "    te_x=cv.transform(df_test[i])\n",
    "    \n",
    "    if idx==0:\n",
    "        cv_Train_x_2=tr_x\n",
    "        cv_Test_x_2=te_x\n",
    "    else:\n",
    "        cv_Train_x_2=sparse.hstack((cv_Train_x_2,tr_x))\n",
    "        cv_Test_x_2=sparse.hstack((cv_Test_x_2,te_x))\n",
    "    \n",
    "print(cv_Train_x_2.shape,cv_Test_x_2.shape)\n",
    "\n",
    "\n",
    "\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "train_x=df_train[features].values\n",
    "test_x=df_test[features].values\n",
    "train_y=df_train['target'].values\n",
    "\n",
    "train_x=sparse.hstack((train_x,cv_Train_x_2),'csr')\n",
    "test_x=sparse.hstack((test_x,cv_Test_x_2),'csr')\n",
    "oof_train3 = np.zeros((train_x.shape[0],1))\n",
    "oof_test3 = np.zeros((test_x.shape[0],1))\n",
    "oof_test3_skf = np.zeros((NFOLD,test_x.shape[0],))\n",
    "\n",
    "print(train_x.shape,test_x.shape)\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(skf.split(train_x,df_train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    x_tr,y_tr,x_te,y_te=train_x[trn_idx],train_y[trn_idx],train_x[val_idx],train_y[val_idx]\n",
    "    trn_data = lgb.Dataset(x_tr,y_tr)\n",
    "    val_data = lgb.Dataset(x_te,y_te)\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 500)\n",
    "    oof_train3[val_idx] = clf.predict(x_te, num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "\n",
    "    oof_test3_skf[fold_,:]= clf.predict(test_x, num_iteration=clf.best_iteration)    \n",
    "oof_test3 = oof_test3_skf.mean(axis=0)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train3.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "\n",
    "\n",
    "sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = oof_test3\n",
    "sub_df.to_csv(\"sub/submission_%s.csv\"%mean_loss, index=False)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train3.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "train_prob=pd.DataFrame(oof_train3)\n",
    "train_prob.columns=['class1']\n",
    "train_prob.to_csv(\"oof/train_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "test_prob=pd.DataFrame(oof_test3)\n",
    "test_prob.columns=['class1']\n",
    "test_prob.to_csv(\"oof/test_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "np.save(\"train_y\",target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T02:50:44.180Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917, 253) (123623, 253)\n",
      "(201917, 494735) (123623, 494735)\n",
      "(201917, 728276) (123623, 728276)\n",
      "fold 0\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.56265\tvalid_1's rmse: 3.67707\n",
      "[400]\ttraining's rmse: 3.46449\tvalid_1's rmse: 3.65396\n",
      "[600]\ttraining's rmse: 3.40546\tvalid_1's rmse: 3.64799\n",
      "[800]\ttraining's rmse: 3.36073\tvalid_1's rmse: 3.64678\n",
      "[1000]\ttraining's rmse: 3.3234\tvalid_1's rmse: 3.64735\n",
      "[1200]\ttraining's rmse: 3.28849\tvalid_1's rmse: 3.64676\n",
      "[1400]\ttraining's rmse: 3.25596\tvalid_1's rmse: 3.647\n",
      "[1600]\ttraining's rmse: 3.22572\tvalid_1's rmse: 3.64757\n",
      "Early stopping, best iteration is:\n",
      "[1254]\ttraining's rmse: 3.27977\tvalid_1's rmse: 3.64637\n",
      "fold 1\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.56243\tvalid_1's rmse: 3.67414\n",
      "[400]\ttraining's rmse: 3.46314\tvalid_1's rmse: 3.65449\n",
      "[600]\ttraining's rmse: 3.40402\tvalid_1's rmse: 3.6491\n",
      "[800]\ttraining's rmse: 3.3593\tvalid_1's rmse: 3.64731\n",
      "[1000]\ttraining's rmse: 3.32062\tvalid_1's rmse: 3.64751\n",
      "[1200]\ttraining's rmse: 3.28623\tvalid_1's rmse: 3.64662\n",
      "[1400]\ttraining's rmse: 3.25315\tvalid_1's rmse: 3.64754\n",
      "[1600]\ttraining's rmse: 3.22292\tvalid_1's rmse: 3.64781\n",
      "Early stopping, best iteration is:\n",
      "[1200]\ttraining's rmse: 3.28623\tvalid_1's rmse: 3.64662\n",
      "fold 2\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.56049\tvalid_1's rmse: 3.68319\n",
      "[400]\ttraining's rmse: 3.46115\tvalid_1's rmse: 3.65908\n",
      "[600]\ttraining's rmse: 3.40154\tvalid_1's rmse: 3.65253\n",
      "[800]\ttraining's rmse: 3.35874\tvalid_1's rmse: 3.64922\n",
      "[1000]\ttraining's rmse: 3.32014\tvalid_1's rmse: 3.64804\n",
      "[1200]\ttraining's rmse: 3.28486\tvalid_1's rmse: 3.64752\n",
      "[1400]\ttraining's rmse: 3.25323\tvalid_1's rmse: 3.647\n",
      "[1600]\ttraining's rmse: 3.22035\tvalid_1's rmse: 3.64647\n",
      "[1800]\ttraining's rmse: 3.19134\tvalid_1's rmse: 3.64641\n",
      "[2000]\ttraining's rmse: 3.1625\tvalid_1's rmse: 3.64626\n",
      "[2200]\ttraining's rmse: 3.13557\tvalid_1's rmse: 3.64639\n",
      "[2400]\ttraining's rmse: 3.1103\tvalid_1's rmse: 3.647\n",
      "Early stopping, best iteration is:\n",
      "[2054]\ttraining's rmse: 3.15501\tvalid_1's rmse: 3.64604\n",
      "fold 3\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.55769\tvalid_1's rmse: 3.68444\n",
      "[400]\ttraining's rmse: 3.45909\tvalid_1's rmse: 3.66077\n",
      "[600]\ttraining's rmse: 3.3997\tvalid_1's rmse: 3.65338\n",
      "[800]\ttraining's rmse: 3.35511\tvalid_1's rmse: 3.65\n",
      "[1000]\ttraining's rmse: 3.31674\tvalid_1's rmse: 3.64808\n",
      "[1200]\ttraining's rmse: 3.2819\tvalid_1's rmse: 3.64794\n",
      "[1400]\ttraining's rmse: 3.24965\tvalid_1's rmse: 3.6478\n",
      "[1600]\ttraining's rmse: 3.219\tvalid_1's rmse: 3.64774\n",
      "[1800]\ttraining's rmse: 3.1899\tvalid_1's rmse: 3.64755\n",
      "[2000]\ttraining's rmse: 3.16223\tvalid_1's rmse: 3.64808\n",
      "[2200]\ttraining's rmse: 3.13648\tvalid_1's rmse: 3.64844\n",
      "Early stopping, best iteration is:\n",
      "[1700]\ttraining's rmse: 3.20336\tvalid_1's rmse: 3.64737\n",
      "fold 4\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 3.5595\tvalid_1's rmse: 3.67286\n",
      "[400]\ttraining's rmse: 3.46146\tvalid_1's rmse: 3.65416\n",
      "[600]\ttraining's rmse: 3.40459\tvalid_1's rmse: 3.64987\n",
      "[800]\ttraining's rmse: 3.36065\tvalid_1's rmse: 3.64761\n",
      "[1000]\ttraining's rmse: 3.32068\tvalid_1's rmse: 3.64652\n",
      "[1200]\ttraining's rmse: 3.2848\tvalid_1's rmse: 3.64631\n",
      "[1400]\ttraining's rmse: 3.25179\tvalid_1's rmse: 3.64612\n",
      "[1600]\ttraining's rmse: 3.22032\tvalid_1's rmse: 3.64643\n",
      "[1800]\ttraining's rmse: 3.19184\tvalid_1's rmse: 3.64664\n",
      "Early stopping, best iteration is:\n",
      "[1439]\ttraining's rmse: 3.24537\tvalid_1's rmse: 3.64601\n",
      "3.646479485310898\n",
      "3.646479485310898\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "train_x=df_train[features].values\n",
    "test_x=df_test[features].values\n",
    "print(train_x.shape,test_x.shape)\n",
    "train_y= df_train['target'].values\n",
    "train_x = sparse.hstack((train_x,cv_Train_x),'csr')\n",
    "test_x = sparse.hstack((test_x,cv_Test_x),'csr')\n",
    "print(train_x.shape,test_x.shape)\n",
    "\n",
    "train_x=sparse.hstack((train_x,cv_Train_x_2),'csr')\n",
    "test_x=sparse.hstack((test_x,cv_Test_x_2),'csr')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "oof_train4 = np.zeros((train_x.shape[0],1))\n",
    "oof_test4 = np.zeros((test_x.shape[0],1))\n",
    "oof_test4_skf = np.zeros((NFOLD,test_x.shape[0],))\n",
    "\n",
    "print(train_x.shape,test_x.shape)\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(skf.split(train_x,df_train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    \n",
    "\n",
    "    x_tr,y_tr,x_te,y_te=train_x[trn_idx],train_y[trn_idx],train_x[val_idx],train_y[val_idx]\n",
    "    trn_data = lgb.Dataset(x_tr,y_tr)\n",
    "    val_data = lgb.Dataset(x_te,y_te)\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 500)\n",
    "    oof_train4[val_idx] = clf.predict(x_te, num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "\n",
    "    oof_test4_skf[fold_,:]= clf.predict(test_x, num_iteration=clf.best_iteration)    \n",
    "oof_test4 = oof_test4_skf.mean(axis=0)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train4.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "\n",
    "\n",
    "sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = oof_test4\n",
    "sub_df.to_csv(\"sub/submission_%s.csv\"%mean_loss, index=False)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train4.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "train_prob=pd.DataFrame(oof_train4)\n",
    "train_prob.columns=['class1']\n",
    "train_prob.to_csv(\"oof/train_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "test_prob=pd.DataFrame(oof_test4)\n",
    "test_prob.columns=['class1']\n",
    "test_prob.to_csv(\"oof/test_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "np.save(\"train_y\",target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T02:50:44.181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 201912, 201913, 201916])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T02:50:44.185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917, 4) (123623, 4)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "3.6419621998951706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "train_stack = np.concatenate([oof_train,oof_train2,oof_train3,oof_train4],axis=1)\n",
    "test_stack = np.concatenate([oof_test.reshape(-1,1),oof_test2.reshape(-1,1),\\\n",
    "                             oof_test3.reshape(-1,1),oof_test4.reshape(-1,1)],axis=1)\n",
    "print(train_stack.shape,test_stack.shape)\n",
    "\n",
    "oof_stack = np.zeros(train_stack.shape[0])\n",
    "predictions_3 = np.zeros(test_stack.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(skf.split(train_stack,df_train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "\n",
    "    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n",
    "    \n",
    "    clf_3 = BayesianRidge()\n",
    "    clf_3.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack[val_idx] = clf_3.predict(val_data)\n",
    "    predictions_3 += clf_3.predict(test_stack) / 5\n",
    "    \n",
    "stack_loss=np.sqrt(mean_squared_error(target.values, oof_stack))\n",
    "print(stack_loss)\n",
    "sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = predictions_3\n",
    "sub_df.to_csv(\"sub/submission_%s.csv\"%stack_loss, index=False)\n",
    "\n",
    "train_prob=pd.DataFrame(oof_stack)\n",
    "train_prob.columns=['class1']\n",
    "train_prob.to_csv(\"oof/train_prob_%s.csv\"%stack_loss,index=False)\n",
    "\n",
    "test_prob=pd.DataFrame(predictions_3)\n",
    "test_prob.columns=['class1']\n",
    "test_prob.to_csv(\"oof/test_prob_%s.csv\"%stack_loss,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
