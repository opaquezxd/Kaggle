{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca59b28296f56731e1969f765cfb902ce1b50cf2"
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Here we'll engineer features having to do w/ transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:15.748613Z",
     "start_time": "2019-02-22T14:48:14.969972Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "# Plot settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3025e16aeaf61d8802d2699419ea609cadf97c63"
   },
   "source": [
    "## Load card data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:27.117277Z",
     "start_time": "2019-02-22T14:48:26.734055Z"
    },
    "_uuid": "07ade37022948fb3c267ac1333d9aab9d6426d7b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load card data\n",
    "# no nulls in test/train except for ONE ROW in test! (first_active_month)\n",
    "dtypes = {\n",
    "  'card_id':            'str',     # 201917 unique vals\n",
    "  'target':             'float32', # -33.22 thru ~18\n",
    "  'first_active_month': 'str',     # 2011-10 thru 2018-02\n",
    "  'feature_1':          'uint8',   # 1 thru 5\n",
    "  'feature_2':          'uint8',   # 1 thru 3\n",
    "  'feature_3':          'uint8',   # 0 and 1\n",
    "}\n",
    "train = pd.read_csv('../input/train.csv',\n",
    "                    usecols=dtypes.keys(),\n",
    "                    dtype=dtypes)\n",
    "del dtypes['target']\n",
    "test = pd.read_csv('../input/test.csv',\n",
    "                   usecols=dtypes.keys(),\n",
    "                   dtype=dtypes)\n",
    "\n",
    "# Add target col to test\n",
    "test['target'] = np.nan\n",
    "\n",
    "# Merge test and train\n",
    "cards = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8ce1a9a9c012084a21c7804acd3bba07502d8a7"
   },
   "source": [
    "As a quick sanity check, let's make sure there aren't any `card_id`s which are in both test and train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:30.089506Z",
     "start_time": "2019-02-22T14:48:29.809496Z"
    },
    "_uuid": "b76ae3cf90d2cc15adeebf856aa0d9ace06bc6cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num unique in train:   123623\n",
      "Num unique in test:    201917\n",
      "The sum:               325540\n",
      "Num unique in merged:  325540\n"
     ]
    }
   ],
   "source": [
    "print('Num unique in train:  ', test['card_id'].nunique())\n",
    "print('Num unique in test:   ', train['card_id'].nunique())\n",
    "print('The sum:              ', test['card_id'].nunique()+train['card_id'].nunique())\n",
    "print('Num unique in merged: ', cards['card_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf67d10e5d13fa0a7fdf185ccf10907075a35b3c"
   },
   "source": [
    "OK good, there aren't.  Now we can delete the original dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:32.046112Z",
     "start_time": "2019-02-22T14:48:31.992196Z"
    },
    "_uuid": "b2870cb16dd6e2747ccb314757b490cd523e85dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f4d7b98cdd78c409057dda5568ac1a57e15ce996"
   },
   "source": [
    "Let's take a look at the cards data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:34.323229Z",
     "start_time": "2019-02-22T14:48:34.298157Z"
    },
    "_uuid": "12171ea79dc554da5ff4648ab400de2b31a8af02"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75259</th>\n",
       "      <td>2016-06</td>\n",
       "      <td>C_ID_f0369a00ae</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155110</th>\n",
       "      <td>2017-07</td>\n",
       "      <td>C_ID_66614b94e5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.302811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39264</th>\n",
       "      <td>2017-09</td>\n",
       "      <td>C_ID_c25c000626</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37005</th>\n",
       "      <td>2017-09</td>\n",
       "      <td>C_ID_05d9083ec0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153220</th>\n",
       "      <td>2013-03</td>\n",
       "      <td>C_ID_8c7b169d89</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.503706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112297</th>\n",
       "      <td>2017-09</td>\n",
       "      <td>C_ID_115a1093d4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57195</th>\n",
       "      <td>2017-11</td>\n",
       "      <td>C_ID_7747475439</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51406</th>\n",
       "      <td>2016-07</td>\n",
       "      <td>C_ID_e36050d198</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.983796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77398</th>\n",
       "      <td>2016-04</td>\n",
       "      <td>C_ID_7e8f7e2ff2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.368944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54018</th>\n",
       "      <td>2015-06</td>\n",
       "      <td>C_ID_fbf259addb</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.732663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "75259             2016-06  C_ID_f0369a00ae          3          3          1   \n",
       "155110            2017-07  C_ID_66614b94e5          2          1          0   \n",
       "39264             2017-09  C_ID_c25c000626          3          1          1   \n",
       "37005             2017-09  C_ID_05d9083ec0          3          1          1   \n",
       "153220            2013-03  C_ID_8c7b169d89          3          3          1   \n",
       "112297            2017-09  C_ID_115a1093d4          3          1          1   \n",
       "57195             2017-11  C_ID_7747475439          1          1          0   \n",
       "51406             2016-07  C_ID_e36050d198          2          2          0   \n",
       "77398             2016-04  C_ID_7e8f7e2ff2          4          2          0   \n",
       "54018             2015-06  C_ID_fbf259addb          3          1          1   \n",
       "\n",
       "          target  \n",
       "75259        NaN  \n",
       "155110  0.302811  \n",
       "39264        NaN  \n",
       "37005        NaN  \n",
       "153220 -4.503706  \n",
       "112297       NaN  \n",
       "57195        NaN  \n",
       "51406   1.983796  \n",
       "77398  -0.368944  \n",
       "54018   2.732663  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b673e116e922ffefad18f44a73e3401fe9c3940"
   },
   "source": [
    "The `card_id`s always start with `C_ID_`.  That's kind of a waste of space...  All the `card_id`s are the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:36.585313Z",
     "start_time": "2019-02-22T14:48:36.489837Z"
    },
    "_uuid": "a3c53b0ba677a7e421e3e67228fd9e28b2cd5057"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards['card_id'].apply(len).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ce2e1266b31d7d9a36d44b798804711c3a8bdae"
   },
   "source": [
    "If we cut off the prefix, the remaining strings appear to be all hexidecimal (represented by values 0-9 and a-f):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:39.096997Z",
     "start_time": "2019-02-22T14:48:38.992928Z"
    },
    "_uuid": "278f35e45e68e72afd3bd07bb8efcdb106527904",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61824     861c74cde7\n",
       "70671     f304fb16d0\n",
       "5971      0c2ab9e79e\n",
       "87727     8b568439a7\n",
       "10194     eb710aca31\n",
       "8346      5b5be2ee4d\n",
       "105052    552097e0bd\n",
       "4910      9ad13db507\n",
       "54058     5b45b2ad2b\n",
       "97197     eab7d37d7a\n",
       "Name: card_id, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards['card_id'].str.slice(5, 15).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65f1248055fecc09f62d7ddb34f91fac5a33a71c"
   },
   "source": [
    "We can check that every single entry is indeed a hexideximal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:41.653665Z",
     "start_time": "2019-02-22T14:48:41.132125Z"
    },
    "_uuid": "9e330cb0229a866eb7f846cc6927d2c40696d10a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cards['card_id']\n",
    " .str.slice(5, 15)\n",
    " .apply(lambda x: all(e in '0123456789abcdef' for e in x))\n",
    " .all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af22570611dc7881508a100be2110415f8dcd321"
   },
   "source": [
    "To save space, we could convert the `card_id`s to integers like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:43.694493Z",
     "start_time": "2019-02-22T14:48:43.690767Z"
    },
    "_uuid": "9a69779b312d54b50bc6b9e383a2388a9d8c66d9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cards['card_id'] = cards['card_id'].apply(lambda x: int(x, 16)).astype('uint64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d3473f62d90473fedd174b387fd28ff1a67ac8f"
   },
   "source": [
    "There are 5 bytes worth of hex info in the card ids, and the values span the full 5-byte range, so we'd have to use a 64-bit integer to represent them (instead of a 32-bit int, which is only 4 bytes).  BUT, there are only 201,917 unique `card_id`s in `train.csv` and 123,623 in `test.csv`, which can easily be represented by a `uint32` (which stores values up to ~4 billion).  The inneficiency of using a 64-bit representation for something where a 32-bit reprentation would do... Bothers me. \\*eye twitches\\*\n",
    "\n",
    "To use a 32-bit integer, we'll create a map between the card_id and a unique integer which identifies it, and then map the string values to integer values.  (we need to create a map so that we can map the values in the transactions data in the exact same way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:48.010056Z",
     "start_time": "2019-02-22T14:48:46.769782Z"
    },
    "_uuid": "5dafcb139377895cf8412c5487a2f3dbf430902a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a map from card_id to unique int\n",
    "card_id_map = dict(zip(\n",
    "    cards['card_id'].values,\n",
    "    cards['card_id'].astype('category').cat.codes.values\n",
    "))\n",
    "\n",
    "# Map the values\n",
    "cards['card_id'] = cards['card_id'].map(card_id_map).astype('uint32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "160c3c16b3b1b90d846efb5036eb7c1790185984"
   },
   "source": [
    "Now our `card_id`s are 32-bit integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:51.362564Z",
     "start_time": "2019-02-22T14:48:51.342033Z"
    },
    "_uuid": "9d95062f55050052711b6af3a27bb8e09072bf74"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74684</th>\n",
       "      <td>2017-11</td>\n",
       "      <td>298531</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6350</th>\n",
       "      <td>2017-09</td>\n",
       "      <td>75404</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104725</th>\n",
       "      <td>2016-11</td>\n",
       "      <td>34051</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.753118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158746</th>\n",
       "      <td>2017-04</td>\n",
       "      <td>153471</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.143921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11895</th>\n",
       "      <td>2017-12</td>\n",
       "      <td>228282</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.468371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197870</th>\n",
       "      <td>2017-11</td>\n",
       "      <td>318259</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.039006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30432</th>\n",
       "      <td>2017-04</td>\n",
       "      <td>173816</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.015747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123614</th>\n",
       "      <td>2015-09</td>\n",
       "      <td>189848</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.046738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45846</th>\n",
       "      <td>2017-12</td>\n",
       "      <td>243132</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67864</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>135812</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.903769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       first_active_month  card_id  feature_1  feature_2  feature_3    target\n",
       "74684             2017-11   298531          2          3          0       NaN\n",
       "6350              2017-09    75404          2          1          0       NaN\n",
       "104725            2016-11    34051          3          2          1 -0.753118\n",
       "158746            2017-04   153471          2          1          0  1.143921\n",
       "11895             2017-12   228282          3          1          1 -5.468371\n",
       "197870            2017-11   318259          4          1          0 -1.039006\n",
       "30432             2017-04   173816          4          1          0  2.015747\n",
       "123614            2015-09   189848          3          1          1  1.046738\n",
       "45846             2017-12   243132          3          2          1       NaN\n",
       "67864             2017-01   135812          4          2          0 -0.903769"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bbac4ee0c0937a2832f7729ba4d38e6f667b1ef0"
   },
   "source": [
    "Next, we'll convert the `first_active_month` from a string to a datetime.  Unfortunately, there is one.  Single.  Row.  Where `first_active_month` is NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:54.196853Z",
     "start_time": "2019-02-22T14:48:54.170512Z"
    },
    "_uuid": "d4298d659e2a23b8deec97d753d8571128f4818f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11578</th>\n",
       "      <td>NaN</td>\n",
       "      <td>247334</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      first_active_month  card_id  feature_1  feature_2  feature_3  target\n",
       "11578                NaN   247334          5          2          1     NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards[cards['first_active_month'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d49733366e4108f0b532bff24068c002e3c4791"
   },
   "source": [
    "But other than that row, all the other `first_active_months` are in `YYYY-MM` format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:57.832168Z",
     "start_time": "2019-02-22T14:48:57.650272Z"
    },
    "_uuid": "efeabddcfb608d8bcbfad597ce7200412bf37bae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 'NaN'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nanlen(x):\n",
    "    if type(x) is float:\n",
    "        return 'NaN'\n",
    "    else:\n",
    "        return len(x)\n",
    "    \n",
    "cards['first_active_month'].apply(nanlen).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df9651365b8de8afab26adc200cddeea9d4e9218"
   },
   "source": [
    "So, we can convert `first_active_month` column to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:48:59.701413Z",
     "start_time": "2019-02-22T14:48:59.610347Z"
    },
    "_uuid": "cc7a093827f3def87eff909c32fb2ced352f694c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert first_active_month to datetime\n",
    "cards['first_active_month'] = pd.to_datetime(cards['first_active_month'],\n",
    "                                             format='%Y-%m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "747cddbc039ccc53b5ef7b67e3ac8a2b20d32c4b"
   },
   "source": [
    "Finally, we'll set the index to be the `card_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:49:02.005365Z",
     "start_time": "2019-02-22T14:49:01.933549Z"
    },
    "_uuid": "323a74283d67981282f0182a2983be02310258e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>141018</th>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.172728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197245</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81007</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.796653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295369</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247523</th>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.817348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80428</th>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260650</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216439</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.030231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3250</th>\n",
       "      <td>2015-08-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.530849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239644</th>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.545609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        first_active_month  feature_1  feature_2  feature_3    target\n",
       "card_id                                                              \n",
       "141018          2017-03-01          3          2          1  5.172728\n",
       "197245          2017-09-01          2          3          0       NaN\n",
       "81007           2016-04-01          2          2          0 -0.796653\n",
       "295369          2017-01-01          3          2          1       NaN\n",
       "247523          2015-12-01          2          2          0 -4.817348\n",
       "80428           2016-02-01          5          1          1       NaN\n",
       "260650          2017-10-01          3          2          1       NaN\n",
       "216439          2017-12-01          2          1          0 -0.030231\n",
       "3250            2015-08-01          5          1          1 -0.530849\n",
       "239644          2017-03-01          2          1          0  2.545609"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make card_id the index\n",
    "cards.set_index('card_id', inplace=True)\n",
    "gc.collect()\n",
    "cards.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "100ac408ef8a7d7eadb68955bdece284e600ac7b"
   },
   "source": [
    "## Load Merchants Data\n",
    "\n",
    "NOTE: some cols are in both merchants dataset and the transactions datasets, and the values differ.  Probably those are values which are properties of the merchants but have changed between the time the purchase was made and the time at which the merchants table was compiled.  So, for per-transaction information we'll use the version of the information in the transactions dataset, not the info from the merchants dataset.  (I.e., that's why we don't read some cols in below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:49:05.775247Z",
     "start_time": "2019-02-22T14:49:04.807853Z"
    },
    "_uuid": "d5f1d7f17691b61265b1caa0309f1cebf01385b1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Datatypes of each column\n",
    "# (don't load cols which are in transactions data, just use those vals)\n",
    "# Nulls: NO nulls except for 13 rows in avg_sales_lag{3,6,12}\n",
    "dtypes = {\n",
    "  'merchant_id':                 'str',     # 334633 unique values\n",
    "  'merchant_group_id':           'uint32',  # 1 thru 112586 (w/ some missing, ~109k uniques)\n",
    "  'numerical_1':                 'float32', # ~ -0.06 thru ~ 183.8 (only 951 unique vals?)\n",
    "  'numerical_2':                 'float32', # roughly the same as above\n",
    "  'most_recent_sales_range':     'str',     # A, B, C, D, or E\n",
    "  'most_recent_purchases_range': 'str',     # A, B, C, D, or E\n",
    "  'avg_sales_lag3':              'float32', # most between 0 and 2, if you transform by 1/x, all but 3 are between 0 and 4\n",
    "  'avg_purchases_lag3':          'float32', # most between 0 and 2, if you transform by 1/x, all but 3 are between 0 and 4\n",
    "  'active_months_lag3':          'uint8',   # 1 to 3 \n",
    "  'avg_sales_lag6':              'float32', # similar to avg_sales_lag3\n",
    "  'avg_purchases_lag6':          'float32', # similar to avg_purchases_lag3\n",
    "  'active_months_lag6':          'uint8',   # 1 to 6\n",
    "  'avg_sales_lag12':             'float32', # similar to avg_sales_lag3\n",
    "  'avg_purchases_lag12':         'float32', # similar to avg_purchases_lag3\n",
    "  'active_months_lag12':         'uint8',   # 1 to 12\n",
    "  'category_4':                  'str',     # Y or N\n",
    "}\n",
    "\n",
    "# Load the data\n",
    "merchants = pd.read_csv('../input/merchants.csv',\n",
    "                        usecols=dtypes.keys(),\n",
    "                        dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:49:08.752724Z",
     "start_time": "2019-02-22T14:49:07.938239Z"
    },
    "_uuid": "f0cd00b858d648cf198b51434fa8b4cc25a1c636",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map merchant_id to integer\n",
    "merch_id_map = dict(zip(\n",
    "    merchants['merchant_id'].values,\n",
    "    merchants['merchant_id'].astype('category').cat.codes.values\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:49:16.554858Z",
     "start_time": "2019-02-22T14:49:15.809772Z"
    },
    "_uuid": "dc90bad0594ca99651196a3a3dbc7518b93f37eb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_merch_data(df):\n",
    "    \n",
    "    # Convert merchant ID to numbers\n",
    "    df['merchant_id'] = df['merchant_id'].map(merch_id_map).astype('float32')\n",
    "\n",
    "    # Inverse transforms\n",
    "    inversions = [\n",
    "        'avg_sales_lag3',\n",
    "        'avg_sales_lag6',\n",
    "        'avg_sales_lag12',\n",
    "        'avg_purchases_lag3',\n",
    "        'avg_purchases_lag6',\n",
    "        'avg_purchases_lag12',\n",
    "    ]\n",
    "    for col in inversions:\n",
    "        df[col] = 1.0/df[col]\n",
    "\n",
    "    # Encode categorical columns\n",
    "    bool_map = {'Y': 1, 'N': 0}\n",
    "    five_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "    conversions = [\n",
    "        ('category_4', bool_map, 'uint8'),\n",
    "        ('most_recent_sales_range', five_map, 'uint8'),\n",
    "        ('most_recent_purchases_range', five_map, 'uint8')\n",
    "    ]\n",
    "    for col, mapper, new_type in conversions:\n",
    "        df[col] = df[col].map(mapper).astype(new_type)\n",
    "        \n",
    "    # Clean up\n",
    "    gc.collect()\n",
    "\n",
    "# Preprocess the merchants data\n",
    "preprocess_merch_data(merchants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4ea201480b33d5870823ff1663f9e757cebca24"
   },
   "source": [
    "## Load Transactions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T14:49:23.050Z"
    },
    "_uuid": "a93bbbca484035d9aaa75708900be4422798fe6d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Datatypes of each column\n",
    "# only NaNs are in category_3, merchant_id, and category_2\n",
    "dtypes = {\n",
    "    'authorized_flag':      'str',     # Y or N\n",
    "    'card_id':              'str',     # 325540 unique values\n",
    "    'city_id':              'int16',   # -1 then 1 to 347 (is -1 supposed to be nan?)\n",
    "    'category_1':           'str',     # Y or N\n",
    "    'installments':         'int8',    # -25, then -1 thru 12 (-1 supposed to be nan?)\n",
    "    'category_3':           'str',     # A, B, C, and nan (ordinal?)\n",
    "    'merchant_category_id': 'int16',   # 2 to 891\n",
    "    'merchant_id':          'str',     # 334633 unique values and nans (164697 nans!)\n",
    "    'month_lag':            'int8',    # -13 thru 0\n",
    "    'purchase_amount':      'float32', # min: -0.746, med: -0.699, max: 11269.667\n",
    "    'purchase_date':        'str',     # YYYY-MM-DD hh:mm:ss\n",
    "    'category_2':           'float32', # 1 thru 5 and nan (ordinal?)\n",
    "    'state_id':             'int8',    # -1 then 1 thru 24\n",
    "    'subsector_id':         'int8'     # 1 thru 41\n",
    "}\n",
    "\n",
    "# Load the data\n",
    "hist_trans = pd.read_csv('../input/historical_transactions.csv', \n",
    "                         usecols=dtypes.keys(),\n",
    "                         dtype=dtypes)\n",
    "new_trans = pd.read_csv('../input/new_merchant_transactions.csv', \n",
    "                        usecols=dtypes.keys(),\n",
    "                        dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3734555f765e3b89a2da508add95afd832dcbdf7"
   },
   "source": [
    "We also need to convert the `card_id` to an integer as before, `merchant_id` to an integer in the same way, convert the `purchase_date` column (which has been loaded as a string) to datetime format, and encode the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T14:49:35.161Z"
    },
    "_uuid": "a76537c42819263aaef08950c03c7bd7987b90b3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_trans_data(df):\n",
    "    \n",
    "    # Convert card_id and merchant_id to numbers\n",
    "    df['card_id'] = df['card_id'].map(card_id_map).astype('uint32')\n",
    "    df['merchant_id'] = df['merchant_id'].map(merch_id_map).astype('float32')\n",
    "\n",
    "    # Convert purchase_date to datetime\n",
    "    df['purchase_date'] = df['purchase_date'].str.slice(0, 19)\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'],\n",
    "                                         format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Encode categorical columns\n",
    "    bool_map = {'Y': 1, 'N': 0}\n",
    "    three_map = {'A': 0, 'B': 1, 'C': 2}\n",
    "    conversions = [\n",
    "        ('authorized_flag', bool_map, 'uint8'),\n",
    "        ('category_1', bool_map, 'uint8'),\n",
    "        ('category_3', three_map, 'float32'), #has NaNs so have to use float\n",
    "    ]\n",
    "    for col, mapper, new_type in conversions:\n",
    "        df[col] = df[col].map(mapper).astype(new_type)\n",
    "        \n",
    "    # Clean up\n",
    "    gc.collect()\n",
    "\n",
    "# Preprocess the transactions data\n",
    "preprocess_trans_data(hist_trans)\n",
    "preprocess_trans_data(new_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cdbee816b5f7c92d83d64289073a4ab14cf6fa4c"
   },
   "source": [
    "## Merge merchants with transactions data\n",
    "\n",
    "For each transaction, we need to add the information about the merchant involved in that transaction to the transactions dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T14:49:43.214Z"
    },
    "_uuid": "52eda494151d06e24c167f4c9d5f4662c6aff4e6"
   },
   "outputs": [],
   "source": [
    "# Merge transactions with merchants data\n",
    "hist_trans = pd.merge(hist_trans, merchants, on='merchant_id')\n",
    "new_trans = pd.merge(new_trans, merchants, on='merchant_id')\n",
    "del merchants\n",
    "gc.collect()\n",
    "\n",
    "hist_trans.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f8a69ce9b14c80e7c0a8e87a1078ee41c617dc8"
   },
   "source": [
    "## Feature Engineering (for Transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e5fdba1e41ba6d0d8de535a560062cbab58454c"
   },
   "source": [
    "First let's one-hot `category_2` and `category_3`, while keeping the original column (since they may be ordinal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T14:49:47.399Z"
    },
    "_uuid": "07cf2f2cb3f71acdc76b8c0f5dd086bd8efedce2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_one_hot(df, col):\n",
    "    \"\"\"Add one-hot columns\"\"\"\n",
    "    ucats = df[col].unique() #unique categories\n",
    "    ucats = ucats[~np.isnan(ucats)] #that aren't nan\n",
    "    for oh_col in ucats:\n",
    "        df[col+'_'+str(int(oh_col))] = (df[col] == oh_col).astype('uint8')\n",
    "        \n",
    "for df in [hist_trans, new_trans]:\n",
    "    add_one_hot(df, 'category_2')\n",
    "    add_one_hot(df, 'category_3')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "16fb02b581f368ffa8655d91297424c56585f5c2"
   },
   "source": [
    "First, let's create some features involving when the transactions occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T14:49:50.039Z"
    },
    "_uuid": "5a3a1f7c5aa90fcb98b9847e9eaa5b9f9be0e8e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute features from purchase time/date\n",
    "for df in [hist_trans, new_trans]:\n",
    "    tpd = df['purchase_date']\n",
    "    ref_date = np.datetime64('2017-09-01')\n",
    "    one_hour = np.timedelta64(1, 'h')\n",
    "    df['purchase_hour'] = tpd.dt.hour.astype('uint8')\n",
    "    df['purchase_day'] = tpd.dt.dayofweek.astype('uint8')\n",
    "    df['purchase_week'] = tpd.dt.weekofyear.astype('uint8')\n",
    "    df['purchase_month'] = tpd.dt.month.astype('uint8')\n",
    "    df['purchase_weekend'] = (df['purchase_day'] >=5 ).astype('uint8')\n",
    "    df['purchase_time'] = ((tpd - ref_date) / one_hour).astype('float32')\n",
    "    df['ref_date'] = ((tpd - pd.to_timedelta(df['month_lag'], 'M')\n",
    "                          - ref_date ) / one_hour).astype('float32')\n",
    "\n",
    "    # Time sime first active\n",
    "    tsfa = pd.merge(df[['card_id']], \n",
    "                    cards[['first_active_month']].copy().reset_index(),\n",
    "                    on='card_id', how='left')\n",
    "    df['time_since_first_active'] = ((tpd - tsfa['first_active_month'])\n",
    "                                     / one_hour).astype('float32')\n",
    "\n",
    "    # TODO: whether the day is a holiday or week before a holiday or something?\n",
    "\n",
    "    # Clean up\n",
    "    del tsfa\n",
    "    del df['purchase_date']\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf6d299695ba0173cbe8ca4809827f44468cb4f4"
   },
   "source": [
    "Now we can convert `first_active_month` to months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T14:49:56.005Z"
    },
    "_uuid": "75fa33d2e2bb70e5e9e4b19b4d196a40d5147dc2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cards['first_active_month'] = (12*(cards['first_active_month'].dt.year-2011) + \n",
    "                               cards['first_active_month'].dt.month).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T14:49:56.955Z"
    },
    "_uuid": "d88b8aa65f857a8361f1a0c94a10d8fa5b5468e6"
   },
   "outputs": [],
   "source": [
    "hist_trans.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "738047202cb3513d807d67a0d923393c0aeef271"
   },
   "source": [
    "## Feature Engineering (aggregations)\n",
    "\n",
    "Now we engineer features for each card ID by applying aggregation functions to each card's transactions data.  First we need to group the transactions by `card_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T14:50:01.939Z"
    },
    "_uuid": "e4d391b3eae5d9adca90108d4be2228398585a38",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group transactions by card id\n",
    "hist_trans = hist_trans.groupby('card_id', sort=False)\n",
    "new_trans = new_trans.groupby('card_id', sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ca752e4fdb1d54a48bfd851d60e49f67e71b573"
   },
   "source": [
    "First let's define some custom aggregation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T14:50:03.359Z"
    },
    "_uuid": "41f81d8477a10340e5eaa4fe7feb3d48dafb2b3d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(series):\n",
    "    \"\"\"Categorical entropy\"\"\"\n",
    "    probs = series.value_counts().values.astype('float32')\n",
    "    probs = probs / np.sum(probs)\n",
    "    probs[probs==0] = np.nan\n",
    "    return -np.nansum(probs * np.log2(probs))\n",
    "\n",
    "def mean_diff(series):\n",
    "    \"\"\"Mean difference between consecutive items in a series\"\"\"\n",
    "    ss = series.sort_values()\n",
    "    return (ss - ss.shift()).mean()\n",
    "\n",
    "def period(series):\n",
    "    \"\"\"Period of a series (max-min)\"\"\"\n",
    "    return series.max() - series.min()\n",
    "\n",
    "def mode(series):\n",
    "    \"\"\"Most common element in a series\"\"\"\n",
    "    tmode = series.mode()\n",
    "    if len(tmode) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return tmode[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c9f2bee30f1ae1e1b79001e5b4c59be6b502761"
   },
   "source": [
    "And then we can perform the aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-22T14:50:07.604Z"
    },
    "_uuid": "ae65f60f6c2b7e9a4deb730f87a72182eb600102",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Aggregations to perform for each predictor type\n",
    "binary_aggs = ['sum', 'mean', 'nunique']\n",
    "categorical_aggs = ['nunique', entropy, mode]\n",
    "continuous_aggs = ['min', 'max', 'sum', 'mean', 'std', 'skew', mean_diff, period]\n",
    "\n",
    "# Aggregations to perform on each column\n",
    "aggs = {\n",
    "    'authorized_flag':             binary_aggs,\n",
    "    'city_id':                     categorical_aggs,\n",
    "    'category_1':                  binary_aggs,\n",
    "    'installments':                continuous_aggs,\n",
    "    'category_3':                  categorical_aggs + ['mean'], #might be ordinal\n",
    "    'category_3_0':                ['mean'],\n",
    "    'category_3_1':                ['mean'],\n",
    "    'category_3_2':                ['mean'],\n",
    "    'category_2_1':                ['mean'],\n",
    "    'category_2_2':                ['mean'],\n",
    "    'category_2_3':                ['mean'],\n",
    "    'category_2_4':                ['mean'],\n",
    "    'category_2_5':                ['mean'],\n",
    "    'merchant_category_id':        categorical_aggs,\n",
    "    'merchant_id':                 categorical_aggs,\n",
    "    'month_lag':                   continuous_aggs,\n",
    "    'purchase_amount':             continuous_aggs,\n",
    "    'purchase_time':               continuous_aggs + ['count'],\n",
    "    'purchase_hour':               categorical_aggs + ['mean'],\n",
    "    'purchase_day':                categorical_aggs + ['mean'],\n",
    "    'purchase_week':               categorical_aggs + continuous_aggs,\n",
    "    'purchase_month':              categorical_aggs + continuous_aggs,\n",
    "    'purchase_weekend':            binary_aggs,\n",
    "    'ref_date':                    continuous_aggs,\n",
    "    'time_since_first_active':     continuous_aggs,\n",
    "    'category_2':                  categorical_aggs + ['mean'], #also might be ordinal\n",
    "    'state_id':                    categorical_aggs,\n",
    "    'subsector_id':                categorical_aggs,\n",
    "    'merchant_group_id':           categorical_aggs,\n",
    "    'numerical_1':                 continuous_aggs,\n",
    "    'numerical_2':                 continuous_aggs,\n",
    "    'most_recent_sales_range':     categorical_aggs + ['mean'], #ordinal?\n",
    "    'most_recent_purchases_range': categorical_aggs + ['mean'], #orindal?\n",
    "    'avg_sales_lag3':              continuous_aggs,\n",
    "    'avg_purchases_lag3':          continuous_aggs,\n",
    "    'active_months_lag3':          continuous_aggs,\n",
    "    'avg_sales_lag6':              continuous_aggs,\n",
    "    'avg_purchases_lag6':          continuous_aggs,\n",
    "    'active_months_lag6':          continuous_aggs,\n",
    "    'avg_sales_lag12':             continuous_aggs,\n",
    "    'avg_purchases_lag12':         continuous_aggs,\n",
    "    'active_months_lag12':         continuous_aggs,\n",
    "    'category_4':                  binary_aggs,\n",
    "}\n",
    "\n",
    "# Perform each aggregation\n",
    "for col, funcs in aggs.items():\n",
    "    for func in funcs:\n",
    "        \n",
    "        # Get name of aggregation function\n",
    "        if isinstance(func, str):\n",
    "            func_str = func\n",
    "        else:\n",
    "            func_str = func.__name__\n",
    "            \n",
    "        # Name for new column\n",
    "        new_col = col + '_' + func_str\n",
    "            \n",
    "        # Compute this aggregation\n",
    "        cards['hist_'+new_col] = hist_trans[col].agg(func).astype('float32')\n",
    "        cards['new_'+new_col] = new_trans[col].agg(func).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "b5aaab5fbca0871ea515e060bc251140f4e17f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing new_authorized_flag_mean (only 1 unique value)\n",
      "Removing new_authorized_flag_nunique (only 1 unique value)\n",
      "Removing hist_active_months_lag3_max (only 1 unique value)\n",
      "Removing hist_active_months_lag6_max (only 1 unique value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3430"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_noninformative(df):\n",
    "    \"\"\"Remove non-informative columns (all nan, or all same value)\"\"\"\n",
    "    for col in df:\n",
    "        if df[col].isnull().all():\n",
    "            print('Removing '+col+' (all NaN)')\n",
    "            del df[col]\n",
    "        elif df[col].nunique()<2:\n",
    "            print('Removing '+col+' (only 1 unique value)')\n",
    "            del df[col]\n",
    "\n",
    "remove_noninformative(cards)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "5c950ccd5d5271681a752b0da18729692c59de42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  COLUMN  DATATYPE\n",
      "                      first_active_month  float32\n",
      "                               feature_1  uint8\n",
      "                               feature_2  uint8\n",
      "                               feature_3  uint8\n",
      "                                  target  float32\n",
      "                hist_authorized_flag_sum  float32\n",
      "                 new_authorized_flag_sum  float32\n",
      "               hist_authorized_flag_mean  float32\n",
      "            hist_authorized_flag_nunique  float32\n",
      "                    hist_city_id_nunique  float32\n",
      "                     new_city_id_nunique  float32\n",
      "                    hist_city_id_entropy  float32\n",
      "                     new_city_id_entropy  float32\n",
      "                       hist_city_id_mode  float32\n",
      "                        new_city_id_mode  float32\n",
      "                     hist_category_1_sum  float32\n",
      "                      new_category_1_sum  float32\n",
      "                    hist_category_1_mean  float32\n",
      "                     new_category_1_mean  float32\n",
      "                 hist_category_1_nunique  float32\n",
      "                  new_category_1_nunique  float32\n",
      "                   hist_installments_min  float32\n",
      "                    new_installments_min  float32\n",
      "                   hist_installments_max  float32\n",
      "                    new_installments_max  float32\n",
      "                   hist_installments_sum  float32\n",
      "                    new_installments_sum  float32\n",
      "                  hist_installments_mean  float32\n",
      "                   new_installments_mean  float32\n",
      "                   hist_installments_std  float32\n",
      "                    new_installments_std  float32\n",
      "                  hist_installments_skew  float32\n",
      "                   new_installments_skew  float32\n",
      "             hist_installments_mean_diff  float32\n",
      "              new_installments_mean_diff  float32\n",
      "                hist_installments_period  float32\n",
      "                 new_installments_period  float32\n",
      "                 hist_category_3_nunique  float32\n",
      "                  new_category_3_nunique  float32\n",
      "                 hist_category_3_entropy  float32\n",
      "                  new_category_3_entropy  float32\n",
      "                    hist_category_3_mode  float32\n",
      "                     new_category_3_mode  float32\n",
      "                    hist_category_3_mean  float32\n",
      "                     new_category_3_mean  float32\n",
      "                  hist_category_3_0_mean  float32\n",
      "                   new_category_3_0_mean  float32\n",
      "                  hist_category_3_1_mean  float32\n",
      "                   new_category_3_1_mean  float32\n",
      "                  hist_category_3_2_mean  float32\n",
      "                   new_category_3_2_mean  float32\n",
      "                  hist_category_2_1_mean  float32\n",
      "                   new_category_2_1_mean  float32\n",
      "                  hist_category_2_2_mean  float32\n",
      "                   new_category_2_2_mean  float32\n",
      "                  hist_category_2_3_mean  float32\n",
      "                   new_category_2_3_mean  float32\n",
      "                  hist_category_2_4_mean  float32\n",
      "                   new_category_2_4_mean  float32\n",
      "                  hist_category_2_5_mean  float32\n",
      "                   new_category_2_5_mean  float32\n",
      "       hist_merchant_category_id_nunique  float32\n",
      "        new_merchant_category_id_nunique  float32\n",
      "       hist_merchant_category_id_entropy  float32\n",
      "        new_merchant_category_id_entropy  float32\n",
      "          hist_merchant_category_id_mode  float32\n",
      "           new_merchant_category_id_mode  float32\n",
      "                hist_merchant_id_nunique  float32\n",
      "                 new_merchant_id_nunique  float32\n",
      "                hist_merchant_id_entropy  float32\n",
      "                 new_merchant_id_entropy  float32\n",
      "                   hist_merchant_id_mode  float32\n",
      "                    new_merchant_id_mode  float32\n",
      "                      hist_month_lag_min  float32\n",
      "                       new_month_lag_min  float32\n",
      "                      hist_month_lag_max  float32\n",
      "                       new_month_lag_max  float32\n",
      "                      hist_month_lag_sum  float32\n",
      "                       new_month_lag_sum  float32\n",
      "                     hist_month_lag_mean  float32\n",
      "                      new_month_lag_mean  float32\n",
      "                      hist_month_lag_std  float32\n",
      "                       new_month_lag_std  float32\n",
      "                     hist_month_lag_skew  float32\n",
      "                      new_month_lag_skew  float32\n",
      "                hist_month_lag_mean_diff  float32\n",
      "                 new_month_lag_mean_diff  float32\n",
      "                   hist_month_lag_period  float32\n",
      "                    new_month_lag_period  float32\n",
      "                hist_purchase_amount_min  float32\n",
      "                 new_purchase_amount_min  float32\n",
      "                hist_purchase_amount_max  float32\n",
      "                 new_purchase_amount_max  float32\n",
      "                hist_purchase_amount_sum  float32\n",
      "                 new_purchase_amount_sum  float32\n",
      "               hist_purchase_amount_mean  float32\n",
      "                new_purchase_amount_mean  float32\n",
      "                hist_purchase_amount_std  float32\n",
      "                 new_purchase_amount_std  float32\n",
      "               hist_purchase_amount_skew  float32\n",
      "                new_purchase_amount_skew  float32\n",
      "          hist_purchase_amount_mean_diff  float32\n",
      "           new_purchase_amount_mean_diff  float32\n",
      "             hist_purchase_amount_period  float32\n",
      "              new_purchase_amount_period  float32\n",
      "                  hist_purchase_time_min  float32\n",
      "                   new_purchase_time_min  float32\n",
      "                  hist_purchase_time_max  float32\n",
      "                   new_purchase_time_max  float32\n",
      "                  hist_purchase_time_sum  float32\n",
      "                   new_purchase_time_sum  float32\n",
      "                 hist_purchase_time_mean  float32\n",
      "                  new_purchase_time_mean  float32\n",
      "                  hist_purchase_time_std  float32\n",
      "                   new_purchase_time_std  float32\n",
      "                 hist_purchase_time_skew  float32\n",
      "                  new_purchase_time_skew  float32\n",
      "            hist_purchase_time_mean_diff  float32\n",
      "             new_purchase_time_mean_diff  float32\n",
      "               hist_purchase_time_period  float32\n",
      "                new_purchase_time_period  float32\n",
      "                hist_purchase_time_count  float32\n",
      "                 new_purchase_time_count  float32\n",
      "              hist_purchase_hour_nunique  float32\n",
      "               new_purchase_hour_nunique  float32\n",
      "              hist_purchase_hour_entropy  float32\n",
      "               new_purchase_hour_entropy  float32\n",
      "                 hist_purchase_hour_mode  float32\n",
      "                  new_purchase_hour_mode  float32\n",
      "                 hist_purchase_hour_mean  float32\n",
      "                  new_purchase_hour_mean  float32\n",
      "               hist_purchase_day_nunique  float32\n",
      "                new_purchase_day_nunique  float32\n",
      "               hist_purchase_day_entropy  float32\n",
      "                new_purchase_day_entropy  float32\n",
      "                  hist_purchase_day_mode  float32\n",
      "                   new_purchase_day_mode  float32\n",
      "                  hist_purchase_day_mean  float32\n",
      "                   new_purchase_day_mean  float32\n",
      "              hist_purchase_week_nunique  float32\n",
      "               new_purchase_week_nunique  float32\n",
      "              hist_purchase_week_entropy  float32\n",
      "               new_purchase_week_entropy  float32\n",
      "                 hist_purchase_week_mode  float32\n",
      "                  new_purchase_week_mode  float32\n",
      "                  hist_purchase_week_min  float32\n",
      "                   new_purchase_week_min  float32\n",
      "                  hist_purchase_week_max  float32\n",
      "                   new_purchase_week_max  float32\n",
      "                  hist_purchase_week_sum  float32\n",
      "                   new_purchase_week_sum  float32\n",
      "                 hist_purchase_week_mean  float32\n",
      "                  new_purchase_week_mean  float32\n",
      "                  hist_purchase_week_std  float32\n",
      "                   new_purchase_week_std  float32\n",
      "                 hist_purchase_week_skew  float32\n",
      "                  new_purchase_week_skew  float32\n",
      "            hist_purchase_week_mean_diff  float32\n",
      "             new_purchase_week_mean_diff  float32\n",
      "               hist_purchase_week_period  float32\n",
      "                new_purchase_week_period  float32\n",
      "             hist_purchase_month_nunique  float32\n",
      "              new_purchase_month_nunique  float32\n",
      "             hist_purchase_month_entropy  float32\n",
      "              new_purchase_month_entropy  float32\n",
      "                hist_purchase_month_mode  float32\n",
      "                 new_purchase_month_mode  float32\n",
      "                 hist_purchase_month_min  float32\n",
      "                  new_purchase_month_min  float32\n",
      "                 hist_purchase_month_max  float32\n",
      "                  new_purchase_month_max  float32\n",
      "                 hist_purchase_month_sum  float32\n",
      "                  new_purchase_month_sum  float32\n",
      "                hist_purchase_month_mean  float32\n",
      "                 new_purchase_month_mean  float32\n",
      "                 hist_purchase_month_std  float32\n",
      "                  new_purchase_month_std  float32\n",
      "                hist_purchase_month_skew  float32\n",
      "                 new_purchase_month_skew  float32\n",
      "           hist_purchase_month_mean_diff  float32\n",
      "            new_purchase_month_mean_diff  float32\n",
      "              hist_purchase_month_period  float32\n",
      "               new_purchase_month_period  float32\n",
      "               hist_purchase_weekend_sum  float32\n",
      "                new_purchase_weekend_sum  float32\n",
      "              hist_purchase_weekend_mean  float32\n",
      "               new_purchase_weekend_mean  float32\n",
      "           hist_purchase_weekend_nunique  float32\n",
      "            new_purchase_weekend_nunique  float32\n",
      "                       hist_ref_date_min  float32\n",
      "                        new_ref_date_min  float32\n",
      "                       hist_ref_date_max  float32\n",
      "                        new_ref_date_max  float32\n",
      "                       hist_ref_date_sum  float32\n",
      "                        new_ref_date_sum  float32\n",
      "                      hist_ref_date_mean  float32\n",
      "                       new_ref_date_mean  float32\n",
      "                       hist_ref_date_std  float32\n",
      "                        new_ref_date_std  float32\n",
      "                      hist_ref_date_skew  float32\n",
      "                       new_ref_date_skew  float32\n",
      "                 hist_ref_date_mean_diff  float32\n",
      "                  new_ref_date_mean_diff  float32\n",
      "                    hist_ref_date_period  float32\n",
      "                     new_ref_date_period  float32\n",
      "        hist_time_since_first_active_min  float32\n",
      "         new_time_since_first_active_min  float32\n",
      "        hist_time_since_first_active_max  float32\n",
      "         new_time_since_first_active_max  float32\n",
      "        hist_time_since_first_active_sum  float32\n",
      "         new_time_since_first_active_sum  float32\n",
      "       hist_time_since_first_active_mean  float32\n",
      "        new_time_since_first_active_mean  float32\n",
      "        hist_time_since_first_active_std  float32\n",
      "         new_time_since_first_active_std  float32\n",
      "       hist_time_since_first_active_skew  float32\n",
      "        new_time_since_first_active_skew  float32\n",
      "  hist_time_since_first_active_mean_diff  float32\n",
      "   new_time_since_first_active_mean_diff  float32\n",
      "     hist_time_since_first_active_period  float32\n",
      "      new_time_since_first_active_period  float32\n",
      "                 hist_category_2_nunique  float32\n",
      "                  new_category_2_nunique  float32\n",
      "                 hist_category_2_entropy  float32\n",
      "                  new_category_2_entropy  float32\n",
      "                    hist_category_2_mode  float32\n",
      "                     new_category_2_mode  float32\n",
      "                    hist_category_2_mean  float32\n",
      "                     new_category_2_mean  float32\n",
      "                   hist_state_id_nunique  float32\n",
      "                    new_state_id_nunique  float32\n",
      "                   hist_state_id_entropy  float32\n",
      "                    new_state_id_entropy  float32\n",
      "                      hist_state_id_mode  float32\n",
      "                       new_state_id_mode  float32\n",
      "               hist_subsector_id_nunique  float32\n",
      "                new_subsector_id_nunique  float32\n",
      "               hist_subsector_id_entropy  float32\n",
      "                new_subsector_id_entropy  float32\n",
      "                  hist_subsector_id_mode  float32\n",
      "                   new_subsector_id_mode  float32\n",
      "          hist_merchant_group_id_nunique  float32\n",
      "           new_merchant_group_id_nunique  float32\n",
      "          hist_merchant_group_id_entropy  float32\n",
      "           new_merchant_group_id_entropy  float32\n",
      "             hist_merchant_group_id_mode  float32\n",
      "              new_merchant_group_id_mode  float32\n",
      "                    hist_numerical_1_min  float32\n",
      "                     new_numerical_1_min  float32\n",
      "                    hist_numerical_1_max  float32\n",
      "                     new_numerical_1_max  float32\n",
      "                    hist_numerical_1_sum  float32\n",
      "                     new_numerical_1_sum  float32\n",
      "                   hist_numerical_1_mean  float32\n",
      "                    new_numerical_1_mean  float32\n",
      "                    hist_numerical_1_std  float32\n",
      "                     new_numerical_1_std  float32\n",
      "                   hist_numerical_1_skew  float32\n",
      "                    new_numerical_1_skew  float32\n",
      "              hist_numerical_1_mean_diff  float32\n",
      "               new_numerical_1_mean_diff  float32\n",
      "                 hist_numerical_1_period  float32\n",
      "                  new_numerical_1_period  float32\n",
      "                    hist_numerical_2_min  float32\n",
      "                     new_numerical_2_min  float32\n",
      "                    hist_numerical_2_max  float32\n",
      "                     new_numerical_2_max  float32\n",
      "                    hist_numerical_2_sum  float32\n",
      "                     new_numerical_2_sum  float32\n",
      "                   hist_numerical_2_mean  float32\n",
      "                    new_numerical_2_mean  float32\n",
      "                    hist_numerical_2_std  float32\n",
      "                     new_numerical_2_std  float32\n",
      "                   hist_numerical_2_skew  float32\n",
      "                    new_numerical_2_skew  float32\n",
      "              hist_numerical_2_mean_diff  float32\n",
      "               new_numerical_2_mean_diff  float32\n",
      "                 hist_numerical_2_period  float32\n",
      "                  new_numerical_2_period  float32\n",
      "    hist_most_recent_sales_range_nunique  float32\n",
      "     new_most_recent_sales_range_nunique  float32\n",
      "    hist_most_recent_sales_range_entropy  float32\n",
      "     new_most_recent_sales_range_entropy  float32\n",
      "       hist_most_recent_sales_range_mode  float32\n",
      "        new_most_recent_sales_range_mode  float32\n",
      "       hist_most_recent_sales_range_mean  float32\n",
      "        new_most_recent_sales_range_mean  float32\n",
      "hist_most_recent_purchases_range_nunique  float32\n",
      " new_most_recent_purchases_range_nunique  float32\n",
      "hist_most_recent_purchases_range_entropy  float32\n",
      " new_most_recent_purchases_range_entropy  float32\n",
      "   hist_most_recent_purchases_range_mode  float32\n",
      "    new_most_recent_purchases_range_mode  float32\n",
      "   hist_most_recent_purchases_range_mean  float32\n",
      "    new_most_recent_purchases_range_mean  float32\n",
      "                 hist_avg_sales_lag3_min  float32\n",
      "                  new_avg_sales_lag3_min  float32\n",
      "                 hist_avg_sales_lag3_max  float32\n",
      "                  new_avg_sales_lag3_max  float32\n",
      "                 hist_avg_sales_lag3_sum  float32\n",
      "                  new_avg_sales_lag3_sum  float32\n",
      "                hist_avg_sales_lag3_mean  float32\n",
      "                 new_avg_sales_lag3_mean  float32\n",
      "                 hist_avg_sales_lag3_std  float32\n",
      "                  new_avg_sales_lag3_std  float32\n",
      "                hist_avg_sales_lag3_skew  float32\n",
      "                 new_avg_sales_lag3_skew  float32\n",
      "           hist_avg_sales_lag3_mean_diff  float32\n",
      "            new_avg_sales_lag3_mean_diff  float32\n",
      "              hist_avg_sales_lag3_period  float32\n",
      "               new_avg_sales_lag3_period  float32\n",
      "             hist_avg_purchases_lag3_min  float32\n",
      "              new_avg_purchases_lag3_min  float32\n",
      "             hist_avg_purchases_lag3_max  float32\n",
      "              new_avg_purchases_lag3_max  float32\n",
      "             hist_avg_purchases_lag3_sum  float32\n",
      "              new_avg_purchases_lag3_sum  float32\n",
      "            hist_avg_purchases_lag3_mean  float32\n",
      "             new_avg_purchases_lag3_mean  float32\n",
      "             hist_avg_purchases_lag3_std  float32\n",
      "              new_avg_purchases_lag3_std  float32\n",
      "            hist_avg_purchases_lag3_skew  float32\n",
      "             new_avg_purchases_lag3_skew  float32\n",
      "       hist_avg_purchases_lag3_mean_diff  float32\n",
      "        new_avg_purchases_lag3_mean_diff  float32\n",
      "          hist_avg_purchases_lag3_period  float32\n",
      "           new_avg_purchases_lag3_period  float32\n",
      "             hist_active_months_lag3_min  float32\n",
      "              new_active_months_lag3_min  float32\n",
      "              new_active_months_lag3_max  float32\n",
      "             hist_active_months_lag3_sum  float32\n",
      "              new_active_months_lag3_sum  float32\n",
      "            hist_active_months_lag3_mean  float32\n",
      "             new_active_months_lag3_mean  float32\n",
      "             hist_active_months_lag3_std  float32\n",
      "              new_active_months_lag3_std  float32\n",
      "            hist_active_months_lag3_skew  float32\n",
      "             new_active_months_lag3_skew  float32\n",
      "       hist_active_months_lag3_mean_diff  float32\n",
      "        new_active_months_lag3_mean_diff  float32\n",
      "          hist_active_months_lag3_period  float32\n",
      "           new_active_months_lag3_period  float32\n",
      "                 hist_avg_sales_lag6_min  float32\n",
      "                  new_avg_sales_lag6_min  float32\n",
      "                 hist_avg_sales_lag6_max  float32\n",
      "                  new_avg_sales_lag6_max  float32\n",
      "                 hist_avg_sales_lag6_sum  float32\n",
      "                  new_avg_sales_lag6_sum  float32\n",
      "                hist_avg_sales_lag6_mean  float32\n",
      "                 new_avg_sales_lag6_mean  float32\n",
      "                 hist_avg_sales_lag6_std  float32\n",
      "                  new_avg_sales_lag6_std  float32\n",
      "                hist_avg_sales_lag6_skew  float32\n",
      "                 new_avg_sales_lag6_skew  float32\n",
      "           hist_avg_sales_lag6_mean_diff  float32\n",
      "            new_avg_sales_lag6_mean_diff  float32\n",
      "              hist_avg_sales_lag6_period  float32\n",
      "               new_avg_sales_lag6_period  float32\n",
      "             hist_avg_purchases_lag6_min  float32\n",
      "              new_avg_purchases_lag6_min  float32\n",
      "             hist_avg_purchases_lag6_max  float32\n",
      "              new_avg_purchases_lag6_max  float32\n",
      "             hist_avg_purchases_lag6_sum  float32\n",
      "              new_avg_purchases_lag6_sum  float32\n",
      "            hist_avg_purchases_lag6_mean  float32\n",
      "             new_avg_purchases_lag6_mean  float32\n",
      "             hist_avg_purchases_lag6_std  float32\n",
      "              new_avg_purchases_lag6_std  float32\n",
      "            hist_avg_purchases_lag6_skew  float32\n",
      "             new_avg_purchases_lag6_skew  float32\n",
      "       hist_avg_purchases_lag6_mean_diff  float32\n",
      "        new_avg_purchases_lag6_mean_diff  float32\n",
      "          hist_avg_purchases_lag6_period  float32\n",
      "           new_avg_purchases_lag6_period  float32\n",
      "             hist_active_months_lag6_min  float32\n",
      "              new_active_months_lag6_min  float32\n",
      "              new_active_months_lag6_max  float32\n",
      "             hist_active_months_lag6_sum  float32\n",
      "              new_active_months_lag6_sum  float32\n",
      "            hist_active_months_lag6_mean  float32\n",
      "             new_active_months_lag6_mean  float32\n",
      "             hist_active_months_lag6_std  float32\n",
      "              new_active_months_lag6_std  float32\n",
      "            hist_active_months_lag6_skew  float32\n",
      "             new_active_months_lag6_skew  float32\n",
      "       hist_active_months_lag6_mean_diff  float32\n",
      "        new_active_months_lag6_mean_diff  float32\n",
      "          hist_active_months_lag6_period  float32\n",
      "           new_active_months_lag6_period  float32\n",
      "                hist_avg_sales_lag12_min  float32\n",
      "                 new_avg_sales_lag12_min  float32\n",
      "                hist_avg_sales_lag12_max  float32\n",
      "                 new_avg_sales_lag12_max  float32\n",
      "                hist_avg_sales_lag12_sum  float32\n",
      "                 new_avg_sales_lag12_sum  float32\n",
      "               hist_avg_sales_lag12_mean  float32\n",
      "                new_avg_sales_lag12_mean  float32\n",
      "                hist_avg_sales_lag12_std  float32\n",
      "                 new_avg_sales_lag12_std  float32\n",
      "               hist_avg_sales_lag12_skew  float32\n",
      "                new_avg_sales_lag12_skew  float32\n",
      "          hist_avg_sales_lag12_mean_diff  float32\n",
      "           new_avg_sales_lag12_mean_diff  float32\n",
      "             hist_avg_sales_lag12_period  float32\n",
      "              new_avg_sales_lag12_period  float32\n",
      "            hist_avg_purchases_lag12_min  float32\n",
      "             new_avg_purchases_lag12_min  float32\n",
      "            hist_avg_purchases_lag12_max  float32\n",
      "             new_avg_purchases_lag12_max  float32\n",
      "            hist_avg_purchases_lag12_sum  float32\n",
      "             new_avg_purchases_lag12_sum  float32\n",
      "           hist_avg_purchases_lag12_mean  float32\n",
      "            new_avg_purchases_lag12_mean  float32\n",
      "            hist_avg_purchases_lag12_std  float32\n",
      "             new_avg_purchases_lag12_std  float32\n",
      "           hist_avg_purchases_lag12_skew  float32\n",
      "            new_avg_purchases_lag12_skew  float32\n",
      "      hist_avg_purchases_lag12_mean_diff  float32\n",
      "       new_avg_purchases_lag12_mean_diff  float32\n",
      "         hist_avg_purchases_lag12_period  float32\n",
      "          new_avg_purchases_lag12_period  float32\n",
      "            hist_active_months_lag12_min  float32\n",
      "             new_active_months_lag12_min  float32\n",
      "            hist_active_months_lag12_max  float32\n",
      "             new_active_months_lag12_max  float32\n",
      "            hist_active_months_lag12_sum  float32\n",
      "             new_active_months_lag12_sum  float32\n",
      "           hist_active_months_lag12_mean  float32\n",
      "            new_active_months_lag12_mean  float32\n",
      "            hist_active_months_lag12_std  float32\n",
      "             new_active_months_lag12_std  float32\n",
      "           hist_active_months_lag12_skew  float32\n",
      "            new_active_months_lag12_skew  float32\n",
      "      hist_active_months_lag12_mean_diff  float32\n",
      "       new_active_months_lag12_mean_diff  float32\n",
      "         hist_active_months_lag12_period  float32\n",
      "          new_active_months_lag12_period  float32\n",
      "                     hist_category_4_sum  float32\n",
      "                      new_category_4_sum  float32\n",
      "                    hist_category_4_mean  float32\n",
      "                     new_category_4_mean  float32\n",
      "                 hist_category_4_nunique  float32\n",
      "                  new_category_4_nunique  float32\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(e) for e in cards.columns])\n",
    "fmt_str = \"{:>\"+str(max_len)+\"}  {}\"\n",
    "print(fmt_str.format('COLUMN', 'DATATYPE'))\n",
    "for col in cards:\n",
    "    print(fmt_str.format(col, cards[col].dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "1815ddb656dc69005dced4f3eb45cebc43ae67bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "UInt64Index: 325540 entries, 186538 to 172900\n",
      "Columns: 443 entries, first_active_month to new_category_4_nunique\n",
      "dtypes: float32(440), uint8(3)\n",
      "memory usage: 549.8 MB\n"
     ]
    }
   ],
   "source": [
    "cards.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "7120cfdb32e9f0c70e2701e12f500de833b4f331",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save to feather file\n",
    "cards.reset_index(inplace=True)\n",
    "cards.to_feather('card_features_all.feather')\n",
    "\n",
    "# Read back in with:\n",
    "# cards = pd.read_feather('../input/elo-feature-engineering/card_features.feather')\n",
    "# cards.set_index('card_id', inplace=True)\n",
    "# You can read in the data from another kernel by setting this kernel as a data source:\n",
    "# https://www.kaggle.com/rtatman/importing-data-from-a-kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
