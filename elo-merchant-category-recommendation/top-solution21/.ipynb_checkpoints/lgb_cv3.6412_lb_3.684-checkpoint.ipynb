{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T03:49:16.630035Z",
     "start_time": "2019-02-09T03:49:15.359340Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date, datetime\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(4590)\n",
    "\n",
    "skf= StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "\n",
    "# by feature select \n",
    "FILTER_FEATURE = ['new_hist_month_diff_min_-_hist_month_diff_min', 'new_hist_merchant_category_id_mean_mean_add_hist_merchant_category_id_mean_mean', 'new_hist_month_lag_mean_mean_add_hist_month_lag_mean_mean', 'hist_year_nunique', 'new_hist_month_diff_max_/_hist_month_diff_max', 'new_hist_merchant_id_mean_mean_add_hist_merchant_id_mean_mean', 'new_hist_month_diff_min_/_hist_month_diff_min', 'new_hist_year_mean_mean_add_hist_year_mean_mean', 'new_hist_dayofweek_mean_mean_/_hist_dayofweek_mean_mean', 'new_hist_weekofyear_mean_mean_add_hist_weekofyear_mean_mean', 'new_hist_dayofweek_nunique_/_hist_dayofweek_nunique', 'new_hist_month_diff_max', 'new_hist_year_mean_mean_/_hist_year_mean_mean', 'new_hist_merchant_category_id_mean_mean_/_hist_merchant_category_id_mean_mean', 'new_hist_merchant_id_mean_mean_/_hist_merchant_id_mean_mean', 'new_hist_year_nunique_/_hist_year_nunique', 'new_hist_month_diff_min', 'new_hist_merchant_category_id_mean_mean', 'hist_month_diff_mean_hist_month_diff_min_ctr', 'new_hist_year_nunique_-_hist_year_nunique', 'dayofweek', 'new_hist_month_mean_mean_/_hist_month_mean_mean', 'new_hist_month_diff_var', 'new_hist_installments_min_-_hist_installments_min', 'new_hist_subsector_id_mean_mean', 'new_hist_dayofweek_nunique', 'new_hist_weekend_mean', 'new_hist_category_2_mean_mean_/_hist_category_2_mean_mean', 'new_hist_merchant_category_id_nunique_-_hist_merchant_category_id_nunique', 'new_hist_month_diff_mean_add_hist_month_diff_mean', 'new_hist_authorized_flag_mean', 'new_hist_merchant_category_id_mean_mean_-_hist_merchant_category_id_mean_mean', 'new_hist_year_mean_mean_-_hist_year_mean_mean', 'new_hist_category_2_mean_mean', 'new_hist_card_id_mean_mean_add_hist_card_id_mean_mean', 'new_hist_month_diff_max_-_hist_month_diff_max', 'new_hist_weekend_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T03:51:12.245474Z",
     "start_time": "2019-02-09T03:49:16.633706Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_test = pd.read_csv('../input/test.csv')\n",
    "df_hist_trans = pd.read_csv('../input/historical_transactions.csv',parse_dates=['purchase_date'])\n",
    "df_new_merchant_trans = pd.read_csv('../input/new_merchant_transactions.csv',parse_dates=['purchase_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T03:51:12.276677Z",
     "start_time": "2019-02-09T03:51:12.250465Z"
    }
   },
   "outputs": [],
   "source": [
    "max_date=df_new_merchant_trans['purchase_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T03:51:12.344634Z",
     "start_time": "2019-02-09T03:51:12.295910Z"
    },
    "_uuid": "520b71064293a20e0f2a379dad0acc274374a3c4"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.269Z"
    },
    "_uuid": "44198b73053869f8dfc083204c592fe9193f239c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n",
      "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n",
      "Mem. usage decreased to 1749.11 Mb (43.7% reduction)\n",
      "Mem. usage decreased to 114.20 Mb (45.5% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "df_hist_trans = reduce_mem_usage(df_hist_trans)\n",
    "df_new_merchant_trans = reduce_mem_usage(df_new_merchant_trans)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0            2017-06  C_ID_92a2005557          5          2          1   \n",
       "1            2017-01  C_ID_3d0044924f          4          1          0   \n",
       "2            2016-08  C_ID_d639edf6cd          2          2          0   \n",
       "3            2017-09  C_ID_186d6a6901          4          3          0   \n",
       "4            2017-11  C_ID_cdbd2c0db2          1          3          0   \n",
       "\n",
       "     target  \n",
       "0 -0.820312  \n",
       "1  0.392822  \n",
       "2  0.687988  \n",
       "3  0.142456  \n",
       "4 -0.159790  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.278Z"
    },
    "_uuid": "dda90662d05e22310dd713df106ea07f4b8bccfc"
   },
   "outputs": [],
   "source": [
    "def get_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.279Z"
    },
    "_uuid": "7c91d3b9e9dbaff01962b0facbace75705a9ce18"
   },
   "outputs": [],
   "source": [
    "\n",
    "for df in [df_hist_trans,df_new_merchant_trans]:\n",
    "#     df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['year'] = df['purchase_date'].dt.year\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
    "    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n",
    "    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n",
    "    df['month_diff'] = ((max_date - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "#     df['authorized_flag_purchase_amount'] = df.apply(lambda x: x['purchase_amount'] \\\n",
    "#                             if x['authorized_flag']>0 else 0 ,axis=1)\n",
    "    \n",
    "#     df['unauthorized_flag_purchase_amount'] = df.apply(lambda x: x['purchase_amount'] \\\n",
    "#                             if x['authorized_flag']<1 else 0 ,axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.281Z"
    },
    "_uuid": "ddf1d5bb0ade2b22b0f072c208c1506ea64503ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def  get_agg_fea(count_df,prefix):\n",
    "    aggs = {}\n",
    "    for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "    aggs['installments'] = ['sum','max','min','mean','var']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var']\n",
    "    # aggs['month_diff'] = ['mean']\n",
    "    aggs['month_diff'] = ['max','min','mean','var']\n",
    "    aggs['authorized_flag'] = ['sum', 'mean']\n",
    "    aggs['weekend'] = ['sum', 'mean']\n",
    "    aggs['category_1'] = ['sum', 'mean']\n",
    "    aggs['card_id'] = ['size']\n",
    "\n",
    "    for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id',\\\n",
    "                'category_1','category_2','category_3','month_lag','card_id']:\n",
    "        count_df[col+'_mean'] = count_df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        aggs[col+'_mean'] = ['mean']   \n",
    "\n",
    "    new_columns = get_new_columns(prefix,aggs)\n",
    "    count_df_gp = count_df.groupby('card_id').agg(aggs)\n",
    "    count_df_gp.columns = new_columns\n",
    "    count_df_gp.reset_index(drop=False,inplace=True)\n",
    "    count_df_gp['%s_purchase_date_diff'%prefix] = (count_df_gp['%s_purchase_date_max'%prefix] - count_df_gp['%s_purchase_date_min'%prefix]).dt.days\n",
    "    count_df_gp['%s_purchase_date_average'%prefix] = count_df_gp['%s_purchase_date_diff'%prefix]/count_df_gp['%s_card_id_size'%prefix]\n",
    "    count_df_gp['%s_purchase_date_uptonow'%prefix] = (max_date - count_df_gp['%s_purchase_date_max'%prefix]).dt.days\n",
    "    return count_df_gp\n",
    "\n",
    "\n",
    "\n",
    "hist_count_df=get_agg_fea(df_hist_trans,'hist')\n",
    "df_train = df_train.merge(hist_count_df,on='card_id',how='left')\n",
    "df_test = df_test.merge(hist_count_df,on='card_id',how='left')\n",
    "del hist_count_df\n",
    "gc.collect()\n",
    "new_count_df = get_agg_fea(df_new_merchant_trans,'new_hist')\n",
    "df_train = df_train.merge(new_count_df,on='card_id',how='left')\n",
    "df_test = df_test.merge(new_count_df,on='card_id',how='left')\n",
    "del new_count_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_hist_month_nunique\n",
      "new_hist_hour_nunique\n",
      "new_hist_weekofyear_nunique\n",
      "new_hist_dayofweek_nunique\n",
      "new_hist_year_nunique\n"
     ]
    }
   ],
   "source": [
    "new_hist_feature=[ i for i in df_train  if 'new_hist' in i]\n",
    "\n",
    "for fea in new_hist_feature:\n",
    "    if fea in ['new_hist_purchase_date_max','new_hist_purchase_date_min']:\n",
    "        continue\n",
    "    print(fea)\n",
    "    new_fea=fea\n",
    "    hist_fea=fea.replace(\"new_hist\",'hist')\n",
    "    df_train[new_fea+\"_add_\"+hist_fea]=df_train[new_fea]+df_train[hist_fea]\n",
    "    df_test[new_fea+\"_add_\"+hist_fea]=df_test[new_fea]+df_test[hist_fea]\n",
    "    df_train[new_fea+\"_-_\"+hist_fea]=df_train[new_fea]-df_train[hist_fea]\n",
    "    df_test[new_fea+\"_-_\"+hist_fea]=df_test[new_fea]-df_test[hist_fea]\n",
    "    df_train[new_fea+\"_/_\"+hist_fea]=df_train[new_fea]/df_train[hist_fea]\n",
    "    df_test[new_fea+\"_/_\"+hist_fea]=df_test[new_fea]/df_test[hist_fea]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.286Z"
    },
    "_uuid": "a075cc90ab1322829e4fad3ff39fce307c5db93c"
   },
   "outputs": [],
   "source": [
    "# del df_hist_trans;\n",
    "# gc.collect()\n",
    "\n",
    "# del df_new_merchant_trans;\n",
    "# gc.collect()\n",
    "\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.289Z"
    },
    "_uuid": "6f3182aeac0c3bf7a061a1b9e25e859f25fee9b5"
   },
   "outputs": [],
   "source": [
    "df_train['outliers'] = 0\n",
    "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "df_train['outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.291Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_ctr_fea(tj_data,self_data,items=['feature_1','feature_2','feature_3','new_hist_month_diff_mean',\n",
    "                                          'hist_month_diff_mean', 'hist_month_diff_max','hist_month_diff_min' ]):\n",
    "\n",
    "    items = items\n",
    "    \n",
    "    tj_drop_columns=[i for i in  tj_data.columns  if \"_ctr\" in i]\n",
    "    if len(tj_drop_columns)>0:\n",
    "        tj_data=tj_data.drop(columns=tj_drop_columns)\n",
    "    \n",
    "    self_drop_columns=[i for i in  self_data  if \"_ctr\" in i]\n",
    "    if len(self_drop_columns)>0:\n",
    "        print(self_drop_columns)\n",
    "        self_data.drop(columns=self_drop_columns,inplace=True)\n",
    "        \n",
    "    \n",
    "    for item in items:\n",
    "        if type(item)==list:\n",
    "            pr_name=\"_\".join(item)\n",
    "            merge_columns=item+[pr_name+'_ctr']\n",
    "        else:\n",
    "            pr_name=item\n",
    "            merge_columns=[item,pr_name+'_ctr']\n",
    "        temp = tj_data.groupby(item, as_index = False)['outliers'].agg({pr_name+'_click':'sum',pr_name+'_count':'count'})\n",
    "        temp[pr_name+'_ctr'] =1000* (temp[pr_name+'_click']+0.01)/(temp[pr_name+'_count']+0.01)\n",
    "        \n",
    "        self_data = pd.merge(self_data, temp[merge_columns], on=item, how='left')\n",
    "        \n",
    "    for i in range(len(items)):\n",
    "        for j in range(i+1, len(items)):\n",
    "            item_g = [items[i], items[j]]\n",
    "            merge_columns=item_g+['_'.join(item_g)+'_ctr']\n",
    "            temp = tj_data.groupby(item_g, as_index=False)['outliers'].agg({'_'.join(item_g)+'_click': 'sum','_'.join(item_g)+'count':'count'})\n",
    "            temp['_'.join(item_g)+'_ctr'] =1000* (temp['_'.join(item_g)+'_click']+0.01)/(temp['_'.join(item_g)+'count']+0.01)\n",
    "            self_data = pd.merge(self_data, temp[merge_columns], on=item_g, how='left')\n",
    "            \n",
    "    return self_data\n",
    "            \n",
    "df_test=get_data_ctr_fea(df_train,df_test)  \n",
    "print(\"df_test ctr caluate finished\")\n",
    "\n",
    "from sklearn.model_selection import  StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=4590, shuffle=True)\n",
    "\n",
    "df_train['index'] = [i for i in range(len(df_train))]\n",
    "\n",
    "for k, (df_train_in, df_test_in) in enumerate(skf.split(df_train, df_train.outliers.values)):\n",
    "    df_train_df=df_train.iloc[df_train_in]\n",
    "    val_df=df_train.iloc[df_test_in]\n",
    "    val_df=get_data_ctr_fea(df_train_df,val_df)\n",
    "    if k==0:\n",
    "        new_df_train_df=val_df\n",
    "    else:\n",
    "        new_df_train_df=pd.concat([new_df_train_df,val_df])\n",
    "\n",
    "df_train=new_df_train_df\n",
    "df_train.sort_values(by='index',inplace=True)\n",
    "df_train.drop(columns = ['index'],inplace=True)\n",
    "\n",
    "print(\"df_train ctr caluate finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.294Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.298Z"
    },
    "_uuid": "e9f61512c195c66a75dfe220072c5b2d860b78a3"
   },
   "outputs": [],
   "source": [
    "# Dealing with the one nan in df_test.first_active_month a bit arbitrarily for now\n",
    "df_test.loc[df_test['first_active_month'].isna(),'first_active_month'] = df_test.iloc[11577]['first_active_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.300Z"
    },
    "_uuid": "ce2082fc1fb0e3f8f7d27fc166aa7a8351b65504"
   },
   "outputs": [],
   "source": [
    "for df in [df_train,df_test]:\n",
    "    \n",
    "    \n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n",
    "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
    "    df['month'] = df['first_active_month'].dt.month\n",
    "    df['elapsed_time'] = (max_date- df['first_active_month']).dt.days\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n",
    "                     'new_hist_purchase_date_min']:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n",
    "    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
    "\n",
    "    \n",
    "    df['purchase_amount_diff'] = df['new_hist_purchase_amount_sum']-df['hist_purchase_amount_sum']\n",
    "    df['purchase_amount_rate'] = df['purchase_amount_diff']/df['hist_purchase_amount_sum']\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.302Z"
    },
    "_uuid": "a93c5976d3b395ba8ff0d1002b8075be3e914c54"
   },
   "outputs": [],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.305Z"
    },
    "_uuid": "c4f20f27679889542acfd60d1f1ac381b201ac43"
   },
   "outputs": [],
   "source": [
    "# bed_fea =['new_hist_month_diff_min', 'new_hist_authorized_flag_mean', 'new_hist_year_mean_mean', 'new_hist_year_nunique_/_hist_year_nunique', 'new_hist_year_nunique_-_hist_year_nunique']\n",
    "del_col =  ['card_id', 'first_active_month','target','outliers']+FILTER_FEATURE\n",
    "\n",
    "features = [c for c in df_train.columns if c not in del_col ]\n",
    "\n",
    "target = df_train['target']\n",
    "\n",
    "train_y = target \n",
    "# df_train.drop(columns  = [\"card_id\",\"target\"],inplace=True)\n",
    "# df_test.drop(columns = [\"card_id\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.306Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.308Z"
    }
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.310Z"
    },
    "_uuid": "c9bbc95244978b519d94131907b547c2b6c94191",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"min_child_samples\": 30,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.5,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         'lambda_l2':0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"n_jobs\":10,\n",
    "         \"random_state\": 4590}\n",
    "NFOLD = 5\n",
    "oof_train = np.zeros((len(df_train),1))\n",
    "oof_test = np.zeros((len(df_test),1))\n",
    "oof_test_skf = np.zeros((NFOLD,len(df_test),))\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(skf.split(df_train,df_train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features],target.iloc[trn_idx])#ategorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][features],target.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 500)\n",
    "    oof_train[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    oof_test_skf[fold_,:]= clf.predict(df_test[features], num_iteration=clf.best_iteration)    \n",
    "oof_test = oof_test_skf.mean(axis=0)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train.reshape(-1), target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.312Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_loss=np.sqrt(mean_squared_error(oof_train.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "train_prob=pd.DataFrame(oof_train)\n",
    "train_prob.columns=['class1']\n",
    "train_prob.to_csv(\"oof/train_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "test_prob=pd.DataFrame(oof_test)\n",
    "test_prob.columns=['class1']\n",
    "test_prob.to_csv(\"oof/test_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "np.save(\"train_y\",target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.315Z"
    },
    "_uuid": "40b64481054fa71e692829c7039eccceb31b77fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cv: 3.6497796168629439  lb:3.693\n",
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:500].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"Feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.317Z"
    },
    "_uuid": "355e9c24949b8e5d677fe5a2f117228c3310dab6"
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = oof_test\n",
    "sub_df.to_csv(\"sub/submission_%s.csv\"%mean_loss, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.319Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_importance_df.sort_values(by='importance',inplace=True)\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.321Z"
    }
   },
   "outputs": [],
   "source": [
    "# #feature select\n",
    "\n",
    "# def get_feature_importances(data, shuffle, seed=None):\n",
    "#     # Gather real features\n",
    "#     train_features = [f for f in data if f not in ['card_id', 'first_active_month','target','outliers']]\n",
    "#     # Go over fold and keep track of CV score (train and valid) and feature importances\n",
    "    \n",
    "#     # Shuffle target if required\n",
    "#     y = data['target'].copy()\n",
    "#     if shuffle:\n",
    "#         # Here you could as well use a binomial distribution\n",
    "#         y = data['target'].copy().sample(frac=1.0)\n",
    "    \n",
    "#     # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest\n",
    "#     dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True)\n",
    "#     lgb_params = {\n",
    "#         'objective': 'regression',\n",
    "#         'boosting_type': 'rf',\n",
    "#         'subsample': 0.623,\n",
    "#         'colsample_bytree': 0.7,\n",
    "#         'num_leaves': 127,\n",
    "#         'max_depth': 8,\n",
    "#         'seed': 4590,\n",
    "#         'bagging_freq': 1,\n",
    "#         'n_jobs': 4\n",
    "#     }\n",
    "    \n",
    "#     # Fit the model\n",
    "#     clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200)\n",
    "\n",
    "#     # Get feature importances\n",
    "#     imp_df = pd.DataFrame()\n",
    "#     imp_df[\"feature\"] = list(train_features)\n",
    "#     imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n",
    "#     imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n",
    "    \n",
    "#     return imp_df\n",
    "\n",
    "\n",
    "# # Seed the unexpected randomness of this world\n",
    "# np.random.seed(123)\n",
    "# # Get the actual importance, i.e. without shuffling\n",
    "# actual_imp_df = get_feature_importances(data=df_train, shuffle=False)\n",
    "\n",
    "\n",
    "# null_imp_df = pd.DataFrame()\n",
    "# nb_runs = 80\n",
    "# import time\n",
    "# start = time.time()\n",
    "# dsp = ''\n",
    "# for i in range(nb_runs):\n",
    "#     # Get current run importances\n",
    "#     imp_df = get_feature_importances(data=df_train, shuffle=True)\n",
    "#     imp_df['run'] = i + 1 \n",
    "#     # Concat the latest importances with the old ones\n",
    "#     null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
    "#     # Erase previous message\n",
    "#     for l in range(len(dsp)):\n",
    "#         print('\\b', end='', flush=True)\n",
    "#     # Display current run and time used\n",
    "#     spent = (time.time() - start) / 60\n",
    "#     dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n",
    "#     print(dsp, end='', flush=True)\n",
    "\n",
    "# \"\"\"\n",
    "# Score features\n",
    "# There are several ways to score features :\n",
    "\n",
    "# Compute the number of samples in the actual importances that are away from the null importances recorded distribution.\n",
    "# Compute ratios like Actual / Null Max, Actual / Null Mean, Actual Mean / Null Max\n",
    "# In a first step I will use the log actual feature importance divided by the 75 percentile of null distribution.\n",
    "\n",
    "# \"\"\"\n",
    "    \n",
    "# feature_scores = []\n",
    "# for _f in actual_imp_df['feature'].unique():\n",
    "#     f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "#     f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n",
    "#     gain_score = np.log(1e-10 + f_act_imps_gain / (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n",
    "#     f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n",
    "#     f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n",
    "#     split_score = np.log(1e-10 + f_act_imps_split / (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n",
    "#     feature_scores.append((_f, split_score, gain_score))\n",
    "\n",
    "# scores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "\n",
    "# thr_list=scores_df['gain_score'].tolist()\n",
    "# thr_list=[ i for i in set(thr_list)]\n",
    "# thr_list=sorted(thr_list)\n",
    "\n",
    "\n",
    "# def test_feature(test_fea):\n",
    "#     kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "#     train_x = df_train[test_fea].values\n",
    "#     train_y = df_train['target']\n",
    "#     oof = np.zeros((train_x.shape[0],1))\n",
    "#     df_train.reset_index(drop=True,inplace=True)\n",
    "#     for idx,(idx_trn,idx_val) in enumerate(kf.split(train_x,df_train['outliers'].values)):\n",
    "#         print('第 %d fold'%idx)\n",
    "#         tr_x,tr_y,val_x,val_y=train_x[idx_trn],train_y[idx_trn],train_x[idx_val],train_y[idx_val]\n",
    "\n",
    "#         trn_data = lgb.Dataset(tr_x,tr_y)#, categorical_feature=categorical_feats)\n",
    "#         val_data = lgb.Dataset(val_x,val_y)#, categorical_feature=categorical_feats)\n",
    "\n",
    "#         clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 500)\n",
    "#         oof[idx_val] = clf.predict(val_x, num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "\n",
    "#     loss=np.sqrt(mean_squared_error(oof.reshape(-1), target))\n",
    "#     print('mean loss %f'%loss)\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# test_dict={}\n",
    "# for idx,i in enumerate(thr_list):\n",
    "#     if idx>50:\n",
    "#         break\n",
    "#     print(i)\n",
    "#     bed_fea=set(scores_df[scores_df['gain_score']<=i]['feature'].tolist())\n",
    "#     bed_fea=[i for i in bed_fea]\n",
    "#     print(bed_fea)\n",
    "#     del_col =  ['card_id', 'first_active_month','target','outliers']+bed_fea\n",
    "#     df_test_fea = [c for c in df_train.columns if c not in del_col ]\n",
    "#     test_dict[''.join(bed_fea)]=test_feature(df_test_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.322Z"
    }
   },
   "outputs": [],
   "source": [
    "# min(test_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.325Z"
    }
   },
   "outputs": [],
   "source": [
    "df_hist_ca=['authorized_flag','city_id','category_1','installments','category_3','merchant_category_id','merchant_id','category_2','state_id',\\\n",
    "          'subsector_id','month_lag','purchase_amount' ]\n",
    "\n",
    "hist_text_feature={}\n",
    "gp=df_hist_trans.groupby('card_id')\n",
    "for fea  in df_hist_ca:\n",
    "    hist_text_feature[fea+\"_hist_text\"]=[]\n",
    "hist_text_feature['card_id']=[]\n",
    "for cid,cid_df in  gp:\n",
    "    hist_text_feature['card_id'].append(cid)\n",
    "    for  fea in  df_hist_ca:\n",
    "        if fea=='card_id':\n",
    "            continue\n",
    "        hist_text_feature[fea+\"_hist_text\"].append(\" \".join(cid_df[fea].astype(str).tolist()))\n",
    "df_hist_text=pd.DataFrame(hist_text_feature)\n",
    "\n",
    "\n",
    "new_mer_text_feature={}\n",
    "gp=df_new_merchant_trans.groupby('card_id')\n",
    "for fea  in df_hist_ca:\n",
    "    new_mer_text_feature[fea+\"_new_mer_text\"]=[]\n",
    "new_mer_text_feature['card_id']=[]\n",
    "for cid,cid_df in  gp:\n",
    "    new_mer_text_feature['card_id'].append(cid)\n",
    "    for  fea in  df_hist_ca:\n",
    "        if fea=='card_id':\n",
    "            continue\n",
    "        new_mer_text_feature[fea+\"_new_mer_text\"].append(\" \".join(cid_df[fea].astype(str).tolist()))\n",
    "df_new_mer_text=pd.DataFrame(new_mer_text_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.327Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train=pd.merge(df_train,df_hist_text,how='left',on='card_id')\n",
    "df_test=pd.merge(df_test,df_hist_text,how='left',on='card_id')\n",
    "\n",
    "df_all=pd.concat([df_train,df_test])\n",
    "\n",
    "text_fea=[i for i in df_all.columns if \"text\"in i]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "cv=CountVectorizer(analyzer='word',token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "\n",
    "\n",
    "\n",
    "for idx,i in enumerate(text_fea):\n",
    "    cv.fit(df_all[i])\n",
    "    tr_x=cv.transform(df_train[i])\n",
    "    te_x=cv.transform(df_test[i])\n",
    "    \n",
    "    if idx==0:\n",
    "        cv_Train_x=tr_x\n",
    "        cv_Test_x=te_x\n",
    "    else:\n",
    "        cv_Train_x=sparse.hstack((cv_Train_x,tr_x))\n",
    "        cv_Test_x=sparse.hstack((cv_Test_x,te_x))\n",
    "    \n",
    "print(cv_Train_x.shape,cv_Test_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.328Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "train_x=df_train[features].values\n",
    "test_x=df_test[features].values\n",
    "train_y=df_train['target'].values\n",
    "\n",
    "train_x=sparse.hstack((train_x,cv_Train_x),'csr')\n",
    "test_x=sparse.hstack((test_x,cv_Test_x),'csr')\n",
    "oof_train2 = np.zeros((train_x.shape[0],1))\n",
    "oof_test2 = np.zeros((test_x.shape[0],1))\n",
    "oof_test2_skf = np.zeros((NFOLD,test_x.shape[0],))\n",
    "\n",
    "print(train_x.shape,test_x.shape)\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(skf.split(train_x,df_train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    x_tr,y_tr,x_te,y_te=train_x[trn_idx],train_y[trn_idx],train_x[val_idx],train_y[val_idx]\n",
    "    trn_data = lgb.Dataset(x_tr,y_tr)\n",
    "    val_data = lgb.Dataset(x_te,y_te)\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 500)\n",
    "    oof_train2[val_idx] = clf.predict(x_te, num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "\n",
    "    oof_test2_skf[fold_,:]= clf.predict(test_x, num_iteration=clf.best_iteration)    \n",
    "oof_test2 = oof_test2_skf.mean(axis=0)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train2.reshape(-1), target))\n",
    "print(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.331Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = oof_test2\n",
    "sub_df.to_csv(\"sub/submission_%s.csv\"%mean_loss, index=False)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train2.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "train_prob=pd.DataFrame(oof_train2)\n",
    "train_prob.columns=['class1']\n",
    "train_prob.to_csv(\"oof/train_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "test_prob=pd.DataFrame(oof_test2)\n",
    "test_prob.columns=['class1']\n",
    "test_prob.to_csv(\"oof/test_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "np.save(\"train_y\",target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.333Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train=pd.merge(df_train,df_new_mer_text,how='left',on='card_id')\n",
    "df_test=pd.merge(df_test,df_new_mer_text,how='left',on='card_id')\n",
    "\n",
    "df_all=pd.concat([df_train,df_test])\n",
    "\n",
    "text_fea=[i for i in df_all.columns if \"new_mer_text\"in i]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "cv=CountVectorizer(analyzer='word',token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "\n",
    "\n",
    "\n",
    "for idx,i in enumerate(text_fea):\n",
    "    df_all[i].fillna('-1',inplace=True)\n",
    "    df_train[i].fillna('-1',inplace=True)\n",
    "    df_test[i].fillna('-1',inplace=True)\n",
    "    cv.fit(df_all[i])\n",
    "    tr_x=cv.transform(df_train[i])\n",
    "    te_x=cv.transform(df_test[i])\n",
    "    \n",
    "    if idx==0:\n",
    "        cv_Train_x_2=tr_x\n",
    "        cv_Test_x_2=te_x\n",
    "    else:\n",
    "        cv_Train_x_2=sparse.hstack((cv_Train_x_2,tr_x))\n",
    "        cv_Test_x_2=sparse.hstack((cv_Test_x_2,te_x))\n",
    "    \n",
    "print(cv_Train_x_2.shape,cv_Test_x_2.shape)\n",
    "\n",
    "\n",
    "\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "train_x=df_train[features].values\n",
    "test_x=df_test[features].values\n",
    "train_y=df_train['target'].values\n",
    "\n",
    "train_x=sparse.hstack((train_x,cv_Train_x_2),'csr')\n",
    "test_x=sparse.hstack((test_x,cv_Test_x_2),'csr')\n",
    "oof_train3 = np.zeros((train_x.shape[0],1))\n",
    "oof_test3 = np.zeros((test_x.shape[0],1))\n",
    "oof_test3_skf = np.zeros((NFOLD,test_x.shape[0],))\n",
    "\n",
    "print(train_x.shape,test_x.shape)\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(skf.split(train_x,df_train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    x_tr,y_tr,x_te,y_te=train_x[trn_idx],train_y[trn_idx],train_x[val_idx],train_y[val_idx]\n",
    "    trn_data = lgb.Dataset(x_tr,y_tr)\n",
    "    val_data = lgb.Dataset(x_te,y_te)\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 500)\n",
    "    oof_train3[val_idx] = clf.predict(x_te, num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "\n",
    "    oof_test3_skf[fold_,:]= clf.predict(test_x, num_iteration=clf.best_iteration)    \n",
    "oof_test3 = oof_test3_skf.mean(axis=0)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train3.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "\n",
    "\n",
    "sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = oof_test3\n",
    "sub_df.to_csv(\"sub/submission_%s.csv\"%mean_loss, index=False)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train3.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "train_prob=pd.DataFrame(oof_train3)\n",
    "train_prob.columns=['class1']\n",
    "train_prob.to_csv(\"oof/train_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "test_prob=pd.DataFrame(oof_test3)\n",
    "test_prob.columns=['class1']\n",
    "test_prob.to_csv(\"oof/test_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "np.save(\"train_y\",target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.335Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "train_x=df_train[features].values\n",
    "test_x=df_test[features].values\n",
    "print(train_x.shape,test_x.shape)\n",
    "train_y= df_train['target'].values\n",
    "train_x = sparse.hstack((train_x,cv_Train_x),'csr')\n",
    "test_x = sparse.hstack((test_x,cv_Test_x),'csr')\n",
    "print(train_x.shape,test_x.shape)\n",
    "\n",
    "train_x=sparse.hstack((train_x,cv_Train_x_2),'csr')\n",
    "test_x=sparse.hstack((test_x,cv_Test_x_2),'csr')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "oof_train4 = np.zeros((train_x.shape[0],1))\n",
    "oof_test4 = np.zeros((test_x.shape[0],1))\n",
    "oof_test4_skf = np.zeros((NFOLD,test_x.shape[0],))\n",
    "\n",
    "print(train_x.shape,test_x.shape)\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(skf.split(train_x,df_train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    x_tr,y_tr,x_te,y_te=train_x[trn_idx],train_y[trn_idx],train_x[val_idx],train_y[val_idx]\n",
    "    trn_data = lgb.Dataset(x_tr,y_tr)\n",
    "    val_data = lgb.Dataset(x_te,y_te)\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 500)\n",
    "    oof_train4[val_idx] = clf.predict(x_te, num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "\n",
    "    oof_test4_skf[fold_,:]= clf.predict(test_x, num_iteration=clf.best_iteration)    \n",
    "oof_test4 = oof_test4_skf.mean(axis=0)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train4.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "\n",
    "\n",
    "sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = oof_test4\n",
    "sub_df.to_csv(\"sub/submission_%s.csv\"%mean_loss, index=False)\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train4.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "train_prob=pd.DataFrame(oof_train4)\n",
    "train_prob.columns=['class1']\n",
    "train_prob.to_csv(\"oof/train_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "test_prob=pd.DataFrame(oof_test4)\n",
    "test_prob.columns=['class1']\n",
    "test_prob.to_csv(\"oof/test_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "np.save(\"train_y\",target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:49:15.337Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "train_stack = np.concatenate([oof_train,oof_train2,oof_train3,oof_train4],axis=1)\n",
    "test_stack = np.concatenate([oof_test.reshape(-1,1),oof_test2.reshape(-1,1),\\\n",
    "                             oof_test3.reshape(-1,1),oof_test4.reshape(-1,1)],axis=1)\n",
    "print(train_stack.shape,test_stack.shape)\n",
    "\n",
    "oof_stack = np.zeros(train_stack.shape[0])\n",
    "predictions_3 = np.zeros(test_stack.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(skf.split(train_stack,df_train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n",
    "    \n",
    "    clf_3 = BayesianRidge()\n",
    "    clf_3.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack[val_idx] = clf_3.predict(val_data)\n",
    "    predictions_3 += clf_3.predict(test_stack) / 5\n",
    "    \n",
    "stack_loss=np.sqrt(mean_squared_error(target.values, oof_stack))\n",
    "print(stack_loss)\n",
    "sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = predictions_3\n",
    "sub_df.to_csv(\"sub/submission_%s.csv\"%stack_loss, index=False)\n",
    "\n",
    "train_prob=pd.DataFrame(oof_stack)\n",
    "train_prob.columns=['class1']\n",
    "train_prob.to_csv(\"oof/train_prob_%s.csv\"%stack_loss,index=False)\n",
    "\n",
    "test_prob=pd.DataFrame(predictions_3)\n",
    "test_prob.columns=['class1']\n",
    "test_prob.to_csv(\"oof/test_prob_%s.csv\"%stack_loss,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
