{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:48:14.121215Z",
     "start_time": "2019-02-21T00:48:13.264473Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import gc\n",
    "pd.set_option('display.width',None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_info_columns', 500)\n",
    "np.random.seed(4950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:56:26.635480Z",
     "start_time": "2019-02-21T00:56:03.168792Z"
    },
    "_uuid": "5ecf41453acfe45b3e0a46cb7f2e86b62c5e6835",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/pre_train_clip.csv\", index_col=0)\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"./data/pre_test_clip.csv\", index_col=0)\n",
    "\n",
    "cats = [ 'first_active_month',\n",
    "        'feature_1', 'feature_2', 'feature_3', \n",
    "        'hist_first_year', \n",
    "        'hist_first_quarter', \n",
    "        'hist_first_month',\n",
    "        'hist_re_year', \n",
    "        'hist_re_quarter', \n",
    "        'hist_re_month',\n",
    "        'hist_now_year', \n",
    "        'hist_now_quarter', \n",
    "        'hist_now_month',\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:56:27.434437Z",
     "start_time": "2019-02-21T00:56:26.637141Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([train, test])    \n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for feat in cats:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "#     data[feat][data[feat]>=0] = lbe.fit_transform(data[feat][data[feat]>=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:56:27.907279Z",
     "start_time": "2019-02-21T00:56:27.436116Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[cats] = data[0:201917][cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:56:28.183325Z",
     "start_time": "2019-02-21T00:56:27.908953Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = data[201917:][cats].reset_index(drop=True)\n",
    "test[cats] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:56:28.241661Z",
     "start_time": "2019-02-21T00:56:28.185017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67 62 57 70 72 58 61 69 59 52 65 63 56 55 71 47 53 68 50 60 48 18 51 54\n",
      " 27 66 73 64 45 42 38 33 39 31 32 28 43 30 49 44 40 46 26 29 35 36 34 25\n",
      " 37 19 41 23 21 20  6  9 11  7 10  5 13 24 22 17 12 14  0 15  4  3 16 74\n",
      "  8 75  1]\n",
      "[65 62 69 73 49 68 71 63 52 64 53 61 57 72 70 59 67 54 16 37 56 66 58 60\n",
      " 40 51 55 22 39 43 47 35 34 46 50 38 19 42 45 48 29 28 31 44  8 41 32 27\n",
      " 36 20 30 33 11 25 74  3 21  7 24 23 26 15 17 12 10  5  6  4 13  9 18 14\n",
      "  0  1  2]\n",
      "[67 62 57 70 72 58 61 69 59 52 65 63 56 55 71 47 53 68 50 60 48 18 51 54\n",
      " 27 66 73 64 45 42 38 33 39 31 32 28 43 30 49 44 40 46 26 29 35 36 34 25\n",
      " 37 19 41 23 21 20  6  9 11  7 10  5 13 24 22 17 12 14  0 15  4  3 16 74\n",
      "  8 75  1  2]\n",
      "[4 3 1 0 2]\n",
      "[2 1 4 0 3]\n",
      "[4 3 1 0 2]\n",
      "[1 0 2]\n",
      "[2 0 1]\n",
      "[1 0 2]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[6 5 4 2 3 1 0 7]\n",
      "[6 4 5 2 3 1 7 0]\n",
      "[6 5 4 2 3 1 0 7]\n",
      "[1 0 2 3]\n",
      "[1 0 2 3]\n",
      "[1 0 2 3]\n",
      "[ 5  0  7  8 10 11  9  2  3  1  6  4]\n",
      "[ 3  0  7 11  6  9  1  2 10  8  5  4]\n",
      "[ 5  0  7  8 10 11  9  2  3  1  6  4]\n",
      "[1 0]\n",
      "[0 1]\n",
      "[1 0]\n",
      "[0 2 3 1]\n",
      "[3 0 2 1]\n",
      "[0 2 3 1]\n",
      "[ 1  0  6 11  9  4  2 10  8  7  3  5]\n",
      "[11  1  8  6  3  4  0  9  7 10  2  5]\n",
      "[ 1  0  6 11  9  4  2 10  8  7  3  5]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 3 0 2]\n",
      "[1 0 3 2]\n",
      "[1 3 0 2]\n",
      "[ 5  4 10  3  1  8  6  2  0 11  7  9]\n",
      "[ 3  5  0 10  7  8  4  1 11  2  6  9]\n",
      "[ 5  4 10  3  1  8  6  2  0 11  7  9]\n"
     ]
    }
   ],
   "source": [
    "for c in cats:\n",
    "    print(train[c].unique())\n",
    "    print(test[c].unique())\n",
    "    print(data[c].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:56:28.247116Z",
     "start_time": "2019-02-21T00:56:28.243218Z"
    },
    "_uuid": "6cbe0d1da1d7d7759d9f6c4d02b8e6d3d1342d54",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fold_target = train['target']\n",
    "fold_target = (fold_target<-20).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:56:28.253026Z",
     "start_time": "2019-02-21T00:56:28.248806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302 ['first_active_month', 'feature_1', 'feature_2', 'feature_3', 'authorized_flag_mean', 'hist_transactions_count', 'hist_is_month_start_mean', 'hist_weekend_mean', 'hist_category_1_mean', 'hist_category_2_nunique', 'hist_category_3_nunique', 'hist_state_id_nunique', 'hist_city_id_nunique', 'hist_subsector_id_nunique', 'hist_merchant_category_id_nunique', 'hist_merchant_id_nunique', 'hist_quarter_nunique', 'hist_month_nunique', 'hist_weekofyear_nunique', 'hist_dayofweek_nunique', 'hist_day_nunique', 'hist_hour_nunique', 'hist_a2p_mean', 'hist_a2p_median', 'hist_a2p_max', 'hist_a2p_min', 'hist_a2p_std', 'hist_p2r_mean', 'hist_p2r_median', 'hist_p2r_max', 'hist_p2r_min', 'hist_p2r_std', 'hist_p2now_mean', 'hist_p2now_median', 'hist_p2now_max', 'hist_p2now_min', 'hist_p2now_std', 'hist_month_lag_mean', 'hist_month_lag_median', 'hist_month_lag_max', 'hist_month_lag_min', 'hist_month_lag_std', 'hist_purchase_amount_sum', 'hist_purchase_amount_mean', 'hist_purchase_amount_median', 'hist_purchase_amount_max', 'hist_purchase_amount_min', 'hist_purchase_amount_std', 'hist_installments_sum', 'hist_installments_mean', 'hist_installments_median', 'hist_installments_max', 'hist_installments_min', 'hist_installments_std', 'hist_p_vs_m_mean', 'hist_p_vs_m_median', 'hist_p_vs_m_max', 'hist_p_vs_m_min', 'hist_p_vs_m_std', 'hist_p_vs_i_mean', 'hist_p_vs_i_median', 'hist_p_vs_i_max', 'hist_p_vs_i_min', 'hist_p_vs_i_std', 'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_category_2_p_mean_mean', 'hist_category_2_p_mean_median', 'hist_category_2_p_mean_max', 'hist_category_2_p_mean_min', 'hist_category_2_p_mean_std', 'hist_category_3_p_mean_mean', 'hist_category_3_p_mean_median', 'hist_category_3_p_mean_max', 'hist_category_3_p_mean_min', 'hist_category_3_p_mean_std', 'hist_state_id_p_mean_mean', 'hist_state_id_p_mean_median', 'hist_state_id_p_mean_max', 'hist_state_id_p_mean_min', 'hist_state_id_p_mean_std', 'hist_city_id_p_mean_mean', 'hist_city_id_p_mean_median', 'hist_city_id_p_mean_max', 'hist_city_id_p_mean_min', 'hist_city_id_p_mean_std', 'hist_subsector_id_p_mean_mean', 'hist_subsector_id_p_mean_median', 'hist_subsector_id_p_mean_max', 'hist_subsector_id_p_mean_min', 'hist_subsector_id_p_mean_std', 'hist_merchant_category_id_p_mean_mean', 'hist_merchant_category_id_p_mean_median', 'hist_merchant_category_id_p_mean_max', 'hist_merchant_category_id_p_mean_min', 'hist_merchant_category_id_p_mean_std', 'hist_merchant_id_p_mean_mean', 'hist_merchant_id_p_mean_median', 'hist_merchant_id_p_mean_max', 'hist_merchant_id_p_mean_min', 'hist_merchant_id_p_mean_std', 'hist_quarter_p_mean_mean', 'hist_quarter_p_mean_median', 'hist_quarter_p_mean_max', 'hist_quarter_p_mean_min', 'hist_quarter_p_mean_std', 'hist_month_p_mean_mean', 'hist_month_p_mean_median', 'hist_month_p_mean_max', 'hist_month_p_mean_min', 'hist_month_p_mean_std', 'hist_weekofyear_p_mean_mean', 'hist_weekofyear_p_mean_median', 'hist_weekofyear_p_mean_max', 'hist_weekofyear_p_mean_min', 'hist_weekofyear_p_mean_std', 'hist_dayofweek_p_mean_mean', 'hist_dayofweek_p_mean_median', 'hist_dayofweek_p_mean_max', 'hist_dayofweek_p_mean_min', 'hist_dayofweek_p_mean_std', 'hist_day_p_mean_mean', 'hist_day_p_mean_median', 'hist_day_p_mean_max', 'hist_day_p_mean_min', 'hist_day_p_mean_std', 'hist_hour_p_mean_mean', 'hist_hour_p_mean_median', 'hist_hour_p_mean_max', 'hist_hour_p_mean_min', 'hist_hour_p_mean_std', 'hist_first_year', 'hist_first_quarter', 'hist_first_month', 'hist_re_year', 'hist_re_quarter', 'hist_re_month', 'hist_now_year', 'hist_now_quarter', 'hist_now_month', 'hist_a2r', 'hist_r2now', 'hist_a2now', 'hist_p2p', 'hist_sleep', 'hist_p2p_vs_count', 'hist_sleep_vs_count', 'hist_count_vs_p2p', 'hist_sleep_vs_p2p', 'hist_p_vs_p2p', 'hist_i_vs_p2p', 'new_transactions_count', 'new_is_month_start_mean', 'new_weekend_mean', 'new_category_1_mean', 'new_category_2_nunique', 'new_category_3_nunique', 'new_state_id_nunique', 'new_city_id_nunique', 'new_subsector_id_nunique', 'new_merchant_category_id_nunique', 'new_merchant_id_nunique', 'new_quarter_nunique', 'new_month_nunique', 'new_weekofyear_nunique', 'new_dayofweek_nunique', 'new_day_nunique', 'new_hour_nunique', 'new_a2p_mean', 'new_a2p_median', 'new_a2p_max', 'new_a2p_min', 'new_a2p_std', 'new_p2r_mean', 'new_p2r_median', 'new_p2r_max', 'new_p2r_min', 'new_p2r_std', 'new_p2now_mean', 'new_p2now_median', 'new_p2now_max', 'new_p2now_min', 'new_p2now_std', 'new_month_lag_mean', 'new_month_lag_median', 'new_month_lag_max', 'new_month_lag_min', 'new_month_lag_std', 'new_purchase_amount_sum', 'new_purchase_amount_mean', 'new_purchase_amount_median', 'new_purchase_amount_max', 'new_purchase_amount_min', 'new_purchase_amount_std', 'new_installments_sum', 'new_installments_mean', 'new_installments_median', 'new_installments_max', 'new_installments_min', 'new_installments_std', 'new_p_vs_m_mean', 'new_p_vs_m_median', 'new_p_vs_m_max', 'new_p_vs_m_min', 'new_p_vs_m_std', 'new_p_vs_i_mean', 'new_p_vs_i_median', 'new_p_vs_i_max', 'new_p_vs_i_min', 'new_p_vs_i_std', 'new_purchase_date_max', 'new_purchase_date_min', 'new_category_2_p_mean_mean', 'new_category_2_p_mean_median', 'new_category_2_p_mean_max', 'new_category_2_p_mean_min', 'new_category_2_p_mean_std', 'new_category_3_p_mean_mean', 'new_category_3_p_mean_median', 'new_category_3_p_mean_max', 'new_category_3_p_mean_min', 'new_category_3_p_mean_std', 'new_state_id_p_mean_mean', 'new_state_id_p_mean_median', 'new_state_id_p_mean_max', 'new_state_id_p_mean_min', 'new_state_id_p_mean_std', 'new_city_id_p_mean_mean', 'new_city_id_p_mean_median', 'new_city_id_p_mean_max', 'new_city_id_p_mean_min', 'new_city_id_p_mean_std', 'new_subsector_id_p_mean_mean', 'new_subsector_id_p_mean_median', 'new_subsector_id_p_mean_max', 'new_subsector_id_p_mean_min', 'new_subsector_id_p_mean_std', 'new_merchant_category_id_p_mean_mean', 'new_merchant_category_id_p_mean_median', 'new_merchant_category_id_p_mean_max', 'new_merchant_category_id_p_mean_min', 'new_merchant_category_id_p_mean_std', 'new_merchant_id_p_mean_mean', 'new_merchant_id_p_mean_median', 'new_merchant_id_p_mean_max', 'new_merchant_id_p_mean_min', 'new_merchant_id_p_mean_std', 'new_quarter_p_mean_mean', 'new_quarter_p_mean_median', 'new_quarter_p_mean_max', 'new_quarter_p_mean_min', 'new_quarter_p_mean_std', 'new_month_p_mean_mean', 'new_month_p_mean_median', 'new_month_p_mean_max', 'new_month_p_mean_min', 'new_month_p_mean_std', 'new_weekofyear_p_mean_mean', 'new_weekofyear_p_mean_median', 'new_weekofyear_p_mean_max', 'new_weekofyear_p_mean_min', 'new_weekofyear_p_mean_std', 'new_dayofweek_p_mean_mean', 'new_dayofweek_p_mean_median', 'new_dayofweek_p_mean_max', 'new_dayofweek_p_mean_min', 'new_dayofweek_p_mean_std', 'new_day_p_mean_mean', 'new_day_p_mean_median', 'new_day_p_mean_max', 'new_day_p_mean_min', 'new_day_p_mean_std', 'new_hour_p_mean_mean', 'new_hour_p_mean_median', 'new_hour_p_mean_max', 'new_hour_p_mean_min', 'new_hour_p_mean_std', 'new_p2p', 'new_sleep', 'new_p2p_vs_count', 'new_sleep_vs_count', 'new_count_vs_p2p', 'new_sleep_vs_p2p', 'new_p_vs_p2p', 'new_i_vs_p2p', 'outliers', 'c_p2p_diff', 'c_sleep_diff', 'c_p_diff', 'c_i_diff', 'p2p_count_diff', 'p2p_sleep_diff', 'p2p_p_diff', 'p2p_i_diff', 'c_p2p_diff_vs', 'c_sleep_diff_vs', 'c_p_diff_vs', 'c_i_diff_vs', 'p2p_count_diff_vs', 'p2p_sleep_diff_vs', 'p2p_p_diff_vs', 'p2p_i_diff_vs']\n"
     ]
    }
   ],
   "source": [
    "features = [i for i in train.columns if i not in ['card_id','target']]\n",
    "print(len(features),features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:56:28.276975Z",
     "start_time": "2019-02-21T00:56:28.254641Z"
    }
   },
   "outputs": [],
   "source": [
    "obj_fea = []\n",
    "\n",
    "for i in features:\n",
    "    if str(train[i].dtype)=='object':\n",
    "        print(i)\n",
    "        obj_fea.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T01:02:19.935544Z",
     "start_time": "2019-02-21T01:02:19.909426Z"
    },
    "_uuid": "f9012fa16fb65f06901b51f62df2bea7ce47e3cc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "         'objective':'regression',\n",
    "         \"metric\": 'rmse',\n",
    "         \"boosting\": \"gbdt\",\n",
    "         'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_freq\": 1,\n",
    "   \n",
    "         \"random_state\": 1024,\n",
    "         \"verbosity\": -1,\n",
    "}\n",
    "\n",
    "n_fold = 5\n",
    "\n",
    "# param ={\n",
    "#         'task': 'train',\n",
    "#         'boosting': 'goss',\n",
    "#         'objective': 'regression',\n",
    "#         'metric': 'rmse',\n",
    "#         'learning_rate': 0.01,\n",
    "#         'subsample': 0.9855232997390695,\n",
    "#         'max_depth': 7,\n",
    "#         'top_rate': 0.9064148448434349,\n",
    "#         'num_leaves': 63,\n",
    "#         'min_child_weight': 41.9612869171337,\n",
    "#         'other_rate': 0.0721768246018207,\n",
    "#         'reg_alpha': 9.677537745007898,\n",
    "#         'colsample_bytree': 0.5665320670155495,\n",
    "#         'min_split_gain': 9.820197773625843,\n",
    "#         'reg_lambda': 8.2532317400459,\n",
    "#         'min_data_in_leaf': 21,\n",
    "#         'verbose': -1,\n",
    "#         'seed':int(2**n_fold),\n",
    "#         'bagging_seed':int(2**n_fold),\n",
    "#         'drop_seed':int(2**n_fold)\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-21T01:02:20.785Z"
    },
    "_uuid": "e77474bcceb7e39ca2cc5cd2b67db2fb6b0a29ab",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 200 rounds.\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import RepeatedKFold\n",
    "# folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=2333)\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=4590)\n",
    "oof_train = np.zeros((len(train),1))\n",
    "oof_test = np.zeros(len(test))\n",
    "oof_test_skf = np.zeros((5,len(test),1))\n",
    "start = time.time()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "target = train['target']\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, fold_target)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "#     trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n",
    "#     val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data],\n",
    "                    verbose_eval=100, early_stopping_rounds = 200)\n",
    "    oof_train[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    oof_test_skf[fold_,:]= clf.predict(test[features], num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "#     oof_test += clf.predict(test[features], num_iteration=clf.best_iteration) / 10\n",
    "    oof_test += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-21T01:02:21.411Z"
    },
    "_uuid": "c4a7fe4ffcc7e31e398891b4839b1de0525ec6d9"
   },
   "outputs": [],
   "source": [
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_train, target)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-21T01:02:23.128Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "sub_df = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = oof_test\n",
    "sub_df.to_csv(\"sub/submission_%s.csv\"%mean_loss, index=False)\n",
    "\n",
    "train_prob=pd.DataFrame(oof_train)\n",
    "train_prob.columns=['class1']\n",
    "train_prob.to_csv(\"oof/train_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "test_prob=pd.DataFrame(oof_test)\n",
    "test_prob.columns=['class1']\n",
    "test_prob.to_csv(\"oof/test_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-21T01:02:43.303Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature select\n",
    "\n",
    "def get_feature_importances(data, shuffle, seed=None):\n",
    "    # Gather real features\n",
    "    train_features = features\n",
    "    # Go over fold and keep track of CV score (train and valid) and feature importances\n",
    "    \n",
    "    # Shuffle target if required\n",
    "    y = data['target'].copy()\n",
    "    if shuffle:\n",
    "        # Here you could as well use a binomial distribution\n",
    "        y = data['target'].copy().sample(frac=1.0)\n",
    "    \n",
    "    # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest\n",
    "    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True)\n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'boosting_type': 'rf',\n",
    "        'subsample': 0.623,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'num_leaves': 127,\n",
    "        'max_depth': 8,\n",
    "        'seed': 4590,\n",
    "        'bagging_freq': 1,\n",
    "        'n_jobs': 4\n",
    "    }\n",
    "    \n",
    "    # Fit the model\n",
    "    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200)\n",
    "\n",
    "    # Get feature importances\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df[\"feature\"] = list(train_features)\n",
    "    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n",
    "    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n",
    "    \n",
    "    return imp_df\n",
    "\n",
    "\n",
    "# Seed the unexpected randomness of this world\n",
    "np.random.seed(123)\n",
    "# Get the actual importance, i.e. without shuffling\n",
    "actual_imp_df = get_feature_importances(data=train, shuffle=False)\n",
    "\n",
    "\n",
    "null_imp_df = pd.DataFrame()\n",
    "nb_runs = 80\n",
    "import time\n",
    "start = time.time()\n",
    "dsp = ''\n",
    "for i in range(nb_runs):\n",
    "    # Get current run importances\n",
    "    imp_df = get_feature_importances(data=train, shuffle=True)\n",
    "    imp_df['run'] = i + 1 \n",
    "    # Concat the latest importances with the old ones\n",
    "    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
    "    # Erase previous message\n",
    "    for l in range(len(dsp)):\n",
    "        print('\\b', end='', flush=True)\n",
    "    # Display current run and time used\n",
    "    spent = (time.time() - start) / 60\n",
    "    dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n",
    "    print(dsp, end='', flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-21T01:02:55.149Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_scores = []\n",
    "for _f in actual_imp_df['feature'].unique():\n",
    "    f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n",
    "    gain_score = np.log(1e-10 + f_act_imps_gain / (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n",
    "    f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n",
    "    split_score = np.log(1e-10 + f_act_imps_split / (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n",
    "    feature_scores.append((_f, split_score, gain_score))\n",
    "\n",
    "scores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-21T01:03:05.030Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thr_list=scores_df['gain_score'].tolist()\n",
    "thr_list=[ i for i in set(thr_list)]\n",
    "thr_list=sorted(thr_list)\n",
    "thr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-21T01:03:15.267Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_feature(test_fea):\n",
    "    param['n_jobs']=3\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "   \n",
    "    train_y = target\n",
    "    oof_train = np.zeros((len(train),1))\n",
    "    oof_test = np.zeros((len(test),1))\n",
    "    \n",
    "    train.reset_index(drop=True,inplace=True)\n",
    "    for idx,(idx_trn,idx_val) in enumerate(kf.split(train,train['outliers'])):\n",
    "        print('第 %d fold'%idx)\n",
    "        tr_x,tr_y,val_x,val_y=train.iloc[idx_trn][test_fea],train['target'].iloc[idx_trn],train.iloc[idx_val][test_fea],train['target'][idx_val]\n",
    "\n",
    "        trn_data = lgb.Dataset(tr_x,tr_y)#, categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(val_x,val_y)#, categorical_feature=categorical_feats)\n",
    "\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 500)\n",
    "        oof_train[idx_val] = clf.predict(val_x, num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "        oof_test += clf.predict(test[test_fea], num_iteration=clf.best_iteration).reshape(-1,1)/5\n",
    "        \n",
    "        \n",
    "\n",
    "    loss=np.sqrt(mean_squared_error(oof_train.reshape(-1), target))\n",
    "    \n",
    "    print('mean loss %f'%loss)\n",
    "    if loss<3.660:\n",
    "        train_prob=pd.DataFrame(oof_train)\n",
    "        train_prob.columns=['class1']\n",
    "        train_prob.to_csv(\"oof/train_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "        test_prob=pd.DataFrame(oof_test)\n",
    "        test_prob.columns=['class1']\n",
    "        test_prob.to_csv(\"oof/test_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "test_dict={}\n",
    "for idx,i in enumerate(thr_list):\n",
    "    if idx>50:\n",
    "        break\n",
    "    print(i)\n",
    "    bed_fea=set(scores_df[scores_df['gain_score']<=i]['feature'].tolist())\n",
    "    bed_fea=[i for i in bed_fea]\n",
    "    print(bed_fea)\n",
    "    del_col =  ['card_id', 'first_active_month','target','outliers']+bed_fea\n",
    "    df_test_fea = [c for c in features if c not in del_col ]\n",
    "    test_dict[''.join(bed_fea)]=test_feature(df_test_fea)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
