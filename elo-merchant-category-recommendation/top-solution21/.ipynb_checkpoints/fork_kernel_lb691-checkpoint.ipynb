{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T15:56:31.297448Z",
     "start_time": "2019-02-20T15:56:31.283537Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data_Dictionary.xlsx', 'historical_transactions.csv', 'df_train_merchant_text', 'merchant_w2v_300.txt', 'merchants.csv', 'test.csv', 'new_merchant_transactions.csv', 'train.csv', 'sample_submission.csv', 'df_test_merchant_text']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T15:56:31.305534Z",
     "start_time": "2019-02-20T15:56:31.299126Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T15:56:40.950265Z",
     "start_time": "2019-02-20T15:56:31.307488Z"
    },
    "_uuid": "b3143b13f6cc576b2ab4b0ec46a6a648676d9a14",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format='[%(levelname)s] %(asctime)s %(filename)s: %(lineno)d: %(message)s',\n",
    "    datefmt='%Y-%m-%d:%H:%M:%S',\n",
    "    level=logging.DEBUG)\n",
    "\n",
    "DATE_TODAY = dt(2019, 1, 26)\n",
    "\n",
    "FEATS_EXCLUDED = [\n",
    "    'first_active_month', 'target', 'card_id', 'outliers',\n",
    "    'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n",
    "    'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size',\n",
    "    'OOF_PRED', 'month_0']\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    logger.info(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "        by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances.png')\n",
    "\n",
    "\n",
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "# rmse\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = df.columns.tolist()\n",
    "\n",
    "    categorical_columns = list(filter(lambda c: c in ['object'], df.dtypes))\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "\n",
    "    new_columns = list(filter(lambda c: c not in original_columns, df.columns))\n",
    "    return df, new_columns\n",
    "\n",
    "\n",
    "def process_main_df(df):\n",
    "    \n",
    "    # datetime features\n",
    "    df['quarter'] = df['first_active_month'].dt.quarter\n",
    "    df['elapsed_time'] = (DATE_TODAY - df['first_active_month']).dt.days\n",
    "\n",
    "    feature_cols = ['feature_1', 'feature_2', 'feature_3']\n",
    "    for f in feature_cols:    \n",
    "        df['days_' + f] = df['elapsed_time'] * df[f]\n",
    "        df['days_' + f + '_ratio'] = df[f] / df['elapsed_time']\n",
    "\n",
    "    # one hot encoding\n",
    "    df, cols = one_hot_encoder(df, nan_as_category=False)\n",
    "\n",
    "    df_feats = df.reindex(columns=feature_cols)\n",
    "    df['features_sum'] = df_feats.sum(axis=1)\n",
    "    df['features_mean'] = df_feats.mean(axis=1)\n",
    "    df['features_max'] = df_feats.max(axis=1)\n",
    "    df['features_min'] = df_feats.min(axis=1)\n",
    "    df['features_var'] = df_feats.std(axis=1)\n",
    "    df['features_prod'] = df_feats.product(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# preprocessing train & test\n",
    "def train_test(num_rows=None):\n",
    "\n",
    "    def read_csv(filename):\n",
    "        df = pd.read_csv(\n",
    "            filename, index_col=['card_id'], parse_dates=['first_active_month'], nrows=num_rows)\n",
    "        return df\n",
    "    \n",
    "    # load csv\n",
    "    train_df = read_csv('../input/train.csv')\n",
    "    test_df = read_csv('../input/test.csv') \n",
    "    logger.info(\"samples: train {}, test: {}\".format(train_df.shape, test_df.shape))\n",
    "\n",
    "    # outlier\n",
    "    train_df['outliers'] = 0\n",
    "    train_df.loc[train_df['target'] < -30., 'outliers'] = 1\n",
    "\n",
    "    train_df = reduce_mem_usage(process_main_df(train_df))\n",
    "    test_df = reduce_mem_usage(process_main_df(test_df))\n",
    "\n",
    "    feature_cols = ['feature_1', 'feature_2', 'feature_3']\n",
    "    for f in feature_cols:\n",
    "        order_label = train_df.groupby([f])['outliers'].mean()\n",
    "        train_df[f] = train_df[f].map(order_label)\n",
    "        test_df[f] = test_df[f].map(order_label)    \n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def process_date(df):\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['day'] = df['purchase_date'].dt.day\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['weekday'] = df['purchase_date'].dt.weekday\n",
    "    df['weekend'] = (df['purchase_date'].dt.weekday >= 5).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def dist_holiday(df, col_name, date_holiday, date_ref, period=100):\n",
    "    df[col_name] = np.maximum(np.minimum((pd.to_datetime(date_holiday) - df[date_ref]).dt.days, period), 0)\n",
    "\n",
    "\n",
    "def historical_transactions(num_rows=None):\n",
    "    \"\"\"\n",
    "    preprocessing historical transactions\n",
    "    \"\"\"\n",
    "    na_dict = {\n",
    "        'category_2': 1.,\n",
    "        'category_3': 'A',\n",
    "        'merchant_id': 'M_ID_00a6ca8a8a',\n",
    "    }\n",
    "\n",
    "    holidays = [\n",
    "        ('Christmas_Day_2017', '2017-12-25'),  # Christmas: December 25 2017\n",
    "        ('Mothers_Day_2017', '2017-06-04'),  # Mothers Day: May 14 2017\n",
    "        ('fathers_day_2017', '2017-08-13'),  # fathers day: August 13 2017\n",
    "        ('Children_day_2017', '2017-10-12'),  # Childrens day: October 12 2017\n",
    "        ('Valentine_Day_2017', '2017-06-12'),  # Valentine's Day : 12th June, 2017\n",
    "        ('Black_Friday_2017', '2017-11-24'),  # Black Friday: 24th November 2017\n",
    "        ('Mothers_Day_2018', '2018-05-13'),\n",
    "    ]\n",
    "\n",
    "    # agg\n",
    "    aggs = dict()\n",
    "    col_unique = ['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    aggs.update({col: ['nunique'] for col in col_unique})\n",
    "\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "    aggs.update({col: ['nunique', 'mean', 'min', 'max'] for col in col_seas})\n",
    "\n",
    "    aggs_specific = {\n",
    "        'purchase_amount': ['sum', 'max', 'min', 'mean', 'var', 'skew'],\n",
    "        'installments': ['sum', 'max', 'mean', 'var', 'skew'],\n",
    "        'purchase_date': ['max', 'min'],\n",
    "        'month_lag': ['max', 'min', 'mean', 'var', 'skew'],\n",
    "        'month_diff': ['max', 'min', 'mean', 'var', 'skew'],\n",
    "        'authorized_flag': ['mean'],\n",
    "        'weekend': ['mean'], # overwrite\n",
    "        'weekday': ['mean'], # overwrite\n",
    "        'day': ['nunique', 'mean', 'min'], # overwrite\n",
    "        'category_1': ['mean'],\n",
    "        'category_2': ['mean'],\n",
    "        'category_3': ['mean'],\n",
    "        'card_id': ['size', 'count'],\n",
    "        'price': ['sum', 'mean', 'max', 'min', 'var'],\n",
    "        'Christmas_Day_2017': ['mean', 'sum'],\n",
    "        'Mothers_Day_2017': ['mean', 'sum'],\n",
    "        'fathers_day_2017': ['mean', 'sum'],\n",
    "        'Children_day_2017': ['mean', 'sum'],\n",
    "        'Valentine_Day_2017': ['mean', 'sum'],\n",
    "        'Black_Friday_2017': ['mean', 'sum'],\n",
    "        'Mothers_Day_2018': ['mean', 'sum'],\n",
    "        'duration': ['mean', 'min', 'max', 'var', 'skew'],\n",
    "        'amount_month_ratio': ['mean', 'min', 'max', 'var', 'skew'],\n",
    "    }\n",
    "    aggs.update(aggs_specific)\n",
    "\n",
    "    # starting to process\n",
    "    # load csv\n",
    "    df = pd.read_csv('../input/historical_transactions.csv', nrows=num_rows)\n",
    "    logger.info('read historical_transactions {}'.format(df.shape))\n",
    "    \n",
    "    # fillna\n",
    "    df.fillna(na_dict, inplace=True)\n",
    "    df['installments'].replace({\n",
    "        -1: np.nan, 999: np.nan}, inplace=True)\n",
    "\n",
    "    # trim\n",
    "    df['purchase_amount'] = df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(np.int16)\n",
    "    df['category_1'] = df['category_1'].map({'Y': 1, 'N': 0}).astype(np.int16)\n",
    "    df['category_3'] = df['category_3'].map({'A': 0, 'B': 1, 'C':2}).astype(np.int16)\n",
    "\n",
    "    # additional features\n",
    "    df['price'] = df['purchase_amount'] / df['installments']\n",
    "\n",
    "    # datetime features\n",
    "    df = process_date(df)\n",
    "\n",
    "    # holidays\n",
    "    for d_name, d_day in holidays:\n",
    "        dist_holiday(df, d_name, d_day, 'purchase_date')\n",
    "\n",
    "    df['month_diff'] = (DATE_TODAY - df['purchase_date']).dt.days // 30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "\n",
    "    # additional features\n",
    "    df['duration'] = df['purchase_amount'] * df['month_diff']\n",
    "    df['amount_month_ratio'] = df['purchase_amount'] / df['month_diff']\n",
    "\n",
    "    # reduce memory usage\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    for col in ['category_2', 'category_3']:\n",
    "        df[col + '_mean'] = df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        df[col + '_min'] = df.groupby([col])['purchase_amount'].transform('min')\n",
    "        df[col + '_max'] = df.groupby([col])['purchase_amount'].transform('max')\n",
    "        df[col + '_sum'] = df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col + '_mean'] = ['mean']\n",
    "    \n",
    "    df = df.reset_index().groupby('card_id').agg(aggs)\n",
    "\n",
    "    # change column name\n",
    "    df.columns = pd.Index([e[0] + \"_\" + e[1] for e in df.columns.tolist()])\n",
    "    df.columns = ['hist_' + c for c in df.columns]\n",
    "\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_purchase_amount_sum'] / df['hist_month_diff_mean']\n",
    "\n",
    "    df['hist_purchase_date_diff'] = (df['hist_purchase_date_max'] - df['hist_purchase_date_min']).dt.days\n",
    "    df['hist_purchase_date_average'] = df['hist_purchase_date_diff'] / df['hist_card_id_size']\n",
    "    df['hist_purchase_date_uptonow'] = (DATE_TODAY - df['hist_purchase_date_max']).dt.days\n",
    "    df['hist_purchase_date_uptomin'] = (DATE_TODAY - df['hist_purchase_date_min']).dt.days\n",
    "\n",
    "    # reduce memory usage\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def new_merchant_transactions(num_rows=None):\n",
    "    \"\"\"\n",
    "    preprocessing new_merchant_transactions\n",
    "    \"\"\"\n",
    "    na_dict = {\n",
    "        'category_2': 1.,\n",
    "        'category_3': 'A',\n",
    "        'merchant_id': 'M_ID_00a6ca8a8a',\n",
    "    }\n",
    "\n",
    "    holidays = [\n",
    "        ('Christmas_Day_2017', '2017-12-25'),  # Christmas: December 25 2017\n",
    "        # ('Mothers_Day_2017', '2017-06-04'),  # Mothers Day: May 14 2017\n",
    "        # ('fathers_day_2017', '2017-08-13'),  # fathers day: August 13 2017\n",
    "        ('Children_day_2017', '2017-10-12'),  # Childrens day: October 12 2017\n",
    "        # ('Valentine_Day_2017', '2017-06-12'),  # Valentine's Day : 12th June, 2017\n",
    "        ('Black_Friday_2017', '2017-11-24'),  # Black Friday: 24th November 2017\n",
    "        ('Mothers_Day_2018', '2018-05-13'),\n",
    "    ]\n",
    "    \n",
    "    aggs = dict()\n",
    "    col_unique = ['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    aggs.update({col: ['nunique'] for col in col_unique})\n",
    "\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "    aggs.update({col: ['nunique', 'mean', 'min', 'max'] for col in col_seas})\n",
    "\n",
    "    aggs_specific = {\n",
    "        'purchase_amount': ['sum', 'max', 'min', 'mean', 'var', 'skew'],\n",
    "        'installments': ['sum', 'max', 'mean', 'var', 'skew'],\n",
    "        'purchase_date': ['max', 'min'],\n",
    "        'month_lag': ['max', 'min', 'mean', 'var', 'skew'],\n",
    "        'month_diff': ['mean', 'var', 'skew'],\n",
    "        'weekend': ['mean'],\n",
    "        'month': ['mean', 'min', 'max'],\n",
    "        'weekday': ['mean', 'min', 'max'],\n",
    "        'category_1': ['mean'],\n",
    "        'category_2': ['mean'],\n",
    "        'category_3': ['mean'],\n",
    "        'card_id': ['size', 'count'],\n",
    "        'price': ['mean', 'max', 'min', 'var'],\n",
    "        'Christmas_Day_2017': ['mean', 'sum'],\n",
    "        'Children_day_2017': ['mean', 'sum'],\n",
    "        'Black_Friday_2017': ['mean', 'sum'],\n",
    "        'Mothers_Day_2018': ['mean', 'sum'],\n",
    "        'duration': ['mean', 'min', 'max', 'var', 'skew'],\n",
    "        'amount_month_ratio': ['mean', 'min', 'max', 'var', 'skew'],\n",
    "    }\n",
    "    aggs.update(aggs_specific)\n",
    "\n",
    "    # load csv\n",
    "    df = pd.read_csv('../input/new_merchant_transactions.csv', nrows=num_rows)\n",
    "    logger.info('read new_merchant_transactions {}'.format(df.shape))\n",
    "    \n",
    "    # fillna\n",
    "    df.fillna(na_dict, inplace=True)\n",
    "    df['installments'].replace({\n",
    "        -1: np.nan, 999: np.nan}, inplace=True)\n",
    "\n",
    "    # trim\n",
    "    df['purchase_amount'] = df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int).astype(np.int16)\n",
    "    df['category_1'] = df['category_1'].map({'Y': 1, 'N': 0}).astype(int).astype(np.int16)\n",
    "    df['category_3'] = df['category_3'].map({'A': 0, 'B': 1, 'C': 2}).astype(int).astype(np.int16)\n",
    "\n",
    "    # additional features\n",
    "    df['price'] = df['purchase_amount'] / df['installments']\n",
    "\n",
    "    # datetime features\n",
    "    df = process_date(df)\n",
    "    for d_name, d_day in holidays:\n",
    "        dist_holiday(df, d_name, d_day, 'purchase_date')\n",
    "\n",
    "    df['month_diff'] = (DATE_TODAY - df['purchase_date']).dt.days // 30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "\n",
    "    # additional features\n",
    "    df['duration'] = df['purchase_amount'] * df['month_diff']\n",
    "    df['amount_month_ratio'] = df['purchase_amount'] / df['month_diff']\n",
    "\n",
    "    # reduce memory usage\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    for col in ['category_2', 'category_3']:\n",
    "        df[col+'_mean'] = df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        df[col+'_min'] = df.groupby([col])['purchase_amount'].transform('min')\n",
    "        df[col+'_max'] = df.groupby([col])['purchase_amount'].transform('max')\n",
    "        df[col+'_sum'] = df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col + '_mean'] = ['mean']\n",
    "\n",
    "    df = df.reset_index().groupby('card_id').agg(aggs)\n",
    "\n",
    "    # change column name\n",
    "    df.columns = pd.Index([e[0] + \"_\" + e[1] for e in df.columns.tolist()])\n",
    "    df.columns = ['new_' + c for c in df.columns]\n",
    "\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_purchase_amount_sum'] / df['new_month_diff_mean']\n",
    "    \n",
    "    df['new_purchase_date_diff'] = (df['new_purchase_date_max'] - df['new_purchase_date_min']).dt.days\n",
    "    df['new_purchase_date_average'] = df['new_purchase_date_diff'] / df['new_card_id_size']\n",
    "    df['new_purchase_date_uptonow'] = (DATE_TODAY - df['new_purchase_date_max']).dt.days\n",
    "    df['new_purchase_date_uptomin'] = (DATE_TODAY - df['new_purchase_date_min']).dt.days\n",
    "\n",
    "    # reduce memory usage\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    return df\n",
    "        \n",
    "\n",
    "# additional features\n",
    "def additional_features(df):\n",
    "    \n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "\n",
    "    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_last_buy'] = (df['new_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "\n",
    "    date_features = [\n",
    "        'hist_purchase_date_max', 'hist_purchase_date_min', 'new_purchase_date_max', 'new_purchase_date_min']\n",
    "    for f in date_features:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "\n",
    "    #\n",
    "    df['card_id_total'] = df['new_card_id_size'] + df['hist_card_id_size']\n",
    "    df['card_id_cnt_total'] = df['new_card_id_count'] + df['hist_card_id_count']\n",
    "    df['card_id_cnt_ratio'] = df['new_card_id_count'] / df['hist_card_id_count']\n",
    "    \n",
    "    df['purchase_amount_total'] = df['new_purchase_amount_sum'] + df['hist_purchase_amount_sum']\n",
    "    df['purchase_amount_mean'] = df['new_purchase_amount_mean'] + df['hist_purchase_amount_mean']\n",
    "    df['purchase_amount_max'] = df['new_purchase_amount_max'] + df['hist_purchase_amount_max']\n",
    "    df['purchase_amount_min'] = df['new_purchase_amount_min'] + df['hist_purchase_amount_min']\n",
    "    df['purchase_amount_ratio'] = df['new_purchase_amount_sum'] / df['hist_purchase_amount_sum']\n",
    "\n",
    "    df['installments_total'] = df['new_installments_sum'] + df['hist_installments_sum']\n",
    "    df['installments_mean'] = df['new_installments_mean'] + df['hist_installments_mean']\n",
    "    df['installments_max'] = df['new_installments_max'] + df['hist_installments_max']\n",
    "    df['installments_ratio'] = df['new_installments_sum'] / df['hist_installments_sum']\n",
    "\n",
    "    df['price_total'] = df['purchase_amount_total'] / df['installments_total']\n",
    "    df['price_mean'] = df['purchase_amount_mean'] / df['installments_mean']\n",
    "    df['price_max'] = df['purchase_amount_max'] / df['installments_max']\n",
    "\n",
    "    #\n",
    "    df['month_diff_mean'] = df['new_month_diff_mean'] + df['hist_month_diff_mean']\n",
    "    df['month_diff_ratio'] = df['new_month_diff_mean'] / df['hist_month_diff_mean']\n",
    "    \n",
    "    df['month_lag_mean'] = df['new_month_lag_mean'] + df['hist_month_lag_mean']\n",
    "    df['month_lag_max'] = df['new_month_lag_max'] + df['hist_month_lag_max']\n",
    "    df['month_lag_min'] = df['new_month_lag_min'] + df['hist_month_lag_min']\n",
    "    df['category_1_mean'] = df['new_category_1_mean'] + df['hist_category_1_mean']\n",
    "        \n",
    "    df['duration_mean'] = df['new_duration_mean'] + df['hist_duration_mean']\n",
    "    df['duration_min'] = df['new_duration_min'] + df['hist_duration_min']\n",
    "    df['duration_max'] = df['new_duration_max'] + df['hist_duration_max']\n",
    "    \n",
    "    df['amount_month_ratio_mean'] = df['new_amount_month_ratio_mean'] + df['hist_amount_month_ratio_mean']\n",
    "    df['amount_month_ratio_min'] = df['new_amount_month_ratio_min'] + df['hist_amount_month_ratio_min']\n",
    "    df['amount_month_ratio_max'] = df['new_amount_month_ratio_max'] + df['hist_amount_month_ratio_max']\n",
    "    \n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']\n",
    "    df['CLV_sq'] = df['new_CLV'] * df['hist_CLV']\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def modeling_xgb_cross_validation(params, X, y, nr_folds=5, verbose=0):\n",
    "    clfs = list()\n",
    "    oof_preds = np.zeros(X.shape[0])\n",
    "    # Split data with kfold\n",
    "    #kfolds = TimeSeriesSplit(n_splits=nr_folds)\n",
    "    kfolds = StratifiedKFold(n_splits=nr_folds, shuffle=True, random_state=42)\n",
    "    split_index = X[['feature_1', 'feature_2', 'feature_3']].apply(lambda x: np.log1p(x)).product(axis=1)\n",
    "    kfolds = KFold(n_splits=nr_folds, shuffle=True, random_state=42)\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kfolds.split(X, split_index)):\n",
    "        if verbose:\n",
    "            print('no {} of {} folds'.format(n_fold, nr_folds))\n",
    "\n",
    "        X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n",
    "        X_valid, y_valid = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            # eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            verbose=verbose, eval_metric='rmse',\n",
    "            early_stopping_rounds=500\n",
    "        )\n",
    "\n",
    "        clfs.append(model)\n",
    "        oof_preds[val_idx] = model.predict(X_valid, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "        del X_train, y_train, X_valid, y_valid\n",
    "        gc.collect()\n",
    "\n",
    "    score = mean_squared_error(y, oof_preds) ** .5\n",
    "    return clfs, score\n",
    "\n",
    "\n",
    "def modeling_lgbm_cross_validation(params, X, y, nr_folds=5, verbose=0):\n",
    "    clfs = list()\n",
    "    oof_preds = np.zeros(X.shape[0])\n",
    "    # Split data with kfold\n",
    "    # kfolds = TimeSeriesSplit(n_splits=nr_folds)\n",
    "    kfolds = StratifiedKFold(n_splits=nr_folds, shuffle=True, random_state=42)\n",
    "    split_index = X[['feature_1', 'feature_2', 'feature_3']].apply(lambda x: np.log1p(x)).product(axis=1)\n",
    "    kfolds = KFold(n_splits=nr_folds, shuffle=True, random_state=42)\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kfolds.split(X, y)):\n",
    "        if verbose:\n",
    "            print('no {} of {} folds'.format(n_fold, nr_folds))\n",
    "\n",
    "        X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n",
    "        X_valid, y_valid = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            # eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            verbose=verbose, eval_metric='rmse',\n",
    "            early_stopping_rounds=500\n",
    "        )\n",
    "\n",
    "        clfs.append(model)\n",
    "        oof_preds[val_idx] = model.predict(X_valid, num_iteration=model.best_iteration_)\n",
    "\n",
    "        del X_train, y_train, X_valid, y_valid\n",
    "        gc.collect()\n",
    "\n",
    "    score = mean_squared_error(y, oof_preds) ** .5\n",
    "    return clfs, score\n",
    "\n",
    "\n",
    "def predict_cross_validation(test, clfs, ntree_limit=None):\n",
    "    sub_preds = np.zeros(test.shape[0])\n",
    "    for i, model in enumerate(clfs, 1):\n",
    "\n",
    "        num_tree = 10000\n",
    "        if not ntree_limit:\n",
    "            ntree_limit = num_tree\n",
    "\n",
    "        if isinstance(model, lgb.sklearn.LGBMRegressor):\n",
    "            if model.best_iteration_:\n",
    "                num_tree = min(ntree_limit, model.best_iteration_)\n",
    "\n",
    "            test_preds = model.predict(test, raw_score=True, num_iteration=num_tree)\n",
    "\n",
    "        if isinstance(model, xgb.sklearn.XGBRegressor):\n",
    "            num_tree = min(ntree_limit, model.best_ntree_limit)\n",
    "            test_preds = model.predict(test, ntree_limit=num_tree)\n",
    "\n",
    "        sub_preds += test_preds\n",
    "\n",
    "    sub_preds = sub_preds / len(clfs)\n",
    "    ret = pd.Series(sub_preds, index=test.index)\n",
    "    ret.index.name = test.index.name\n",
    "    return ret\n",
    "\n",
    "\n",
    "def write_to_parquet(filename, df, debug=False):\n",
    "    print('write to {}: {}'.format(filename, df.shape))\n",
    "\n",
    "    # safety check\n",
    "    cols_type = df.dtypes.to_dict()\n",
    "    for col, col_type in cols_type.items():\n",
    "        if str(col_type).startswith('float16'):\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    df.to_parquet(filename, engine='auto', compression='snappy')\n",
    "    if debug:\n",
    "        df = pd.read_parquet(filename)\n",
    "        print('debug reload save file: {}\\n{}'.format(df.shape, df.head().T))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:26:49.750018Z",
     "start_time": "2019-02-20T15:56:40.952219Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-02-20:10:57:46 <ipython-input-21-524054be9f26>: 237: read historical_transactions (29112361, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1832.41 MB\n",
      "Decreased by 71.3%\n",
      "Memory usage after optimization is: 66.13 MB\n",
      "Decreased by 56.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-02-20:11:15:39 <ipython-input-21-524054be9f26>: 350: read new_merchant_transactions (1963031, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 117.94 MB\n",
      "Decreased by 69.4%\n",
      "Memory usage after optimization is: 47.29 MB\n",
      "Decreased by 59.3%\n"
     ]
    }
   ],
   "source": [
    "hist_df = historical_transactions()\n",
    "        \n",
    "new_merchant_df = new_merchant_transactions()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:26:52.780531Z",
     "start_time": "2019-02-20T16:26:49.751829Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-02-20:11:26:51 <ipython-input-21-524054be9f26>: 146: samples: train (201917, 5), test: (123623, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 8.67 MB\n",
      "Decreased by 73.2%\n",
      "Memory usage after optimization is: 5.07 MB\n",
      "Decreased by 71.7%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([new_merchant_df, hist_df], axis=1)\n",
    "del new_merchant_df, hist_df\n",
    "gc.collect()\n",
    "\n",
    "train_df, test_df = train_test()\n",
    "train_df = train_df.join(df, how='left', on='card_id')\n",
    "test_df = test_df.join(df, how='left', on='card_id')\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:26:52.785386Z",
     "start_time": "2019-02-20T16:26:52.782222Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "excluded_features = FEATS_EXCLUDED\n",
    "train_features = [c for c in train_df.columns if c not in excluded_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:43:26.750201Z",
     "start_time": "2019-02-20T16:43:26.744040Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_df\n",
    "test = test_df\n",
    "features = train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:43:27.634013Z",
     "start_time": "2019-02-20T16:43:27.617196Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "         'objective':'regression',\n",
    "         \"metric\": 'rmse',\n",
    "         \"boosting\": \"gbdt\",\n",
    "         'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_freq\": 1,\n",
    "   \n",
    "         \"random_state\": 1024,\n",
    "         \"verbosity\": -1,\n",
    "}\n",
    "\n",
    "n_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:45:45.509626Z",
     "start_time": "2019-02-20T16:43:42.286809Z"
    },
    "_uuid": "e9cdd9e5263d9bc2ad04d2230591cf6889280c95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 3.65618\tvalid_1's rmse: 3.72141\n",
      "[200]\ttraining's rmse: 3.57396\tvalid_1's rmse: 3.68667\n",
      "[300]\ttraining's rmse: 3.5238\tvalid_1's rmse: 3.67435\n",
      "[400]\ttraining's rmse: 3.48621\tvalid_1's rmse: 3.66708\n",
      "[500]\ttraining's rmse: 3.4548\tvalid_1's rmse: 3.6619\n",
      "[600]\ttraining's rmse: 3.42651\tvalid_1's rmse: 3.6586\n",
      "[700]\ttraining's rmse: 3.40102\tvalid_1's rmse: 3.65642\n",
      "[800]\ttraining's rmse: 3.37829\tvalid_1's rmse: 3.65487\n",
      "[900]\ttraining's rmse: 3.35641\tvalid_1's rmse: 3.65365\n",
      "[1000]\ttraining's rmse: 3.33633\tvalid_1's rmse: 3.65329\n",
      "[1100]\ttraining's rmse: 3.31706\tvalid_1's rmse: 3.6527\n",
      "[1200]\ttraining's rmse: 3.29869\tvalid_1's rmse: 3.65278\n",
      "[1300]\ttraining's rmse: 3.28131\tvalid_1's rmse: 3.65315\n",
      "Early stopping, best iteration is:\n",
      "[1167]\ttraining's rmse: 3.30442\tvalid_1's rmse: 3.65243\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 3.65694\tvalid_1's rmse: 3.71642\n",
      "[200]\ttraining's rmse: 3.57431\tvalid_1's rmse: 3.68271\n",
      "[300]\ttraining's rmse: 3.52501\tvalid_1's rmse: 3.67114\n",
      "[400]\ttraining's rmse: 3.48594\tvalid_1's rmse: 3.66542\n",
      "[500]\ttraining's rmse: 3.45335\tvalid_1's rmse: 3.66094\n",
      "[600]\ttraining's rmse: 3.4255\tvalid_1's rmse: 3.65956\n",
      "[700]\ttraining's rmse: 3.39967\tvalid_1's rmse: 3.65805\n",
      "[800]\ttraining's rmse: 3.37663\tvalid_1's rmse: 3.65719\n",
      "[900]\ttraining's rmse: 3.35492\tvalid_1's rmse: 3.6564\n",
      "[1000]\ttraining's rmse: 3.33401\tvalid_1's rmse: 3.65547\n",
      "[1100]\ttraining's rmse: 3.31513\tvalid_1's rmse: 3.65523\n",
      "[1200]\ttraining's rmse: 3.29606\tvalid_1's rmse: 3.65545\n",
      "Early stopping, best iteration is:\n",
      "[1089]\ttraining's rmse: 3.31713\tvalid_1's rmse: 3.6551\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 3.65574\tvalid_1's rmse: 3.72408\n",
      "[200]\ttraining's rmse: 3.57128\tvalid_1's rmse: 3.68881\n",
      "[300]\ttraining's rmse: 3.52111\tvalid_1's rmse: 3.67626\n",
      "[400]\ttraining's rmse: 3.48299\tvalid_1's rmse: 3.66967\n",
      "[500]\ttraining's rmse: 3.44992\tvalid_1's rmse: 3.66561\n",
      "[600]\ttraining's rmse: 3.42262\tvalid_1's rmse: 3.66333\n",
      "[700]\ttraining's rmse: 3.39804\tvalid_1's rmse: 3.66128\n",
      "[800]\ttraining's rmse: 3.37495\tvalid_1's rmse: 3.66021\n",
      "[900]\ttraining's rmse: 3.35356\tvalid_1's rmse: 3.65907\n",
      "[1000]\ttraining's rmse: 3.33339\tvalid_1's rmse: 3.65835\n",
      "[1100]\ttraining's rmse: 3.31468\tvalid_1's rmse: 3.65773\n",
      "[1200]\ttraining's rmse: 3.29608\tvalid_1's rmse: 3.65731\n",
      "[1300]\ttraining's rmse: 3.27782\tvalid_1's rmse: 3.65688\n",
      "[1400]\ttraining's rmse: 3.26065\tvalid_1's rmse: 3.65671\n",
      "[1500]\ttraining's rmse: 3.24468\tvalid_1's rmse: 3.65755\n",
      "[1600]\ttraining's rmse: 3.22825\tvalid_1's rmse: 3.65752\n",
      "Early stopping, best iteration is:\n",
      "[1407]\ttraining's rmse: 3.2596\tvalid_1's rmse: 3.65665\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 3.65354\tvalid_1's rmse: 3.71969\n",
      "[200]\ttraining's rmse: 3.5683\tvalid_1's rmse: 3.68972\n",
      "[300]\ttraining's rmse: 3.51771\tvalid_1's rmse: 3.67744\n",
      "[400]\ttraining's rmse: 3.47778\tvalid_1's rmse: 3.67018\n",
      "[500]\ttraining's rmse: 3.44548\tvalid_1's rmse: 3.66497\n",
      "[600]\ttraining's rmse: 3.41776\tvalid_1's rmse: 3.6626\n",
      "[700]\ttraining's rmse: 3.39276\tvalid_1's rmse: 3.66096\n",
      "[800]\ttraining's rmse: 3.3701\tvalid_1's rmse: 3.66004\n",
      "[900]\ttraining's rmse: 3.34783\tvalid_1's rmse: 3.65906\n",
      "[1000]\ttraining's rmse: 3.32757\tvalid_1's rmse: 3.6581\n",
      "[1100]\ttraining's rmse: 3.30846\tvalid_1's rmse: 3.65763\n",
      "[1200]\ttraining's rmse: 3.29001\tvalid_1's rmse: 3.65752\n",
      "[1300]\ttraining's rmse: 3.27163\tvalid_1's rmse: 3.65728\n",
      "[1400]\ttraining's rmse: 3.25352\tvalid_1's rmse: 3.65748\n",
      "Early stopping, best iteration is:\n",
      "[1239]\ttraining's rmse: 3.2828\tvalid_1's rmse: 3.65709\n",
      "fold n°5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 3.6586\tvalid_1's rmse: 3.70404\n",
      "[200]\ttraining's rmse: 3.57181\tvalid_1's rmse: 3.6765\n",
      "[300]\ttraining's rmse: 3.51913\tvalid_1's rmse: 3.66589\n",
      "[400]\ttraining's rmse: 3.47959\tvalid_1's rmse: 3.65951\n",
      "[500]\ttraining's rmse: 3.44628\tvalid_1's rmse: 3.65633\n",
      "[600]\ttraining's rmse: 3.41796\tvalid_1's rmse: 3.65414\n",
      "[700]\ttraining's rmse: 3.39246\tvalid_1's rmse: 3.65351\n",
      "[800]\ttraining's rmse: 3.36841\tvalid_1's rmse: 3.65265\n",
      "[900]\ttraining's rmse: 3.34583\tvalid_1's rmse: 3.65249\n",
      "[1000]\ttraining's rmse: 3.3251\tvalid_1's rmse: 3.65243\n",
      "[1100]\ttraining's rmse: 3.30488\tvalid_1's rmse: 3.6528\n",
      "Early stopping, best iteration is:\n",
      "[972]\ttraining's rmse: 3.33077\tvalid_1's rmse: 3.65224\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import RepeatedKFold\n",
    "# folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=2333)\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=4590)\n",
    "oof_train = np.zeros((len(train),1))\n",
    "oof_test = np.zeros(len(test))\n",
    "oof_test_skf = np.zeros((5,len(test),1))\n",
    "start = time.time()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "target = train['target']\n",
    "train['outliers'] = train.target.apply(lambda x: 1 if x<-30 else 0)\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['outliers'])):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "#     trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n",
    "#     val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data],\n",
    "                    verbose_eval=100, early_stopping_rounds = 200)\n",
    "    oof_train[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    oof_test_skf[fold_,:]= clf.predict(test[features], num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "#     oof_test += clf.predict(test[features], num_iteration=clf.best_iteration) / 10\n",
    "    oof_test += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:49:27.595328Z",
     "start_time": "2019-02-20T16:49:26.586133Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6547036072293664\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean_loss=np.sqrt(mean_squared_error(oof_train.reshape(-1), target))\n",
    "print(mean_loss)\n",
    "# sub_df = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\n",
    "# sub_df[\"target\"] = oof_test\n",
    "# sub_df.to_csv(\"sub/submission_%s.csv\"%mean_loss, index=False)\n",
    "\n",
    "train_prob=pd.DataFrame(oof_train)\n",
    "train_prob.columns=['class1']\n",
    "train_prob.to_csv(\"oof/train_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "test_prob=pd.DataFrame(oof_test)\n",
    "test_prob.columns=['class1']\n",
    "test_prob.to_csv(\"oof/test_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-20T16:54:14.860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with   42 of   80 (Spent   6.7 min)"
     ]
    }
   ],
   "source": [
    "#feature select\n",
    "\n",
    "def get_feature_importances(data, shuffle, seed=None):\n",
    "    # Gather real features\n",
    "    train_features = features\n",
    "    # Go over fold and keep track of CV score (train and valid) and feature importances\n",
    "    \n",
    "    # Shuffle target if required\n",
    "    y = data['target'].copy()\n",
    "    if shuffle:\n",
    "        # Here you could as well use a binomial distribution\n",
    "        y = data['target'].copy().sample(frac=1.0)\n",
    "    \n",
    "    # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest\n",
    "    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True)\n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'boosting_type': 'rf',\n",
    "        'subsample': 0.623,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'num_leaves': 127,\n",
    "        'max_depth': 8,\n",
    "        'seed': 4590,\n",
    "        'bagging_freq': 1,\n",
    "        'n_jobs': 4\n",
    "    }\n",
    "    \n",
    "    # Fit the model\n",
    "    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200)\n",
    "\n",
    "    # Get feature importances\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df[\"feature\"] = list(train_features)\n",
    "    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n",
    "    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n",
    "    \n",
    "    return imp_df\n",
    "\n",
    "\n",
    "# Seed the unexpected randomness of this world\n",
    "np.random.seed(123)\n",
    "# Get the actual importance, i.e. without shuffling\n",
    "actual_imp_df = get_feature_importances(data=train, shuffle=False)\n",
    "\n",
    "\n",
    "null_imp_df = pd.DataFrame()\n",
    "nb_runs = 80\n",
    "import time\n",
    "start = time.time()\n",
    "dsp = ''\n",
    "for i in range(nb_runs):\n",
    "    # Get current run importances\n",
    "    imp_df = get_feature_importances(data=train, shuffle=True)\n",
    "    imp_df['run'] = i + 1 \n",
    "    # Concat the latest importances with the old ones\n",
    "    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
    "    # Erase previous message\n",
    "    for l in range(len(dsp)):\n",
    "        print('\\b', end='', flush=True)\n",
    "    # Display current run and time used\n",
    "    spent = (time.time() - start) / 60\n",
    "    dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n",
    "    print(dsp, end='', flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:23:16.276843Z",
     "start_time": "2019-02-21T00:23:15.519921Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_scores = []\n",
    "for _f in actual_imp_df['feature'].unique():\n",
    "    f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n",
    "    gain_score = np.log(1e-10 + f_act_imps_gain / (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n",
    "    f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n",
    "    split_score = np.log(1e-10 + f_act_imps_split / (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n",
    "    feature_scores.append((_f, split_score, gain_score))\n",
    "\n",
    "scores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:27:34.673871Z",
     "start_time": "2019-02-21T00:27:34.658266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-23.025850929940457,\n",
       " -0.2658214089278989,\n",
       " -0.18519589154821556,\n",
       " 0.15650483823833278,\n",
       " 0.16450492269016537,\n",
       " 0.18661058618846596,\n",
       " 0.23812973021014297,\n",
       " 0.2574518371641912,\n",
       " 0.2611233959771419,\n",
       " 0.31269887132776997,\n",
       " 0.33720334495444876,\n",
       " 0.3615786976590223,\n",
       " 0.40250968264128767,\n",
       " 0.4137693888027681,\n",
       " 0.43616832512541176,\n",
       " 0.4863690311894196,\n",
       " 0.5211342565534692,\n",
       " 0.5368096467972252,\n",
       " 0.5428490334709197,\n",
       " 0.5903777961160595,\n",
       " 0.628238090140852,\n",
       " 0.6787592597299911,\n",
       " 0.6792102398433272,\n",
       " 0.6861121960316421,\n",
       " 0.7012272474030976,\n",
       " 0.7040918340911749,\n",
       " 0.7068647529127972,\n",
       " 0.7553143746606086,\n",
       " 0.7837247994076565,\n",
       " 0.7879357301083176,\n",
       " 0.8122932870082558,\n",
       " 0.8155565552352437,\n",
       " 0.8347775759927206,\n",
       " 0.8357810054503212,\n",
       " 0.8360723257851232,\n",
       " 0.866636101941268,\n",
       " 0.87354088310298,\n",
       " 0.8739380082344624,\n",
       " 0.918316042514738,\n",
       " 0.9250562402060085,\n",
       " 0.9513304846728846,\n",
       " 0.9714770363027563,\n",
       " 0.9803992344109336,\n",
       " 1.0033118094884566,\n",
       " 1.0119509960424102,\n",
       " 1.031159497719275,\n",
       " 1.0457285041405577,\n",
       " 1.049415778012456,\n",
       " 1.059448520372697,\n",
       " 1.078129858073042,\n",
       " 1.0882691730123473,\n",
       " 1.1185234504655792,\n",
       " 1.1357372490871465,\n",
       " 1.1419928771019132,\n",
       " 1.158334885642318,\n",
       " 1.1908011406198888,\n",
       " 1.1933270250599375,\n",
       " 1.2005510195044995,\n",
       " 1.2332181844475347,\n",
       " 1.2427502387408609,\n",
       " 1.2438834210334482,\n",
       " 1.2486734198275589,\n",
       " 1.2633873726796043,\n",
       " 1.2719299184313575,\n",
       " 1.302165367369208,\n",
       " 1.3134083204526144,\n",
       " 1.3146648376653707,\n",
       " 1.3263525826966927,\n",
       " 1.3341704906721197,\n",
       " 1.334966990003312,\n",
       " 1.3390522085033647,\n",
       " 1.3477586336625527,\n",
       " 1.3511201899878615,\n",
       " 1.3512299765519038,\n",
       " 1.353729382743157,\n",
       " 1.3874733509469384,\n",
       " 1.405457792458931,\n",
       " 1.4193009332360276,\n",
       " 1.425978149938359,\n",
       " 1.4294126008503902,\n",
       " 1.4321730598978022,\n",
       " 1.4373344757263946,\n",
       " 1.4391147603019498,\n",
       " 1.4498901789376657,\n",
       " 1.4551711676101435,\n",
       " 1.460825230896437,\n",
       " 1.4776082116027127,\n",
       " 1.4818902482112417,\n",
       " 1.534672364070021,\n",
       " 1.5699030484853216,\n",
       " 1.570045045893723,\n",
       " 1.5737718568104484,\n",
       " 1.580196967555438,\n",
       " 1.5893106755072266,\n",
       " 1.592772697794853,\n",
       " 1.6068023596080199,\n",
       " 1.6116997726497007,\n",
       " 1.6307662512641408,\n",
       " 1.6324844057556787,\n",
       " 1.637939216606438,\n",
       " 1.6394363820561235,\n",
       " 1.6396279755158523,\n",
       " 1.6470568061295334,\n",
       " 1.6483340389895176,\n",
       " 1.6731574030262815,\n",
       " 1.6935686315537695,\n",
       " 1.7020267212989844,\n",
       " 1.705060843101357,\n",
       " 1.7293755126633639,\n",
       " 1.7303163829239836,\n",
       " 1.7445730804553,\n",
       " 1.7640323779736518,\n",
       " 1.8133523536817813,\n",
       " 1.824338923708247,\n",
       " 1.8346889399674353,\n",
       " 1.8530769743506381,\n",
       " 1.871424230652916,\n",
       " 1.8760370840789795,\n",
       " 1.8848621512239072,\n",
       " 1.8896649829432708,\n",
       " 1.8999801306303423,\n",
       " 1.931699092685042,\n",
       " 1.9656181369518377,\n",
       " 1.988233910620312,\n",
       " 2.00013609343813,\n",
       " 2.0009498495389426,\n",
       " 2.004070143491619,\n",
       " 2.0281552801477627,\n",
       " 2.031403551914085,\n",
       " 2.043174764226986,\n",
       " 2.0670298685928103,\n",
       " 2.081314484098514,\n",
       " 2.0898277961480325,\n",
       " 2.1089462484138575,\n",
       " 2.132068654551677,\n",
       " 2.2060973291951838,\n",
       " 2.210084214298844,\n",
       " 2.2108203495026038,\n",
       " 2.215765135103032,\n",
       " 2.2351908186124656,\n",
       " 2.2694930489178744,\n",
       " 2.274656482090026,\n",
       " 2.3074891028370437,\n",
       " 2.3896264322699987,\n",
       " 2.4791695113996797,\n",
       " 2.4955626887820803,\n",
       " 2.518085171848891,\n",
       " 2.5433093960630546,\n",
       " 2.6540976759056063,\n",
       " 2.766097719525793,\n",
       " 2.8257043471532257,\n",
       " 2.8394075100278116,\n",
       " 2.84947316138915,\n",
       " 3.020941786495248,\n",
       " 3.1934144645027533,\n",
       " 3.234101231102936,\n",
       " 3.2614432824809985,\n",
       " 3.3072936272606355,\n",
       " 3.325548201843183,\n",
       " 3.4766106559052004,\n",
       " 3.4907036012082946,\n",
       " 3.528573905052888,\n",
       " 4.069379017522752,\n",
       " 4.093850992021079,\n",
       " 4.0966728870678875,\n",
       " 4.417793383401297,\n",
       " 4.770686149091031,\n",
       " 4.835601948193014,\n",
       " 5.293815840144323,\n",
       " 6.010036991546325,\n",
       " 6.143616206508922,\n",
       " 6.36863985284381,\n",
       " 6.468751051713372]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thr_list=scores_df['gain_score'].tolist()\n",
    "thr_list=[ i for i in set(thr_list)]\n",
    "thr_list=sorted(thr_list)\n",
    "thr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-21T01:00:02.126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-23.025850929940457\n",
      "['features_min']\n",
      "第 0 fold\n",
      "Training until validation scores don't improve for 500 rounds.\n"
     ]
    }
   ],
   "source": [
    "def test_feature(test_fea):\n",
    "    param['n_jobs']=3\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "   \n",
    "    train_y = target\n",
    "    oof_train = np.zeros((len(train),1))\n",
    "    oof_test = np.zeros((len(test),1))\n",
    "    \n",
    "    train.reset_index(drop=True,inplace=True)\n",
    "    for idx,(idx_trn,idx_val) in enumerate(kf.split(train,train['outliers'])):\n",
    "        print('第 %d fold'%idx)\n",
    "        tr_x,tr_y,val_x,val_y=train.iloc[idx_trn][test_fea],train['target'].iloc[idx_trn],train.iloc[idx_val][test_fea],train['target'][idx_val]\n",
    "\n",
    "        trn_data = lgb.Dataset(tr_x,tr_y)#, categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(val_x,val_y)#, categorical_feature=categorical_feats)\n",
    "\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 500)\n",
    "        oof_train[idx_val] = clf.predict(val_x, num_iteration=clf.best_iteration).reshape(-1,1)\n",
    "        oof_test += clf.predict(test[test_fea], num_iteration=clf.best_iteration).reshape(-1,1)/5\n",
    "        \n",
    "        \n",
    "\n",
    "    loss=np.sqrt(mean_squared_error(oof.reshape(-1), target))\n",
    "    \n",
    "    print('mean loss %f'%loss)\n",
    "    if loss<3.660:\n",
    "        train_prob=pd.DataFrame(oof_train)\n",
    "        train_prob.columns=['class1']\n",
    "        train_prob.to_csv(\"oof/train_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "        test_prob=pd.DataFrame(oof_test)\n",
    "        test_prob.columns=['class1']\n",
    "        test_prob.to_csv(\"oof/test_prob_%s.csv\"%mean_loss,index=False)\n",
    "\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "test_dict={}\n",
    "for idx,i in enumerate(thr_list):\n",
    "    if idx>50:\n",
    "        break\n",
    "    print(i)\n",
    "    bed_fea=set(scores_df[scores_df['gain_score']<=i]['feature'].tolist())\n",
    "    bed_fea=[i for i in bed_fea]\n",
    "    print(bed_fea)\n",
    "    del_col =  ['card_id', 'first_active_month','target','outliers']+bed_fea\n",
    "    df_test_fea = [c for c in features if c not in del_col ]\n",
    "    test_dict[''.join(bed_fea)]=test_feature(df_test_fea)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
